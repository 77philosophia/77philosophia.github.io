<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Garfield&#39;s blog">
<meta property="og:url" content="http://77philosophia.github.io/page/3/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://77philosophia.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/06/from-transformer-to-llm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/from-transformer-to-llm/" class="post-title-link" itemprop="url">从Transformer到LLM:Architecture,Training and Usage</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-06 22:11:16 / Modified: 23:09:12" itemprop="dateCreated datePublished" datetime="2025-01-06T22:11:16+08:00">2025-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="nlp-basic-concepts">NLP basic concepts</h3>
<h4 id="两个nlp的主要问题">1. 两个NLP的主要问题</h4>
<p>表示和建模：</p>
<p>a.Representation:将语言表示为机器语言：如BERT,Openai embedding</p>
<ol start="2" type="a">
<li>Modeling:用统计方法建模，GPT, chatGPT</li>
</ol>
<ol type="1">
<li><p>怎么表示words</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106225834152.png"
alt="image-20250106225834152" />
<figcaption aria-hidden="true">image-20250106225834152</figcaption>
</figure></li>
</ol>
<p>将单词分散到向量空间：将token映射到向量空间，向量空间(vector space)
是用来表示文本数据的一种高纬空间。在这个空间中，每个token或文本单元都被表示为一个向量。这些向量捕捉了文本单元的语义信息，并使得数学计算（如距离计算、相似度计算）成为可能。</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230157598.png"
alt="image-20250106230157598" />
<figcaption aria-hidden="true">image-20250106230157598</figcaption>
</figure>
<ol start="2" type="1">
<li><p>建模</p>
<p>语言模型LLM: 语言模型预测任何单词序列在给定语言语料库中的可能性</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230245416.png"
alt="image-20250106230245416" />
<figcaption aria-hidden="true">image-20250106230245416</figcaption>
</figure></li>
</ol>
<ul>
<li><p>怎么学习语言模型</p>
<ul>
<li><p>自回归模型Autogressive LM:</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230342821.png"
alt="image-20250106230342821" />
<figcaption aria-hidden="true">image-20250106230342821</figcaption>
</figure></li>
<li><p>Bi-gram/N-gram model:</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230354621.png"
alt="image-20250106230354621" />
<figcaption aria-hidden="true">image-20250106230354621</figcaption>
</figure></li>
<li><p>Marked LM:</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230523067.png"
alt="image-20250106230523067" />
<figcaption aria-hidden="true">image-20250106230523067</figcaption>
</figure></li>
<li><p>几种语言模型的学习方式</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230534541.png"
alt="image-20250106230534541" />
<figcaption aria-hidden="true">image-20250106230534541</figcaption>
</figure></li>
<li><p>为什么语言模型有比较好的表示</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230544997.png"
alt="image-20250106230544997" />
<figcaption aria-hidden="true">image-20250106230544997</figcaption>
</figure></li>
<li><p>总结</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230557957.png"
alt="image-20250106230557957" />
<figcaption aria-hidden="true">image-20250106230557957</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="attention机制和transformer">attention机制和transformer</h3>
<h4 id="attention来源翻译任务seq2seq">1.
attention来源：翻译任务seq2seq</h4>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230706080.png"
alt="image-20250106230706080" />
<figcaption aria-hidden="true">image-20250106230706080</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230717889.png"
alt="image-20250106230717889" />
<figcaption aria-hidden="true">image-20250106230717889</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230732617.png"
alt="image-20250106230732617" />
<figcaption aria-hidden="true">image-20250106230732617</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230745379.png"
alt="image-20250106230745379" />
<figcaption aria-hidden="true">image-20250106230745379</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230757155.png"
alt="image-20250106230757155" />
<figcaption aria-hidden="true">image-20250106230757155</figcaption>
</figure>
<h4 id="结构">2. 结构</h4>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230817984.png"
alt="image-20250106230817984" />
<figcaption aria-hidden="true">image-20250106230817984</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230830430.png"
alt="image-20250106230830430" />
<figcaption aria-hidden="true">image-20250106230830430</figcaption>
</figure>
<h3 id="训练lm">训练LM</h3>
<h3 id="pretrained-model用法微调和prompt">Pretrained
model用法（微调和prompt）</h3>
<h3 id="transformer除语言外其他应用">transformer除语言外其他应用</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/06/style-transfer-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/style-transfer-paper/" class="post-title-link" itemprop="url">style-transfer-paper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-06 00:21:27 / Modified: 21:25:32" itemprop="dateCreated datePublished" datetime="2025-01-06T00:21:27+08:00">2025-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="风格">风格</h1>
<p>AIGC领域中的一项重要子任务就是对图像进行风格化处理，<strong>一般涉及到对图像视觉外观和纹理进行编辑（被视为风格信息），同时保留其底层对象、结构和概念不变（被视为是内容信息）</strong>。为了达到这种编辑效果，就需要实现对图像中风格和内容进行分离。现有的方法通常需要训练专门的分离模型或者需要进行大量的优化，使用成本较高。</p>
<p>查看相关paper的网址：</p>
<p>https://www.paperdigest.org/article/?id=style_transfer</p>
<h3
id="南京航空航天大学diffusestunleashing-the-capability-of-the-diffusion-model-for-style-transfer">2024/10/19南京航空航天大学《DiffuseST:Unleashing
the Capability of the Diffusion Model for Style Transfer》</h3>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106101004946.png"
alt="image-20250106101004946" />
<figcaption aria-hidden="true">image-20250106101004946</figcaption>
</figure>
<p>提出了一种training-free的风格转移方法，balancing文本，图像style和图像content的特征结合起来。具体方法：</p>
<p>对于一张内容图像Ic,和一张风格图像Is</p>
<p>a.使用BLIP Diffusion中的BLIP-2的encoder产生Is的text-aligned
embedding的表示。</p>
<p>b.为了提取空间特征，在DDIM
inverse的过程中content和style分支保留U-net的feature</p>
<p>c.在diffusion模型的step-by-step过程中分离content和style的注入。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106101632557.png"
alt="image-20250106101632557" />
<figcaption aria-hidden="true">image-20250106101632557</figcaption>
</figure>
<h3 id="text-representation-extraction">Text representation
extraction</h3>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106105738256.png"
alt="image-20250106105738256" />
<figcaption aria-hidden="true">image-20250106105738256</figcaption>
</figure>
<p>风格的特征很难用text描述，而image features又很少作为sd模型的condition
input.</p>
<p>因此，先采用Blip-diffusion里的blip-2 encoder将style
image的特征表示为文本表示。此外，我们用clip的到content
image的embedding信息。</p>
<h3 id="spatial-representation-extraction">Spatial representation
extraction</h3>
<p>用DDIM算法提取图像特征。每一层u-net有一个残差块，一个self-attention模块去加强特征表示，一个cross-attention模块和text
condition交互。关注前两个blocks，调查发现主要保持图像空间的语义和结构特征。</p>
<h4 id="content和style的injection">content和style的injection</h4>
<p>用相应注入层unet的两层参数替换到target branch.</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106110415815.png"
alt="image-20250106110415815" />
<figcaption aria-hidden="true">image-20250106110415815</figcaption>
</figure>
<h3 id="implicit-style-content-separation-using-b-lora">《Implicit
Style-Content Separation using B-LoRA》</h3>
<h4 id="主要工作">主要工作：</h4>
<p>将LoRA(低秩适应)机制引入到图像编辑领域，提出了一种称为B-LoRA的框架，该框架可以隐式分离单个图像中的风格和内容组件，同时继承了LoRA的各种优势，包括轻量化训练和即插即用等功能。此外，作者通过深度分析现有流行扩散模型（Stable
Diffusion XL,
SDXL）的内部架构，发现仅需要联合设置两个B-LoRA块即可以实现图像内容和风格的分离，从而显著的提升各种下游图像风格化任务的性能和效果。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106112237612.png"
alt="image-20250106112237612" />
<figcaption aria-hidden="true">image-20250106112237612</figcaption>
</figure>
<p>提出了一种称为B-LoRA的风格转换框架，如上图所示，由于B-LoRA继承了原始LoRA的优势，具有高度的任务灵活性，同时不容易出现过拟合（仅优化模型注意层中新加入的低秩权重，预训练模型的参数保持冻结）。<strong>通过对SDXL内部结构进行分析，作者发现仅需要对两个特定的transformer层设置B-LoRA块就可以实现对图像内容和风格的分离</strong>。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106112514801.png"
alt="image-20250106112514801" />
<figcaption aria-hidden="true">image-20250106112514801</figcaption>
</figure>
<p>B-LoRA的另一个优点是即插即用的灵活性，它可以作为单独的组件应用到各种下游图像编辑任务中，而不需要任何额外的训练或微调。例如上图展示的风格迁移、文本引导的风格操作和条件图像生成等任务。</p>
<h4 id="方法">方法：</h4>
<h5 id="对sdxl结构分析">对sdxl结构分析</h5>
<p>sdxl是一个基于扩散的文本到图像生成模型，其主干网络采用了一个大型unet架构，由70个注意力层组成，这些注意力层可以被分成11个transformer块，前两个和最后三个块分别包含4个和6个注意力层，中间6个块各包含10个注意力层，细节如下图所示。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106161123542.png"
alt="image-20250106161123542" />
<figcaption aria-hidden="true">image-20250106161123542</figcaption>
</figure>
<p>SDXL可以接受文本作为条件进行生成，具体来说，给定文本提示，首先使用OpenClip
ViT-bigG和CLIP
ViT-L两个模型对其进行编码，然后将两个编码拼接起来作为最终的文本条件，随后将通过交叉注意力层馈入到网络中。由于本文的目标是将输入图像的风格和内容解耦为单独的信号再进行处理，因而需要对sdxl每个层对生成图像的风格或内容的贡献进行判定。</p>
<p>判定方法非常简单，即将不同的文本提示注入到每个sdxl
transformer块的交叉注意力层中，随后计算这些提示与生成图像之间的语义相似度。当只改变第i个块对应的输入提示时，如果观察到生成图像的变化较为明显，则表明该块对图像质量变化占主导地位。在实际操作时，作者重点检查了sdxl的6个中间transformer块，并且定义了两组随机的文本提示content和style,其中前者通过修改对象类别来定义内容，后者通过修改颜色来定义风格，然后使用CLIP来计算生成图像的变化程度。对于一对提示px,p，作者通过将变化提示px的嵌入注入到i块中，同时将原始p的嵌入注入到其它层中来生成新图像。对6个transformer块均执行后可以得到6幅图像，可以计算得到每一对提示的变化相似度得分：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106162626224.png"
alt="image-20250106162626224" />
<figcaption aria-hidden="true">image-20250106162626224</figcaption>
</figure>
<p>作者总共挑选了400对内容和风格提示进行了实验，实验结果表明sdxl模型中的第2，4个transformer块对生成内容的影响最大，而第5个块对生成风格的影响最大，如下图所示：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106175938180.png"
alt="image-20250106175938180" />
<figcaption aria-hidden="true">image-20250106175938180</figcaption>
</figure>
<h4 id="基于lora的内容和风格分离">基于lora的内容和风格分离</h4>
<p>基于上述发现，作者认为仅需要对第2，4和第5个块进行优化就可以实现隐层特征的解耦，而无需对整体模型微调。作者引入了Lora模块[1]来对这两部分进行单独优化，另w0表示预训练sdxl模型的冻结权重，令delta{w_i}表示每个块的低秩适应矩阵，优化过程主要分为两部分，第一部分优化delta{w_2}delta{w_5}，第二部分优化delta{w_4}delta{w_5}。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106180723828.png"
alt="image-20250106180723828" />
<figcaption aria-hidden="true">image-20250106180723828</figcaption>
</figure>
<p>优化过程和生成结果如上图所示，可以看出，delta{w_2}delta{w_4}更倾向于控制图像中内容信息，且delta{w_4}可以更好的捕捉到图像中的细节信息。作者将这种解耦方式称为B-LoRA,因为其只对两个Transformer块进行了Lora微调，这样可以节省70%的显存占用。</p>
<h4 id="b-lora的风格化操作">B-lora的风格化操作</h4>
<p>在实验图像内容和风格的解耦后，作者重点对delta{w_4}和delta{w_5}两层进行微调，其中delta{w_4}捕获内容，delta{w_5}捕获风格，通过微调他们的参数来实现图像的风格化操作，整体过程如下图所示：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106190120481.png"
alt="image-20250106190120481" />
<figcaption aria-hidden="true">image-20250106190120481</figcaption>
</figure>
<p>给定一个内容图像Ic和一个风格图像Is,分别学习它们对应的B-LoRA权重delta{w_4}和delta{w_5}.然后将这两个B-LoRA权重组合到预训练的sdxl模型中，就可以将Ic的内容与Is的风格进行融合，来生成一个新的风格化图像（如上图1所示）。为了实现文本为条件的图像风格化效果（如上图2所示），只要使用内容图像Ic对应的B-LoRA权重delta{w_4},将其与用户输入的文本提示进行融合就可以实现对图像风格的编辑，这样可以很好的保留Ic的内容特征。此外还可以通过排除delta{w_4}仅使用delta{w_5}的方式来调整模型仅关注图像Is中的特定风格，这样允许用户通过输入不同的文本来单独控制生成内容（如上图3所示）。</p>
<h4 id="效果">效果：</h4>
<p>（1）图像风格迁移：给定一个内容图像和一个风格图像，通过组合两个B-LoRA的权重实现风格迁移。</p>
<p>（2）基于文本的图像风格编辑：仅使用内容图像的B-LoRA权重，加上文本提示实现对图像风格的编辑。</p>
<p>（3）一致的风格生成：使用风格图像的B-LoRA权重，生成具有相同风格的新图像。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106200150033.png"
alt="image-20250106200150033" />
<figcaption aria-hidden="true">image-20250106200150033</figcaption>
</figure>
<p>其中前两行展示了图像风格迁移的效果，即要求模型迁移style图像中的风格，同时保留content图像中的内容。可以看到，本文的方法相比其他方法更加稳定。此外，第三行图像展示了基于文本的图像风格编辑的效果，可以看到本文方法对输入对象的内容进行了良好的保留。</p>
<h4 id="限制">限制：</h4>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106200410031.png"
alt="image-20250106200410031" />
<figcaption aria-hidden="true">image-20250106200410031</figcaption>
</figure>
<p>对一些风格和内容紧密结合的图像来说，风格信息对目标身份起到了决定性作用，因此当对这种图像中的内容进行风格化处理时，很容易丢失目标的身份信息，如上图(a)(b)所示。此外，B-LoRA在面对一些复杂场景时也会出现难以准确捕获场景结构的情况，如上图(c)所示。作者表明这些局限性可以通过进一步探索LoRA解耦的属性来解决，例如在解耦时考虑更加细粒度的结构、形状、颜色、纹理等属性。</p>
<h3
id="harnessing-the-latent-diffusion-model-for-training-free-image-style-transfer">《Harnessing
the latent Diffusion Model for Training-Free Image Style Transfer》</h3>
<p>先看一下效果：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106200730275.png"
alt="image-20250106200730275" />
<figcaption aria-hidden="true">image-20250106200730275</figcaption>
</figure>
<p>在unet做风格迁移时代，方法是finetune模型，计算量很大；作者借鉴了当时做style
transfer的一种方法，AdaIn;但是直接用到现在的diffusion
model里面受限于channels的数量而不能有很好的效果。作者提出了一种STRDP的算法，变化了LDM的去噪过程，将AdaIN用一种不同的方式重复应用到lantent
diffusion
model的unet结构里。这个算法是training-free的，而且可以保留原图的颜色。</p>
<h4 id="related-work">Related work</h4>
<ul>
<li>Direct Image Optimization:基于vgg
features设计style和content的loss,优化像素值。</li>
<li>Image Feature
Transformation:将一个content图像和一个image图像提取特征，然后在content
features里面添加style feature最后做个decode
AdaIN的方法。主要就是将风格数据转移到图像特征当中。也提出了一种网络结构，将图像特征的均值和方差替换掉content特征的均值和方差。</li>
<li>Diffusion Models:
<ul>
<li>重点是可控制性</li>
<li>Guidance:guided
diffusion得到一个提供的梯度作为去噪过程中的指导。需要附加的模型和额外的训练。</li>
<li>Additive control:Controlnet包含了一份克隆的diffusion
model和新的finuetune的参数。用额外的输入比如line
arts,depth去finetune这些新参数。</li>
<li>Tuning.通过fine-tune提供预训练LDM的可控制性。比如有的方法优化text
embedding包含style信息。</li>
</ul></li>
</ul>
<h3 id="背景知识">背景知识</h3>
<ul>
<li>Diffusion
Models:将数据x加噪到高斯，去噪过程让网络预测噪声从而恢复数据x</li>
<li>Latent Diffusion Models:LDM将数据x映射到latent
space中对应z，从而降低计算量。这种表示方法也有一个问题，传统的给diffusion
model加控制的方法，如果不经过额外的训练，无法直接用到LDM中。</li>
<li>Adaptive Instance Normalization:</li>
<li><figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106202947906.png"
alt="image-20250106202947906" />
<figcaption aria-hidden="true">image-20250106202947906</figcaption>
</figure></li>
</ul>
<p>仿照之前的AdaIn的方法，在提取过程中用风格的均值和方差替换content
feature的均值和方差，按照上面的公式，这个框架的缺陷就是要重新训练一个decoder去将输出转换为image.</p>
<h4 id="我们的方法">我们的方法</h4>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106204716157.png"
alt="image-20250106204716157" />
<figcaption aria-hidden="true">image-20250106204716157</figcaption>
</figure>
<p>方法：</p>
<p>首先style image和content image都会被转换到latent
space,然后都会经过前向的diffusion process,通过加噪产生一系列的noisy
latent
representations.再反向的diffusion过程中，不断运用AdaIN集成style和content的隐变量。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212104981.png"
alt="image-20250106212104981" />
<figcaption aria-hidden="true">image-20250106212104981</figcaption>
</figure>
<p>对降噪过程中u-net的参数执行AdaIn算法。</p>
<blockquote>
<p>[!NOTE]</p>
<p>均值和方差可以很大程度的影响风格转换效果，那么我想改变一张图片的风格，如果先对其进行去风格化，再进行风格嵌入，效果是不是会出奇的好呢？</p>
</blockquote>
<p>AdaIN全称为Adaptive Instance
Normalization,是一种图像处理技术，用于实现风格迁移。它的计算公式如下：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212330208.png"
alt="image-20250106212330208" />
<figcaption aria-hidden="true">image-20250106212330208</figcaption>
</figure>
<h4 id="对比效果">对比效果</h4>
<p>保留了原图的颜色</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212355074.png"
alt="image-20250106212355074" />
<figcaption aria-hidden="true">image-20250106212355074</figcaption>
</figure>
<h5 id="参数控制">参数控制：</h5>
<p>S参数来控制在应用style
transfer之前我们对原始的content和noise加多少噪声。如果S=1，原图像加噪成高斯噪声，是无法重构出原图的。因此我们要控制好s的值。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212530108.png"
alt="image-20250106212530108" />
<figcaption aria-hidden="true">image-20250106212530108</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/hires-fix/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/hires-fix/" class="post-title-link" itemprop="url">hires-fix</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 23:53:34" itemprop="dateCreated datePublished" datetime="2025-01-05T23:53:34+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-01-06 00:19:40" itemprop="dateModified" datetime="2025-01-06T00:19:40+08:00">2025-01-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://medium.com/rendernet/using-hires-fix-to-upscale-your-stable-diffusion-images-8d8e2826593e</p>
<p>https://medium.com/<span class="citation"
data-cites="realfabianw/taking-a-closer-look-at-the-highres-fix-function-of-automatic1111-to-optimize-image-quality-d9bbf369e3f">@realfabianw/taking-a-closer-look-at-the-highres-fix-function-of-automatic1111-to-optimize-image-quality-d9bbf369e3f</span></p>
<p>放大图片除了使用A1111's的图生图/extras选项，还可以使用Hires.Fix。后者可以在generate过程中增大分辨率，得到更好的效果。</p>
<h3 id="upscaler">Upscaler</h3>
<p>Latent, R-ESRGAN 4x+ and R-ESRGAN 4x+ Anime6B都可以尝试一下。</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250105235616276.png"
alt="image-20250105235616276" />
<figcaption aria-hidden="true">image-20250105235616276</figcaption>
</figure>
<figure>
<img src="/2025/01/05/hires-fix/image-20250105235627073.png"
alt="image-20250105235627073" />
<figcaption aria-hidden="true">image-20250105235627073</figcaption>
</figure>
<h3 id="hires-steps">Hires steps:</h3>
<p>Hires
steps在初试采样后refine图片质量，他们发生在每一步采样步之后的上采样过程，总的steps是sampling
steps + Hires steps.</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250106001527600.png"
alt="image-20250106001527600" />
<figcaption aria-hidden="true">image-20250106001527600</figcaption>
</figure>
<p>Hires steps的值是0-150，0的时候Hires steps = sampling
steps.所以如果你的sampling steps是20，hires
steps是0，那么总步数是40.选择合适的数值很重要，过低或者过高都会使得图片质量变差。一般10-15根据经验会得到更好的选择。但是如果采样步数大于50，hires
steps可以设置为sampling steps的一半。</p>
<h3 id="denoising-strength">Denoising strength</h3>
<p>对图片质量有很大的影响，也会改变图片内容。</p>
<p>设置为0对图片没有影响，设置成1会很大改变图片内容。</p>
<p>webui设置的默认值是0.7，但是在大多数case里这个设置都太强了，初始建议设置到0.3到0.5然后从上往下调整。</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250106001856856.png"
alt="image-20250106001856856" />
<figcaption aria-hidden="true">image-20250106001856856</figcaption>
</figure>
<p>用webui的xyz工具进行测试选出合适的scaler</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250106001935542.png"
alt="image-20250106001935542" />
<figcaption aria-hidden="true">image-20250106001935542</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/improve-rag/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/improve-rag/" class="post-title-link" itemprop="url">提高rag的技巧</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 23:48:54 / Modified: 23:51:07" itemprop="dateCreated datePublished" datetime="2025-01-05T23:48:54+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://readmedium.com/these-are-the-3-langchain-functions-i-used-to-improve-my-rag-00413ccb7094</p>
<ol type="1">
<li>Multi Query Retriever</li>
<li>Long Context Recorder</li>
<li>Contextual Compression</li>
</ol>
<figure>
<img src="/2025/01/05/improve-rag/image-20250105235015001.png"
alt="image-20250105235015001" />
<figcaption aria-hidden="true">image-20250105235015001</figcaption>
</figure>
<h3 id="long-context-reorder">Long-Context Reorder</h3>
<p>https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/long_context_reorder/</p>
<p>无论你的模型架构如何，当包含超过 <strong>10</strong>
个检索到的文档时，性能都会显著下降。简而言之：当模型必须在长上下文中访问相关信息时，它们往往会忽略提供的文档。</p>
<p>策略：</p>
<p>将不太相关的文档放在列表的中间，而更相关的文档放在开头和结尾。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/prompt-engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/prompt-engineering/" class="post-title-link" itemprop="url">Prompt Design and Engineering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 23:12:42 / Modified: 23:47:01" itemprop="dateCreated datePublished" datetime="2025-01-05T23:12:42+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>[!TIP]</p>
<p>Prompt engineering:指令+questions+input+example</p>
</blockquote>
<h3 id="llms的limitations">LLMs的limitations</h3>
<ul>
<li>短暂记忆</li>
<li>输出不一致性</li>
<li>过期信息</li>
<li>内容捏造</li>
<li>领域专业性</li>
</ul>
<h3 id="一些建议和技巧">一些建议和技巧</h3>
<ul>
<li>CoT prompting</li>
<li>让模型通过其他方式输出真实的东西</li>
<li>直接结束prompt</li>
<li>强势的态度</li>
<li>让ai自我纠正</li>
<li>产生不同的观点</li>
<li>保持状态+角色扮演（gpt的web端维护了一个session，api没有实现）</li>
<li>教算法</li>
<li><strong>LLMs的本质是往前读并补全文本，因此examples的顺序和prompt的顺序很重要。</strong></li>
<li>Affordances:达到一定条件触发的函数</li>
</ul>
<h3 id="高级建议和技巧">高级建议和技巧</h3>
<ul>
<li>CoT
<ul>
<li>Zero-shot: lets think step by step...</li>
<li>Manual-Cot:</li>
</ul></li>
<li>ToT(Tree of thought)</li>
</ul>
<h3 id="工具">工具：</h3>
<ul>
<li>ART</li>
<li>通过自一致性增强依赖性</li>
<li>反思</li>
<li>专家prompt：prompt chainer</li>
<li>Streamlining Complex Tasks with Chains</li>
<li>引导性的输出with rails</li>
<li>流水线的Prompt设计with自动的prompt engineering</li>
</ul>
<h3 id="argmenting-llms-through-external-knowledge--rag">Argmenting LLMs
through External Knowledge -RAG</h3>
<ul>
<li>RAG-aware Prompting Techniques</li>
</ul>
<h3 id="llm-agents">LLM Agents</h3>
<h4 id="agents">Agents</h4>
<ul>
<li>Reasoning without Observation</li>
<li>Reason And Act</li>
<li>Dialog-Enabled Resolving Agents</li>
</ul>
<h3 id="工具和框架">工具和框架</h3>
<ul>
<li>Langchain</li>
<li>Semantic Kernel</li>
<li>The Guidance</li>
<li>Nemo Guardrails</li>
<li>LlamaIndex</li>
<li>FastRAG</li>
<li>Auto-GPT</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/Blip-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/Blip-2/" class="post-title-link" itemprop="url">Blip-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 16:31:01 / Modified: 16:38:22" itemprop="dateCreated datePublished" datetime="2025-01-05T16:31:01+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="背景">背景：</h3>
<p><strong>Vision-language
pre-training,</strong>将视觉和语言的只是结合起来。但是模型通常高<strong>cost</strong>，因为需要端到端训练<strong>vison</strong>和<strong>language</strong>模型<strong>(</strong>通常用<strong>transformers)</strong>。因此需要一种更不复杂的<strong>vision-language</strong>模型，不需要端到端的训练，于是<strong>BLIP-2</strong>就产生了。</p>
<h3
id="blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models"><strong>BLIP-2:
Bootstrapping Language-Image Pre-training with Frozen Image Encoders and
Large Language Models</strong></h3>
<p>作者采用了一个轻量级的<strong>querying
transfomer</strong>结构作为<strong>frozen image</strong>和<strong>text
encoders</strong>的<strong>bottleneck.</strong>首先用<strong>image
encoder</strong>提取图像特征，然后送给语言模型去理解。但是语言模型没有在<strong>image</strong>上面训练过，因此它无法直接理解这些视觉表示。为了解决这个问题，<strong>Q-Former</strong>采用一些可学习的<strong>querying
vectors</strong>，在两个阶段进行预训练；</p>
<p>（1）vision-language representation learning with a frozen image
encoder</p>
<ol start="2" type="1">
<li>vision-to-language generative learning stage with a frozen text
encoder</li>
</ol>
<figure>
<img src="/2025/01/05/Blip-2/image-20250105163736925.png"
alt="image-20250105163736925" />
<figcaption aria-hidden="true">image-20250105163736925</figcaption>
</figure>
<p>Q-Former包括两个子模块：</p>
<p>a.一个image transformer和frozen image encoder交互。</p>
<p>b.一个</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/webui-api/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/webui-api/" class="post-title-link" itemprop="url">webui用api跑图</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 16:28:26 / Modified: 16:30:25" itemprop="dateCreated datePublished" datetime="2025-01-05T16:28:26+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="官方教程">官方教程：</h3>
<p>https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API</p>
<h3 id="步骤">步骤：</h3>
<ol type="1">
<li><p>在启动界面，需要指定api启动。如python launch.py --api</p></li>
<li><p>打开http://localhost:7860/docs可以看到接口文档。</p>
<figure>
<img src="/2025/01/05/webui-api/image-20250105163022602.png"
alt="image-20250105163022602" />
<figcaption aria-hidden="true">image-20250105163022602</figcaption>
</figure></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/ai-scaler/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/ai-scaler/" class="post-title-link" itemprop="url">ai-scaler</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 16:23:35 / Modified: 16:27:30" itemprop="dateCreated datePublished" datetime="2025-01-05T16:23:35+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="为什么需要ai-upscaler">为什么需要ai upscaler</h2>
<p>为了展现图片在屏幕上，图片一般需要被放大，从而看起来质量就会很低。</p>
<h2 id="为什么不用传统的upscaler">为什么不用传统的upscaler</h2>
<p>传统的resize的算法，比如最近邻插值或者Lanczos插值，只用到了图像的像素。图像容易corrupted或者扭曲。没有很好的算法能准确弥补这些缺失的信息。</p>
<h2 id="ai-upscaler怎么工作">AI upscaler怎么工作</h2>
<p>AI
upscaler是神经网络训练了大量数据得到的，在图像放大时可以填充细节信息。</p>
<figure>
<img src="/2025/01/05/ai-scaler/image-20250105162728273.png"
alt="image-20250105162728273" />
<figcaption aria-hidden="true">image-20250105162728273</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/04/langchain-robot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/04/langchain-robot/" class="post-title-link" itemprop="url">用lanchain做ai robot</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-04 23:34:19 / Modified: 23:56:18" itemprop="dateCreated datePublished" datetime="2025-01-04T23:34:19+08:00">2025-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="什么是langchin">什么是langchin</h3>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104233619635.png"
alt="image-20250104233619635" />
<figcaption aria-hidden="true">image-20250104233619635</figcaption>
</figure>
<p>langchain是一个支持大语言模型相关应用开发的框架。</p>
<p>使得建设与ai相关的应用会更容易。</p>
<ul>
<li><p>集成：将外部数据，比如文件、api、应用集成进来</p></li>
<li><p>代理：和环境集成</p></li>
</ul>
<p>组件 - LangChain
使得更换必要的抽象和组件以使用语言模型变得轻而易举。</p>
<p>自定义链 - LangChain
提供现成的支持来使用和自定义“链”——一系列串联在一起的操作。</p>
<p>速度 🚢 - 这个团队的交付速度非常快。您将会跟上最新的 LLM 功能。</p>
<p>社区 👥 - 极好的 Discord 和社区支持，见面会、黑客松等活动。</p>
<h2 id="llms">LLMs</h2>
<ul>
<li>公司开发和控制的专有模型：成本高、许可证限制、闭源</li>
<li>开源模型：开源、灵活性、可能缺乏大公司的支持和资源</li>
</ul>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104233744531.png"
alt="image-20250104233744531" />
<figcaption aria-hidden="true">image-20250104233744531</figcaption>
</figure>
<p>用api</p>
<p>AzureOpenAI:适用于一般的长文本生成任务，如小说写作、文章创作等。</p>
<p>AzureChatOpenAI:适用于涉及大量对话的文本生成任务，尤其是需要管理对话上下文时。</p>
<p>根据你的具体需求选择合适的模型。如果主要任务是长文本创作且对话不是主要部分，建议使用
AzureOpenAI。如果有大量对话且需要更好地管理对话上下文，建议使用
AzureChatOpenAI。</p>
<p>AzureOpenAI 会报错</p>
<p>Error code: 400 - {'error': {'code': 'OperationNotSupported',
'message': 'The completion operation does not work with the specified
model, gpt-4o. Please choose different model and try again. You can
learn more about which models can be used with each operation here:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This basic example demostrate the LLM response and ChatModel Response</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> AzureChatOpenAI</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv, find_dotenv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the OpenAI library configuration using the retrieved environment variables</span></span><br><span class="line">OPENAI_API_TYPE = <span class="string">&quot;azure&quot;</span></span><br><span class="line">OPENAI_API_BASE = <span class="string">&quot;https://sparkopenai2024.openai.azure.com/&quot;</span></span><br><span class="line">OPENAI_API_VERSION = <span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">OPENAI_API_KEY = <span class="string">&quot;xxx&quot;</span></span><br><span class="line">GPT4V_ENDPOINT = <span class="string">&quot;https://sparkopenai2024.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize an instance of AzureOpenAI using the specified settings</span></span><br><span class="line"><span class="comment"># llm = AzureOpenAI(</span></span><br><span class="line"><span class="comment">#     openai_api_version=OPENAI_API_VERSION,</span></span><br><span class="line"><span class="comment">#     openai_api_key=OPENAI_API_KEY,</span></span><br><span class="line"><span class="comment">#     openai_api_base=OPENAI_API_BASE,</span></span><br><span class="line"><span class="comment">#     openai_api_type=OPENAI_API_TYPE,</span></span><br><span class="line"><span class="comment">#     deployment_name=&quot;gpt-4o&quot;  # Name of the deployment for identification</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize an instance of AzureChatOpenAI using the specified settings</span></span><br><span class="line">chat_llm = AzureChatOpenAI(</span><br><span class="line">    openai_api_version=OPENAI_API_VERSION,</span><br><span class="line">    openai_api_key=OPENAI_API_KEY,</span><br><span class="line">    openai_api_base=OPENAI_API_BASE,</span><br><span class="line">    openai_api_type=OPENAI_API_TYPE,</span><br><span class="line">    deployment_name=<span class="string">&quot;gpt-4o&quot;</span>  </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the response from AzureOpenAI LLM for a specific question</span></span><br><span class="line"><span class="comment"># print(&quot;AzureOpenAI LLM Response: &quot;, llm(&quot; what is the weather in mumbai today?&quot;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the response from AzureChatOpenAI for the same question</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;AzureOpenAI ChatLLM Response: &quot;</span>, chat_llm.predict(<span class="string">&quot;what is the weather in mumbai today?&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果直接与大模型交互-&gt;chatGPT。自从发现可以利</span><br><span class="line">用自有数据来增强大语言模型（LL</span><br><span class="line">M）的能力以来，如何将 LL</span><br><span class="line">M 的通用</span><br><span class="line">有效结合一直是热门话题。</span><br><span class="line">知识与个人数据</span><br><span class="line">1. 微调(finetue)</span><br><span class="line">2.</span><br><span class="line">RAG(检索增强)</span><br></pre></td></tr></table></figure>
<h3
id="先来考虑一个关于小说内容的chatrobot">先来考虑一个关于小说内容的chatRobot:</h3>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104234107482.png"
alt="image-20250104234107482" />
<figcaption aria-hidden="true">image-20250104234107482</figcaption>
</figure>
<p><strong>topic</strong>:</p>
<ul>
<li>文档分割</li>
<li>文档 <strong>Load</strong>：数据加载， <strong>LangChain</strong>
提供的 <strong>80</strong>
多种独特的加载器，以访问包括音频和视频在内的各种数据源。</li>
<li>向量存储和嵌入：深入了解嵌入的概念，探索 <strong>LangChain</strong>
中的向量存储集成。</li>
<li>检索：掌握在 <strong>Vector</strong>
存储中访问和索引数据的高级技术，使您能够检索语义查询之外的最相关信息。</li>
<li>问题解答：构建一次性问题解答解决方案<strong>/</strong>总结方案。</li>
</ul>
<h4 id="long-text-summarisation">Long text summarisation</h4>
<h5 id="为什么context-window会是limit">为什么context
window会是limit</h5>
<p>当前大多数的语言模型是基于解码器的模型。这些模型使用了变换器架构中的解码器部分来预测下一个标记的概率。然后，将这个预测的标记附加到输入文本中，形成预测下一个标记的输入。</p>
<p><strong>Context Window Size = Input Sequence Length + Prompt Length +
O</strong></p>
<p><strong>utput Sequence Length</strong></p>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104234423043.png"
alt="image-20250104234423043" />
<figcaption aria-hidden="true">image-20250104234423043</figcaption>
</figure>
<p>举个例子：如果context window的限制是4097，prompt的token
size是50，期待输出的总结内容是200，那么最大的输入文本的token
size就是4097-50-200=3847tokens，大概对应3000个词。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>LLMs受限于固定的上下文窗口，比如chatgpt的limit
tokens是4096，大概对应3000多个词。对于这个问题有两种解决方案：第一种用更大context
window的LLMs;第二种方法是化整为零，将长文本分成很多个短文本，分别送进模型处理然后合并，或者选择最相关的部分然后送进去分析。</p>
</blockquote>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104234750149.png"
alt="image-20250104234750149" />
<figcaption aria-hidden="true">image-20250104234750149</figcaption>
</figure>
<p>文档分割：</p>
<p>CharacterTextSplitter:直接按字符数量分割文本。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c_splitter = CharacterTextSplitter(</span><br><span class="line">    chunk_size=chunk_size,</span><br><span class="line">    chunk_overlap=chunk_overlap,</span><br><span class="line">    separator = &#x27; &#x27;    //主要在哪一块分割</span><br><span class="line">)</span><br><span class="line">c_splitter.split_text(text3)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><p>RecursiveCharacterTextSplitter: 按照递归规则分割文本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">r_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=150,</span><br><span class="line">    chunk_overlap=0,</span><br><span class="line">    separators=[&quot;\n\n&quot;, &quot;\n&quot;, &quot;(?&lt;=\. )&quot;, &quot; &quot;, &quot;&quot;]</span><br><span class="line">)</span><br><span class="line">r_splitter.split_text(some_text)</span><br></pre></td></tr></table></figure></li>
<li><p>TokenTextSplitter：跟LLMs里的token的概念对齐</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)</span><br></pre></td></tr></table></figure></li>
<li><p>Context aware splitting</p></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from langchain.document_loaders import NotionDirectoryLoader</span><br><span class="line">from langchain.text_splitter import MarkdownHeaderTextSplitter</span><br><span class="line"></span><br><span class="line">markdown_document = &quot;&quot;&quot;# Title\n\n \</span><br><span class="line">## Chapter 1\n\n \</span><br><span class="line">Hi this is Jim\n\n Hi this is Joe\n\n \</span><br><span class="line">### Section \n\n \</span><br><span class="line">Hi this is Lance \n\n </span><br><span class="line">## Chapter 2\n\n \</span><br><span class="line">Hi this is Molly&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">headers_to_split_on = [</span><br><span class="line">    (&quot;#&quot;, &quot;Header 1&quot;),</span><br><span class="line">    (&quot;##&quot;, &quot;Header 2&quot;),</span><br><span class="line">    (&quot;###&quot;, &quot;Header 3&quot;),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">markdown_splitter = MarkdownHeaderTextSplitter(</span><br><span class="line">    headers_to_split_on=headers_to_split_on</span><br><span class="line">)</span><br><span class="line">md_header_splits = markdown_splitter.split_text(markdown_document)</span><br></pre></td></tr></table></figure>
<h3 id="rag搜索增强">RAG搜索增强</h3>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104235342635.png"
alt="image-20250104235342635" />
<figcaption aria-hidden="true">image-20250104235342635</figcaption>
</figure>
<blockquote>
<p>[!NOTE]</p>
<p><strong>query</strong>和<strong>prompt</strong>的区别是什么？</p>
<p>在使用 <strong>RetrievalQA chain</strong> 进行问答时， prompt 和
query
是两个不同的概念。理解它们的区别对于构建有效的问答系统非常重要。</p>
<ol type="1">
<li><strong>Query（查询）</strong> ：</li>
</ol>
<p>◦ query 是用户提出的问题或查询。例如，“<strong>What is the capital of
France?</strong>”</p>
<p>◦ 在 qa_chain_mr 中， query
是必须的，因为它是整个问答流程的起点。系统根据 query
去检索相关的文档，然后从中抽取答</p>
<p>案。</p>
<ol start="2" type="1">
<li><strong>Prompt（提示词）</strong> ：</li>
</ol>
<p>◦ prompt
是用于指导语言模型生成答案的额外文本或上下文。它可以包含特定的指示或格式化信息，以帮助模型生成更合适的响应。</p>
<p>提供prompt可以帮助模型更好地理解上下文或期望的回答形式。例如，你可以提供一个
prompt 来指示模型回答时的语气、详细程度或者其他特定要求。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/04/aigc-paper-share/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/04/aigc-paper-share/" class="post-title-link" itemprop="url">AIGC论文reading</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-04 22:26:20 / Modified: 22:43:16" itemprop="dateCreated datePublished" datetime="2025-01-04T22:26:20+08:00">2025-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="stable-diffusion-原理">Stable diffusion 原理</h2>
<p>从data产生noise很容易，从noise产生data是生成</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222656242.png"
alt="image-20250104222656242" />
<figcaption aria-hidden="true">image-20250104222656242</figcaption>
</figure>
<h2 id="一致性保持目标">一致性保持目标：</h2>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222713620.png"
alt="image-20250104222713620" />
<figcaption aria-hidden="true">image-20250104222713620</figcaption>
</figure>
<h1 id="解法">解法：</h1>
<h2 id="storydiffusion做保持">StoryDiffusion：做保持</h2>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222809450.png"
alt="image-20250104222809450" />
<figcaption aria-hidden="true">image-20250104222809450</figcaption>
</figure>
<p>产出这些漫画的研究出自南开大学、字节跳动等机构。在《StoryDiffusion：Consistent
Self-Attention for long-range image and video
generation》这篇论文中，该研究团队提出了一种名为 StoryDiffusion
的新方法，用于生成一致的图像和视频以讲述复杂故事。</p>
<ul>
<li><p>论文地址：https://arxiv.org/pdf/2405.01434v1</p></li>
<li><p>项目主页：https://storydiffusion.github.io/</p></li>
</ul>
<p>如上图所示，使用Consistent
Self-Attention生成的图像成功保持了身份和服装的一致性，这对于讲故事至关重要。</p>
<h4 id="主要亮点一致性图像生成">主要亮点：一致性图像生成</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222859537.png"
alt="image-20250104222859537" />
<figcaption aria-hidden="true">image-20250104222859537</figcaption>
</figure>
<h4 id="总结">总结：</h4>
<p>应用场景：小批次人物服装、身份的保持</p>
<h2
id="omg多人物图像生成的工具垫图-区域分离重绘">OMG：多人物图像生成的工具（垫图
+ 区域分离重绘）</h2>
<p>主页: https://kongzhecn.github.io/omg-project/</p>
<p>代码: https://github.com/kongzhecn/OMG/</p>
<h4 id="背景">背景：</h4>
<p>单概念的定制化生成，Textual Inversion，
LORA，InstanceID等方法已经比较成熟了。而对于多概念的定制化生成，如果直接使用LORA融合等方法，主要挑战在于不同的概念信息之间的信息泄漏，造成属性混乱。比如想要生成一个特定的黑色长发男角色，一个特定的紫色短发女角色，在单图出单角色时正常，但是在单图出双角色时，很有可能发型发色信息就混乱了，比如可能出现紫色长发女角色。比较容易想到的做法就是垫图+掩码。</p>
<p>难点：个性化保持、空间遮挡</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222920431.png"
alt="image-20250104222920431" />
<figcaption aria-hidden="true">image-20250104222920431</figcaption>
</figure>
<p>多概念个性化（multi-concept
personalization）指的是在同一个图像生成过程中，同时整合和表达多个不同的概念或主题。这种方法要求在生成图像时，能够准确保留和表现每个概念的独特特征，同时保证它们之间的协调和和谐。例如，在生成一个包含多个不同人物、背景和物体的图像时，必须确保每个元素都能清晰地展现其独特性，并且整体图像具有一致性和美感。</p>
<h4 id="先看效果">先看效果：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222948149.png"
alt="image-20250104222948149" />
<figcaption aria-hidden="true">image-20250104222948149</figcaption>
</figure>
<h4 id="即插即用">即插即用</h4>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr>
<th><strong>OMG +</strong> <strong>LoRA</strong> <strong>(ID with
multiple images)</strong></th>
<th><img src="/2025/01/04/aigc-paper-share/image-20250104223125232.png"
alt="image-20250104223125232" /></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OMG + InstantID (ID with single image)</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223145816.png"
alt="image-20250104223145816" /></td>
</tr>
<tr>
<td><strong>OMG + ControlNet (Layout Control )</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223207497.png"
alt="image-20250104223207497" /></td>
</tr>
<tr>
<td><strong>OMG + style LoRAs (Style Control)</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223217116.png"
alt="image-20250104223217116" /></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="怎么做到">怎么做到：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223238466.png"
alt="image-20250104223238466" />
<figcaption aria-hidden="true">image-20250104223238466</figcaption>
</figure>
<p>OMG是一种两阶段的生成方法，一阶段先用无角色信息的文生图模型垫图生成布局，然后提取垫图全面的视觉信息（mask
和 attention map），二阶段将各角色的特定概念信息作用于对应的 mask
区域，避免信息泄露，属性错乱，并复用一阶段的 attention
map，维持构图布局不变。这里的概念信息可以是 LoRA 也可以是
InstantID。</p>
<h5
id="一阶段生成构图布局并提取视觉信息">1.一阶段：生成构图布局并提取视觉信息</h5>
<p>首先用一个全局的prompt p在文生图模型T2I上生成一张垫图</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223305875.png"
alt="image-20250104223305875" />
<figcaption aria-hidden="true">image-20250104223305875</figcaption>
</figure>
<p>这里要注意的是，没有任何角色信息(lora/InstanceId)</p>
<p>第一阶段，我们要提取垫图的视觉信息，这里的视觉信息有两项：生成过程中每一时间步每一层的注意力图
attention map
和每个角色的分割掩码。注意力图在生图过程中记得保存即可，而角色的掩码图，则是基于角色类别的基本词（man、woman），使用开集检测分割的模型（如
GroundingDION+SAM、YoloWorld+EfficientSAM）来进行分割。</p>
<h5 id="第二阶段多概念定制化去噪">第二阶段：多概念定制化去噪</h5>
<p>为了避免概念信息泄露，造成角色属性错乱，OMG
在进行第二阶段多概念定制化生成时不进行 LoRA
融合，而是使用多个单概念的模型分别进行推理，并作用于一阶段得到的各自掩码区域中。即一个
LoRA 只负责一个角色区域的生成。这称为概念噪声混合（Concept Noise
Blending）。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223331480.png"
alt="image-20250104223331480" />
<figcaption aria-hidden="true">image-20250104223331480</figcaption>
</figure>
<h5 id="不同时间步开启混合">不同时间步开启混合：</h5>
<p>下图对比了第二阶段中不同时间步开启概念噪声混合对最终生成结果的影响，最左侧是从一开始，第
50 步（因为用了 DDIM 采样器）就开启概念噪声混合，最右侧表示第 0
步才开启，相当于没开启，即完全等于第一阶段的结果，中间是 0-50
步之间开启。可以看到，概念噪声混合开启得越早，概念特征保持得越好。并且，开启得越早对图像构图布局的影响也越大，开启的越晚，构图布局与第一阶段结果越相近。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223346380.png"
alt="image-20250104223346380" />
<figcaption aria-hidden="true">image-20250104223346380</figcaption>
</figure>
<h4 id="总结-1">总结：</h4>
<p>想要在有交叠的情况下，精确地控制多个概念的属性特征，基本思路是 ”垫图
+ 区域分离重绘“
的方案，但是这种方法会有个问题：如果多个概念的基本语义类是一样的，比如两个
woman，这时候 zero-shot
分割模型怎么工作，对于相同语义类的多概念很难进行可控的个性化生成</p>
<h2
id="threatergen多轮对话llmimage-generation">ThreaterGen：多轮对话（LLM+image
generation）</h2>
<p><strong>https://howe140.github.io/theatergen.io/</strong></p>
<h4 id="背景-1">背景：</h4>
<ol type="1">
<li>语义一致性——现有方法在处理复杂描述（如空间关系、数量或指代表达“它们”等）时遇到困难，导致生成的图像在语义上与用户请求不一致；(2)
上下文一致性——在多轮对话中的图像生成过程中，同一实体经常难以在不同轮次中保持一致特征，甚至可能被遗忘。例如，同一只狗在不同轮次中看起来不同而没有用户编辑。</li>
</ol>
<h4 id="先看效果-1">先看效果：</h4>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr>
<th>讲故事</th>
<th><img src="/2025/01/04/aigc-paper-share/image-20250104223537655.png"
alt="image-20250104223537655" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>多轮编辑</td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223550213.png"
alt="image-20250104223550213" /></td>
</tr>
<tr>
<td>定量：在自己的数据集上测试</td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223626463.png"
alt="image-20250104223626463" /></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="怎么做到-1">怎么做到：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223722467.png"
alt="image-20250104223722467" />
<figcaption aria-hidden="true">image-20250104223722467</figcaption>
</figure>
<p>本文提出的 ppl 如图，可以分为三个阶段</p>
<p>1.第一阶段是 LLM 的角色设计：用 LLM 完成角色设计，包括角色的外观描述
+ layout 信息。</p>
<p>2.第二阶段用 T2I 完成第一阶段角色的图片生成和 latent 提取，作为
reference img</p>
<p>3.第三阶段整合前两个阶段的信息，生成符合用户输入 prompt 的图片。</p>
<hr />
<h5 id="第一阶段llm-角色设计">1.第一阶段，LLM 角色设计</h5>
<p>在这一阶段，利用 LLM，根据用户输入的要求，把对应 prompt
的信息做格式化，格式化的输出包含了背景 prompt、neg
prompt，另外最核心的部分是各个主体的 prompt。每一个主体 prompt 都是一个
triplet 对，是一个包含了 id、prompt、layout（bbox 格式）的三元组。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223810670.png"
alt="image-20250104223810670" />
<figcaption aria-hidden="true">image-20250104223810670</figcaption>
</figure>
<h5 id="第二阶段reference-img-生成">2.第二阶段，reference img 生成</h5>
<p>根据第一阶段给出的主体 prompt，用 T2I
模型生成一张对应角色的参考图片，这张参考图后续会通过 ip-adapter
来注入模型。有了 reference img，就可以针对各个 prompt
生成对应的图片，步骤如下：</p>
<p>a）根据 ref img + 待生成
prompt，可以生成一张（只）包含该主体的图片</p>
<p>b）对上述图片做分割，得到前景的物体。因为一条 prompt
里面可能包含多个物体，所以需要对每一个出现的物体都执行第一、二步。</p>
<p>c）根据 LLM 给出的 layout
信息，将第二部去除背景的主体贴在对应位置上（需要做 scale 的适配）。</p>
<p>d）然后对第三步得到的图片，计算 canny 图，另外经过一次 SD
的前传得到每一步的 feature 作为生成的 guidance（称为 latent
guidance）。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223829556.png"
alt="image-20250104223829556" />
<figcaption aria-hidden="true">image-20250104223829556</figcaption>
</figure>
<h5 id="第三阶段信息整合图片生成">第三阶段，信息整合+图片生成</h5>
<p>在这一阶段，会把上述格式化的 prompt 合并，作为 SD 输入的 text
prompt。canny 信息通过 controlnet 注入 SD。而 lantent guidance
注入方式如下，会选择在一定的 step
范围内注入而不是全过程都加。注入的时候，主体 mask 区域内用 latent
guidance，mask外则用 SD 本身生成的内容。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223847906.png"
alt="image-20250104223847906" />
<figcaption aria-hidden="true">image-20250104223847906</figcaption>
</figure>
<h4 id="总结-2">总结：</h4>
<p>1.利用 LLM + T2I 的能力来做连续故事的 ip 保持。通过 LLM
保证生成故事上下文的连续性，以及人物的关联性；通过 LLM
来生成主体的特征描述，将这个特征描述送给 T2I
模型生成该主体的一张参考图。LLM 还被用来生成图片的
layout，用来控制每一个主体的生成位置。</p>
<p>2.多轮生成过程中，允许用户通过外部输入来改变生成结果，允许用户交互</p>
<h2
id="autostudio在多轮交互式图像生成中打造一致的主体">AutoStudio：在多轮交互式图像生成中打造一致的主体</h2>
<h4 id="背景-2">背景：</h4>
<p>文本到图像(TGI)生成模型在生成单张图像方面已经表现出色,更具挑战性的任务——多轮交互式图像生成开始吸引相关研究社区的注意.这个任务要求模型在多个回合中与用户互动，以生成连贯的图像序列。然而，由于用户可能频繁切换主体，目前的努力难以在生成多样化图像的同时保持主体一致性。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223924036.png"
alt="image-20250104223924036" />
<figcaption aria-hidden="true">image-20250104223924036</figcaption>
</figure>
<h4 id="效果">效果：</h4>
<h5
id="量化指标autostudio在所有指标上都明显优于之前的方法">1.量化指标：AutoStudio<strong>在所有指标上都明显优于之前的方法</strong>。</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223944051.png"
alt="image-20250104223944051" />
<figcaption aria-hidden="true">image-20250104223944051</figcaption>
</figure>
<h5 id="可视化">2.可视化：</h5>
<p>Theatergen无法处理人物之间复杂的互动（如拥抱和接吻），而MiniGemini则难以保持主体的一致性。</p>
<p>Intelligent
Grimm和StoryDiffusion无法在多回合互动中保持多个角色之间的一致性，并表现出有限的编辑效果。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224024890.png"
alt="image-20250104224024890" />
<figcaption aria-hidden="true">image-20250104224024890</figcaption>
</figure>
<h4 id="怎么做到-2">怎么做到：</h4>
<p>他们的目标是引入一个多功能、可扩展的框架，通过多智能体协作，可以将任何所需的LLM架构和扩散骨干结合到框架中，以满足用户多轮生成的多样化需求。</p>
<p>具体而言，AutoStudio包括三个基于LLM的智能体：</p>
<ul>
<li><p><strong>主题管理器</strong>解释对话，识别不同的主题，并为其分配适当的上下文；-》ID，prompt</p></li>
<li><p><strong>布局*<em>*</em>生成器</strong>为每个主题生成部分级别的边界框，以控制主题的位置；-》coarse
layout</p></li>
<li><p><strong>监督员</strong>为布局生成器提供布局改进和修正的建议。</p></li>
</ul>
<p>最后，<strong>绘制器</strong>基于扩散模型完成基于改进布局的图像生成。</p>
<p>此外，研究人员在绘制器中引入了一个<strong>并行*<em>*</em>UNet</strong>（P-UNet），它具有一种新颖的架构，利用两个并行的交叉注意力模块分别增强文本和图像嵌入的潜在主题特征。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224046351.png"
alt="image-20250104224046351" />
<figcaption aria-hidden="true">image-20250104224046351</figcaption>
</figure>
<h5 id="主题管理器">主题管理器：</h5>
<p>1.历史输入 2.预定义prompt</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224101887.png"
alt="image-20250104224101887" />
<figcaption aria-hidden="true">image-20250104224101887</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224109475.png"
alt="image-20250104224109475" />
<figcaption aria-hidden="true">image-20250104224109475</figcaption>
</figure>
<h5 id="layout管理器">layout管理器：</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224123560.png"
alt="image-20250104224123560" />
<figcaption aria-hidden="true">image-20250104224123560</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224131380.png"
alt="image-20250104224131380" />
<figcaption aria-hidden="true">image-20250104224131380</figcaption>
</figure>
<h5 id="supervisor管理器">supervisor管理器</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224146486.png"
alt="image-20250104224146486" />
<figcaption aria-hidden="true">image-20250104224146486</figcaption>
</figure>
<h5 id="图像生成">图像生成：</h5>
<p>有了好的布局，就能生成好的具有一致多主体表现的吸引人图像吗？</p>
<p>现在的：模型不微调，图像可控-》depth、open pose-》controlnet</p>
<p>1.主体初始化生成：</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224202197.png"
alt="image-20250104224202197" />
<figcaption aria-hidden="true">image-20250104224202197</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224209991.png"
alt="image-20250104224209991" />
<figcaption aria-hidden="true">image-20250104224209991</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224217340.png"
alt="image-20250104224217340" />
<figcaption aria-hidden="true">image-20250104224217340</figcaption>
</figure>
<p>2.PUnet</p>
<p>将去噪过程中任意UNet层的输入潜在特征表示为Z。我们将UNet层的原始交叉注意力模块拆分为两个并行的文本和图像交叉注意力模块（分别表示为PTCA和PICA）来优化ZZZ。这两个模块具有相同的架构，其关键思想是计算ZZZ与每个主体文本/图像嵌入之间的特征相似性。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224244780.png"
alt="image-20250104224244780" />
<figcaption aria-hidden="true">image-20250104224244780</figcaption>
</figure>
<h4 id="总结-3">总结：</h4>
<p>1.工程化应用和控制很好，总-分的思路去控制生成。</p>
<p>2.引入监督器形成反馈链路。</p>
<p>关于generator《-》supervisor，考试-评分的耦合结构关系，</p>
<p>阿里数学竞赛ai最高分：</p>
<p><a
target="_blank" rel="noopener" href="https://blog.richardstu.com/solution-sharing-and-some-thoughts-about-alibaba-mathematical-competition-for-ai">Solution
Sharing and Some Thoughts about Alibaba Global Mathematical Competition
for AI</a></p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224306153.png"
alt="image-20250104224306153" />
<figcaption aria-hidden="true">image-20250104224306153</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224313553.png"
alt="image-20250104224313553" />
<figcaption aria-hidden="true">image-20250104224313553</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
