<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Garfield&#39;s blog">
<meta property="og:url" content="http://77philosophia.github.io/page/6/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://77philosophia.github.io/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/04/langchain-robot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/04/langchain-robot/" class="post-title-link" itemprop="url">用lanchain做ai robot</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-04 23:34:19 / Modified: 23:56:18" itemprop="dateCreated datePublished" datetime="2025-01-04T23:34:19+08:00">2025-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="什么是langchin">什么是langchin</h3>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104233619635.png"
alt="image-20250104233619635" />
<figcaption aria-hidden="true">image-20250104233619635</figcaption>
</figure>
<p>langchain是一个支持大语言模型相关应用开发的框架。</p>
<p>使得建设与ai相关的应用会更容易。</p>
<ul>
<li><p>集成：将外部数据，比如文件、api、应用集成进来</p></li>
<li><p>代理：和环境集成</p></li>
</ul>
<p>组件 - LangChain
使得更换必要的抽象和组件以使用语言模型变得轻而易举。</p>
<p>自定义链 - LangChain
提供现成的支持来使用和自定义“链”——一系列串联在一起的操作。</p>
<p>速度 🚢 - 这个团队的交付速度非常快。您将会跟上最新的 LLM 功能。</p>
<p>社区 👥 - 极好的 Discord 和社区支持，见面会、黑客松等活动。</p>
<h2 id="llms">LLMs</h2>
<ul>
<li>公司开发和控制的专有模型：成本高、许可证限制、闭源</li>
<li>开源模型：开源、灵活性、可能缺乏大公司的支持和资源</li>
</ul>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104233744531.png"
alt="image-20250104233744531" />
<figcaption aria-hidden="true">image-20250104233744531</figcaption>
</figure>
<p>用api</p>
<p>AzureOpenAI:适用于一般的长文本生成任务，如小说写作、文章创作等。</p>
<p>AzureChatOpenAI:适用于涉及大量对话的文本生成任务，尤其是需要管理对话上下文时。</p>
<p>根据你的具体需求选择合适的模型。如果主要任务是长文本创作且对话不是主要部分，建议使用
AzureOpenAI。如果有大量对话且需要更好地管理对话上下文，建议使用
AzureChatOpenAI。</p>
<p>AzureOpenAI 会报错</p>
<p>Error code: 400 - {'error': {'code': 'OperationNotSupported',
'message': 'The completion operation does not work with the specified
model, gpt-4o. Please choose different model and try again. You can
learn more about which models can be used with each operation here:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This basic example demostrate the LLM response and ChatModel Response</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> AzureChatOpenAI</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv, find_dotenv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the OpenAI library configuration using the retrieved environment variables</span></span><br><span class="line">OPENAI_API_TYPE = <span class="string">&quot;azure&quot;</span></span><br><span class="line">OPENAI_API_BASE = <span class="string">&quot;https://sparkopenai2024.openai.azure.com/&quot;</span></span><br><span class="line">OPENAI_API_VERSION = <span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">OPENAI_API_KEY = <span class="string">&quot;xxx&quot;</span></span><br><span class="line">GPT4V_ENDPOINT = <span class="string">&quot;https://sparkopenai2024.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize an instance of AzureOpenAI using the specified settings</span></span><br><span class="line"><span class="comment"># llm = AzureOpenAI(</span></span><br><span class="line"><span class="comment">#     openai_api_version=OPENAI_API_VERSION,</span></span><br><span class="line"><span class="comment">#     openai_api_key=OPENAI_API_KEY,</span></span><br><span class="line"><span class="comment">#     openai_api_base=OPENAI_API_BASE,</span></span><br><span class="line"><span class="comment">#     openai_api_type=OPENAI_API_TYPE,</span></span><br><span class="line"><span class="comment">#     deployment_name=&quot;gpt-4o&quot;  # Name of the deployment for identification</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize an instance of AzureChatOpenAI using the specified settings</span></span><br><span class="line">chat_llm = AzureChatOpenAI(</span><br><span class="line">    openai_api_version=OPENAI_API_VERSION,</span><br><span class="line">    openai_api_key=OPENAI_API_KEY,</span><br><span class="line">    openai_api_base=OPENAI_API_BASE,</span><br><span class="line">    openai_api_type=OPENAI_API_TYPE,</span><br><span class="line">    deployment_name=<span class="string">&quot;gpt-4o&quot;</span>  </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the response from AzureOpenAI LLM for a specific question</span></span><br><span class="line"><span class="comment"># print(&quot;AzureOpenAI LLM Response: &quot;, llm(&quot; what is the weather in mumbai today?&quot;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the response from AzureChatOpenAI for the same question</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;AzureOpenAI ChatLLM Response: &quot;</span>, chat_llm.predict(<span class="string">&quot;what is the weather in mumbai today?&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果直接与大模型交互-&gt;chatGPT。自从发现可以利</span><br><span class="line">用自有数据来增强大语言模型（LL</span><br><span class="line">M）的能力以来，如何将 LL</span><br><span class="line">M 的通用</span><br><span class="line">有效结合一直是热门话题。</span><br><span class="line">知识与个人数据</span><br><span class="line">1. 微调(finetue)</span><br><span class="line">2.</span><br><span class="line">RAG(检索增强)</span><br></pre></td></tr></table></figure>
<h3
id="先来考虑一个关于小说内容的chatrobot">先来考虑一个关于小说内容的chatRobot:</h3>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104234107482.png"
alt="image-20250104234107482" />
<figcaption aria-hidden="true">image-20250104234107482</figcaption>
</figure>
<p><strong>topic</strong>:</p>
<ul>
<li>文档分割</li>
<li>文档 <strong>Load</strong>：数据加载， <strong>LangChain</strong>
提供的 <strong>80</strong>
多种独特的加载器，以访问包括音频和视频在内的各种数据源。</li>
<li>向量存储和嵌入：深入了解嵌入的概念，探索 <strong>LangChain</strong>
中的向量存储集成。</li>
<li>检索：掌握在 <strong>Vector</strong>
存储中访问和索引数据的高级技术，使您能够检索语义查询之外的最相关信息。</li>
<li>问题解答：构建一次性问题解答解决方案<strong>/</strong>总结方案。</li>
</ul>
<h4 id="long-text-summarisation">Long text summarisation</h4>
<h5 id="为什么context-window会是limit">为什么context
window会是limit</h5>
<p>当前大多数的语言模型是基于解码器的模型。这些模型使用了变换器架构中的解码器部分来预测下一个标记的概率。然后，将这个预测的标记附加到输入文本中，形成预测下一个标记的输入。</p>
<p><strong>Context Window Size = Input Sequence Length + Prompt Length +
O</strong></p>
<p><strong>utput Sequence Length</strong></p>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104234423043.png"
alt="image-20250104234423043" />
<figcaption aria-hidden="true">image-20250104234423043</figcaption>
</figure>
<p>举个例子：如果context window的限制是4097，prompt的token
size是50，期待输出的总结内容是200，那么最大的输入文本的token
size就是4097-50-200=3847tokens，大概对应3000个词。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>LLMs受限于固定的上下文窗口，比如chatgpt的limit
tokens是4096，大概对应3000多个词。对于这个问题有两种解决方案：第一种用更大context
window的LLMs;第二种方法是化整为零，将长文本分成很多个短文本，分别送进模型处理然后合并，或者选择最相关的部分然后送进去分析。</p>
</blockquote>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104234750149.png"
alt="image-20250104234750149" />
<figcaption aria-hidden="true">image-20250104234750149</figcaption>
</figure>
<p>文档分割：</p>
<p>CharacterTextSplitter:直接按字符数量分割文本。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c_splitter = CharacterTextSplitter(</span><br><span class="line">    chunk_size=chunk_size,</span><br><span class="line">    chunk_overlap=chunk_overlap,</span><br><span class="line">    separator = &#x27; &#x27;    //主要在哪一块分割</span><br><span class="line">)</span><br><span class="line">c_splitter.split_text(text3)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><p>RecursiveCharacterTextSplitter: 按照递归规则分割文本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">r_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=150,</span><br><span class="line">    chunk_overlap=0,</span><br><span class="line">    separators=[&quot;\n\n&quot;, &quot;\n&quot;, &quot;(?&lt;=\. )&quot;, &quot; &quot;, &quot;&quot;]</span><br><span class="line">)</span><br><span class="line">r_splitter.split_text(some_text)</span><br></pre></td></tr></table></figure></li>
<li><p>TokenTextSplitter：跟LLMs里的token的概念对齐</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)</span><br></pre></td></tr></table></figure></li>
<li><p>Context aware splitting</p></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from langchain.document_loaders import NotionDirectoryLoader</span><br><span class="line">from langchain.text_splitter import MarkdownHeaderTextSplitter</span><br><span class="line"></span><br><span class="line">markdown_document = &quot;&quot;&quot;# Title\n\n \</span><br><span class="line">## Chapter 1\n\n \</span><br><span class="line">Hi this is Jim\n\n Hi this is Joe\n\n \</span><br><span class="line">### Section \n\n \</span><br><span class="line">Hi this is Lance \n\n </span><br><span class="line">## Chapter 2\n\n \</span><br><span class="line">Hi this is Molly&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">headers_to_split_on = [</span><br><span class="line">    (&quot;#&quot;, &quot;Header 1&quot;),</span><br><span class="line">    (&quot;##&quot;, &quot;Header 2&quot;),</span><br><span class="line">    (&quot;###&quot;, &quot;Header 3&quot;),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">markdown_splitter = MarkdownHeaderTextSplitter(</span><br><span class="line">    headers_to_split_on=headers_to_split_on</span><br><span class="line">)</span><br><span class="line">md_header_splits = markdown_splitter.split_text(markdown_document)</span><br></pre></td></tr></table></figure>
<h3 id="rag搜索增强">RAG搜索增强</h3>
<figure>
<img src="/2025/01/04/langchain-robot/image-20250104235342635.png"
alt="image-20250104235342635" />
<figcaption aria-hidden="true">image-20250104235342635</figcaption>
</figure>
<blockquote>
<p>[!NOTE]</p>
<p><strong>query</strong>和<strong>prompt</strong>的区别是什么？</p>
<p>在使用 <strong>RetrievalQA chain</strong> 进行问答时， prompt 和
query
是两个不同的概念。理解它们的区别对于构建有效的问答系统非常重要。</p>
<ol type="1">
<li><strong>Query（查询）</strong> ：</li>
</ol>
<p>◦ query 是用户提出的问题或查询。例如，“<strong>What is the capital of
France?</strong>”</p>
<p>◦ 在 qa_chain_mr 中， query
是必须的，因为它是整个问答流程的起点。系统根据 query
去检索相关的文档，然后从中抽取答</p>
<p>案。</p>
<ol start="2" type="1">
<li><strong>Prompt（提示词）</strong> ：</li>
</ol>
<p>◦ prompt
是用于指导语言模型生成答案的额外文本或上下文。它可以包含特定的指示或格式化信息，以帮助模型生成更合适的响应。</p>
<p>提供prompt可以帮助模型更好地理解上下文或期望的回答形式。例如，你可以提供一个
prompt 来指示模型回答时的语气、详细程度或者其他特定要求。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/04/aigc-paper-share/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/04/aigc-paper-share/" class="post-title-link" itemprop="url">AIGC论文reading</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-04 22:26:20 / Modified: 22:43:16" itemprop="dateCreated datePublished" datetime="2025-01-04T22:26:20+08:00">2025-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="stable-diffusion-原理">Stable diffusion 原理</h2>
<p>从data产生noise很容易，从noise产生data是生成</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222656242.png"
alt="image-20250104222656242" />
<figcaption aria-hidden="true">image-20250104222656242</figcaption>
</figure>
<h2 id="一致性保持目标">一致性保持目标：</h2>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222713620.png"
alt="image-20250104222713620" />
<figcaption aria-hidden="true">image-20250104222713620</figcaption>
</figure>
<h1 id="解法">解法：</h1>
<h2 id="storydiffusion做保持">StoryDiffusion：做保持</h2>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222809450.png"
alt="image-20250104222809450" />
<figcaption aria-hidden="true">image-20250104222809450</figcaption>
</figure>
<p>产出这些漫画的研究出自南开大学、字节跳动等机构。在《StoryDiffusion：Consistent
Self-Attention for long-range image and video
generation》这篇论文中，该研究团队提出了一种名为 StoryDiffusion
的新方法，用于生成一致的图像和视频以讲述复杂故事。</p>
<ul>
<li><p>论文地址：https://arxiv.org/pdf/2405.01434v1</p></li>
<li><p>项目主页：https://storydiffusion.github.io/</p></li>
</ul>
<p>如上图所示，使用Consistent
Self-Attention生成的图像成功保持了身份和服装的一致性，这对于讲故事至关重要。</p>
<h4 id="主要亮点一致性图像生成">主要亮点：一致性图像生成</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222859537.png"
alt="image-20250104222859537" />
<figcaption aria-hidden="true">image-20250104222859537</figcaption>
</figure>
<h4 id="总结">总结：</h4>
<p>应用场景：小批次人物服装、身份的保持</p>
<h2
id="omg多人物图像生成的工具垫图-区域分离重绘">OMG：多人物图像生成的工具（垫图
+ 区域分离重绘）</h2>
<p>主页: https://kongzhecn.github.io/omg-project/</p>
<p>代码: https://github.com/kongzhecn/OMG/</p>
<h4 id="背景">背景：</h4>
<p>单概念的定制化生成，Textual Inversion，
LORA，InstanceID等方法已经比较成熟了。而对于多概念的定制化生成，如果直接使用LORA融合等方法，主要挑战在于不同的概念信息之间的信息泄漏，造成属性混乱。比如想要生成一个特定的黑色长发男角色，一个特定的紫色短发女角色，在单图出单角色时正常，但是在单图出双角色时，很有可能发型发色信息就混乱了，比如可能出现紫色长发女角色。比较容易想到的做法就是垫图+掩码。</p>
<p>难点：个性化保持、空间遮挡</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222920431.png"
alt="image-20250104222920431" />
<figcaption aria-hidden="true">image-20250104222920431</figcaption>
</figure>
<p>多概念个性化（multi-concept
personalization）指的是在同一个图像生成过程中，同时整合和表达多个不同的概念或主题。这种方法要求在生成图像时，能够准确保留和表现每个概念的独特特征，同时保证它们之间的协调和和谐。例如，在生成一个包含多个不同人物、背景和物体的图像时，必须确保每个元素都能清晰地展现其独特性，并且整体图像具有一致性和美感。</p>
<h4 id="先看效果">先看效果：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222948149.png"
alt="image-20250104222948149" />
<figcaption aria-hidden="true">image-20250104222948149</figcaption>
</figure>
<h4 id="即插即用">即插即用</h4>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr>
<th><strong>OMG +</strong> <strong>LoRA</strong> <strong>(ID with
multiple images)</strong></th>
<th><img src="/2025/01/04/aigc-paper-share/image-20250104223125232.png"
alt="image-20250104223125232" /></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OMG + InstantID (ID with single image)</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223145816.png"
alt="image-20250104223145816" /></td>
</tr>
<tr>
<td><strong>OMG + ControlNet (Layout Control )</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223207497.png"
alt="image-20250104223207497" /></td>
</tr>
<tr>
<td><strong>OMG + style LoRAs (Style Control)</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223217116.png"
alt="image-20250104223217116" /></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="怎么做到">怎么做到：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223238466.png"
alt="image-20250104223238466" />
<figcaption aria-hidden="true">image-20250104223238466</figcaption>
</figure>
<p>OMG是一种两阶段的生成方法，一阶段先用无角色信息的文生图模型垫图生成布局，然后提取垫图全面的视觉信息（mask
和 attention map），二阶段将各角色的特定概念信息作用于对应的 mask
区域，避免信息泄露，属性错乱，并复用一阶段的 attention
map，维持构图布局不变。这里的概念信息可以是 LoRA 也可以是
InstantID。</p>
<h5
id="一阶段生成构图布局并提取视觉信息">1.一阶段：生成构图布局并提取视觉信息</h5>
<p>首先用一个全局的prompt p在文生图模型T2I上生成一张垫图</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223305875.png"
alt="image-20250104223305875" />
<figcaption aria-hidden="true">image-20250104223305875</figcaption>
</figure>
<p>这里要注意的是，没有任何角色信息(lora/InstanceId)</p>
<p>第一阶段，我们要提取垫图的视觉信息，这里的视觉信息有两项：生成过程中每一时间步每一层的注意力图
attention map
和每个角色的分割掩码。注意力图在生图过程中记得保存即可，而角色的掩码图，则是基于角色类别的基本词（man、woman），使用开集检测分割的模型（如
GroundingDION+SAM、YoloWorld+EfficientSAM）来进行分割。</p>
<h5 id="第二阶段多概念定制化去噪">第二阶段：多概念定制化去噪</h5>
<p>为了避免概念信息泄露，造成角色属性错乱，OMG
在进行第二阶段多概念定制化生成时不进行 LoRA
融合，而是使用多个单概念的模型分别进行推理，并作用于一阶段得到的各自掩码区域中。即一个
LoRA 只负责一个角色区域的生成。这称为概念噪声混合（Concept Noise
Blending）。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223331480.png"
alt="image-20250104223331480" />
<figcaption aria-hidden="true">image-20250104223331480</figcaption>
</figure>
<h5 id="不同时间步开启混合">不同时间步开启混合：</h5>
<p>下图对比了第二阶段中不同时间步开启概念噪声混合对最终生成结果的影响，最左侧是从一开始，第
50 步（因为用了 DDIM 采样器）就开启概念噪声混合，最右侧表示第 0
步才开启，相当于没开启，即完全等于第一阶段的结果，中间是 0-50
步之间开启。可以看到，概念噪声混合开启得越早，概念特征保持得越好。并且，开启得越早对图像构图布局的影响也越大，开启的越晚，构图布局与第一阶段结果越相近。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223346380.png"
alt="image-20250104223346380" />
<figcaption aria-hidden="true">image-20250104223346380</figcaption>
</figure>
<h4 id="总结-1">总结：</h4>
<p>想要在有交叠的情况下，精确地控制多个概念的属性特征，基本思路是 ”垫图
+ 区域分离重绘“
的方案，但是这种方法会有个问题：如果多个概念的基本语义类是一样的，比如两个
woman，这时候 zero-shot
分割模型怎么工作，对于相同语义类的多概念很难进行可控的个性化生成</p>
<h2
id="threatergen多轮对话llmimage-generation">ThreaterGen：多轮对话（LLM+image
generation）</h2>
<p><strong>https://howe140.github.io/theatergen.io/</strong></p>
<h4 id="背景-1">背景：</h4>
<ol type="1">
<li>语义一致性——现有方法在处理复杂描述（如空间关系、数量或指代表达“它们”等）时遇到困难，导致生成的图像在语义上与用户请求不一致；(2)
上下文一致性——在多轮对话中的图像生成过程中，同一实体经常难以在不同轮次中保持一致特征，甚至可能被遗忘。例如，同一只狗在不同轮次中看起来不同而没有用户编辑。</li>
</ol>
<h4 id="先看效果-1">先看效果：</h4>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr>
<th>讲故事</th>
<th><img src="/2025/01/04/aigc-paper-share/image-20250104223537655.png"
alt="image-20250104223537655" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>多轮编辑</td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223550213.png"
alt="image-20250104223550213" /></td>
</tr>
<tr>
<td>定量：在自己的数据集上测试</td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223626463.png"
alt="image-20250104223626463" /></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="怎么做到-1">怎么做到：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223722467.png"
alt="image-20250104223722467" />
<figcaption aria-hidden="true">image-20250104223722467</figcaption>
</figure>
<p>本文提出的 ppl 如图，可以分为三个阶段</p>
<p>1.第一阶段是 LLM 的角色设计：用 LLM 完成角色设计，包括角色的外观描述
+ layout 信息。</p>
<p>2.第二阶段用 T2I 完成第一阶段角色的图片生成和 latent 提取，作为
reference img</p>
<p>3.第三阶段整合前两个阶段的信息，生成符合用户输入 prompt 的图片。</p>
<hr />
<h5 id="第一阶段llm-角色设计">1.第一阶段，LLM 角色设计</h5>
<p>在这一阶段，利用 LLM，根据用户输入的要求，把对应 prompt
的信息做格式化，格式化的输出包含了背景 prompt、neg
prompt，另外最核心的部分是各个主体的 prompt。每一个主体 prompt 都是一个
triplet 对，是一个包含了 id、prompt、layout（bbox 格式）的三元组。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223810670.png"
alt="image-20250104223810670" />
<figcaption aria-hidden="true">image-20250104223810670</figcaption>
</figure>
<h5 id="第二阶段reference-img-生成">2.第二阶段，reference img 生成</h5>
<p>根据第一阶段给出的主体 prompt，用 T2I
模型生成一张对应角色的参考图片，这张参考图后续会通过 ip-adapter
来注入模型。有了 reference img，就可以针对各个 prompt
生成对应的图片，步骤如下：</p>
<p>a）根据 ref img + 待生成
prompt，可以生成一张（只）包含该主体的图片</p>
<p>b）对上述图片做分割，得到前景的物体。因为一条 prompt
里面可能包含多个物体，所以需要对每一个出现的物体都执行第一、二步。</p>
<p>c）根据 LLM 给出的 layout
信息，将第二部去除背景的主体贴在对应位置上（需要做 scale 的适配）。</p>
<p>d）然后对第三步得到的图片，计算 canny 图，另外经过一次 SD
的前传得到每一步的 feature 作为生成的 guidance（称为 latent
guidance）。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223829556.png"
alt="image-20250104223829556" />
<figcaption aria-hidden="true">image-20250104223829556</figcaption>
</figure>
<h5 id="第三阶段信息整合图片生成">第三阶段，信息整合+图片生成</h5>
<p>在这一阶段，会把上述格式化的 prompt 合并，作为 SD 输入的 text
prompt。canny 信息通过 controlnet 注入 SD。而 lantent guidance
注入方式如下，会选择在一定的 step
范围内注入而不是全过程都加。注入的时候，主体 mask 区域内用 latent
guidance，mask外则用 SD 本身生成的内容。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223847906.png"
alt="image-20250104223847906" />
<figcaption aria-hidden="true">image-20250104223847906</figcaption>
</figure>
<h4 id="总结-2">总结：</h4>
<p>1.利用 LLM + T2I 的能力来做连续故事的 ip 保持。通过 LLM
保证生成故事上下文的连续性，以及人物的关联性；通过 LLM
来生成主体的特征描述，将这个特征描述送给 T2I
模型生成该主体的一张参考图。LLM 还被用来生成图片的
layout，用来控制每一个主体的生成位置。</p>
<p>2.多轮生成过程中，允许用户通过外部输入来改变生成结果，允许用户交互</p>
<h2
id="autostudio在多轮交互式图像生成中打造一致的主体">AutoStudio：在多轮交互式图像生成中打造一致的主体</h2>
<h4 id="背景-2">背景：</h4>
<p>文本到图像(TGI)生成模型在生成单张图像方面已经表现出色,更具挑战性的任务——多轮交互式图像生成开始吸引相关研究社区的注意.这个任务要求模型在多个回合中与用户互动，以生成连贯的图像序列。然而，由于用户可能频繁切换主体，目前的努力难以在生成多样化图像的同时保持主体一致性。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223924036.png"
alt="image-20250104223924036" />
<figcaption aria-hidden="true">image-20250104223924036</figcaption>
</figure>
<h4 id="效果">效果：</h4>
<h5
id="量化指标autostudio在所有指标上都明显优于之前的方法">1.量化指标：AutoStudio<strong>在所有指标上都明显优于之前的方法</strong>。</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223944051.png"
alt="image-20250104223944051" />
<figcaption aria-hidden="true">image-20250104223944051</figcaption>
</figure>
<h5 id="可视化">2.可视化：</h5>
<p>Theatergen无法处理人物之间复杂的互动（如拥抱和接吻），而MiniGemini则难以保持主体的一致性。</p>
<p>Intelligent
Grimm和StoryDiffusion无法在多回合互动中保持多个角色之间的一致性，并表现出有限的编辑效果。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224024890.png"
alt="image-20250104224024890" />
<figcaption aria-hidden="true">image-20250104224024890</figcaption>
</figure>
<h4 id="怎么做到-2">怎么做到：</h4>
<p>他们的目标是引入一个多功能、可扩展的框架，通过多智能体协作，可以将任何所需的LLM架构和扩散骨干结合到框架中，以满足用户多轮生成的多样化需求。</p>
<p>具体而言，AutoStudio包括三个基于LLM的智能体：</p>
<ul>
<li><p><strong>主题管理器</strong>解释对话，识别不同的主题，并为其分配适当的上下文；-》ID，prompt</p></li>
<li><p><strong>布局*<em>*</em>生成器</strong>为每个主题生成部分级别的边界框，以控制主题的位置；-》coarse
layout</p></li>
<li><p><strong>监督员</strong>为布局生成器提供布局改进和修正的建议。</p></li>
</ul>
<p>最后，<strong>绘制器</strong>基于扩散模型完成基于改进布局的图像生成。</p>
<p>此外，研究人员在绘制器中引入了一个<strong>并行*<em>*</em>UNet</strong>（P-UNet），它具有一种新颖的架构，利用两个并行的交叉注意力模块分别增强文本和图像嵌入的潜在主题特征。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224046351.png"
alt="image-20250104224046351" />
<figcaption aria-hidden="true">image-20250104224046351</figcaption>
</figure>
<h5 id="主题管理器">主题管理器：</h5>
<p>1.历史输入 2.预定义prompt</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224101887.png"
alt="image-20250104224101887" />
<figcaption aria-hidden="true">image-20250104224101887</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224109475.png"
alt="image-20250104224109475" />
<figcaption aria-hidden="true">image-20250104224109475</figcaption>
</figure>
<h5 id="layout管理器">layout管理器：</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224123560.png"
alt="image-20250104224123560" />
<figcaption aria-hidden="true">image-20250104224123560</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224131380.png"
alt="image-20250104224131380" />
<figcaption aria-hidden="true">image-20250104224131380</figcaption>
</figure>
<h5 id="supervisor管理器">supervisor管理器</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224146486.png"
alt="image-20250104224146486" />
<figcaption aria-hidden="true">image-20250104224146486</figcaption>
</figure>
<h5 id="图像生成">图像生成：</h5>
<p>有了好的布局，就能生成好的具有一致多主体表现的吸引人图像吗？</p>
<p>现在的：模型不微调，图像可控-》depth、open pose-》controlnet</p>
<p>1.主体初始化生成：</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224202197.png"
alt="image-20250104224202197" />
<figcaption aria-hidden="true">image-20250104224202197</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224209991.png"
alt="image-20250104224209991" />
<figcaption aria-hidden="true">image-20250104224209991</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224217340.png"
alt="image-20250104224217340" />
<figcaption aria-hidden="true">image-20250104224217340</figcaption>
</figure>
<p>2.PUnet</p>
<p>将去噪过程中任意UNet层的输入潜在特征表示为Z。我们将UNet层的原始交叉注意力模块拆分为两个并行的文本和图像交叉注意力模块（分别表示为PTCA和PICA）来优化ZZZ。这两个模块具有相同的架构，其关键思想是计算ZZZ与每个主体文本/图像嵌入之间的特征相似性。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224244780.png"
alt="image-20250104224244780" />
<figcaption aria-hidden="true">image-20250104224244780</figcaption>
</figure>
<h4 id="总结-3">总结：</h4>
<p>1.工程化应用和控制很好，总-分的思路去控制生成。</p>
<p>2.引入监督器形成反馈链路。</p>
<p>关于generator《-》supervisor，考试-评分的耦合结构关系，</p>
<p>阿里数学竞赛ai最高分：</p>
<p><a
target="_blank" rel="noopener" href="https://blog.richardstu.com/solution-sharing-and-some-thoughts-about-alibaba-mathematical-competition-for-ai">Solution
Sharing and Some Thoughts about Alibaba Global Mathematical Competition
for AI</a></p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224306153.png"
alt="image-20250104224306153" />
<figcaption aria-hidden="true">image-20250104224306153</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224313553.png"
alt="image-20250104224313553" />
<figcaption aria-hidden="true">image-20250104224313553</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/04/laion-dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/04/laion-dataset/" class="post-title-link" itemprop="url">laion数据集</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-04 22:24:05 / Modified: 22:25:08" itemprop="dateCreated datePublished" datetime="2025-01-04T22:24:05+08:00">2025-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="laion-5b">LAION-5B</h2>
<p>LAION-5B 是目前规模最大的开放多模态数据集之一，包含 58.5 亿个 <a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=CLIP&amp;spm=1001.2101.3001.7020">CLIP</a>
[5]过滤的图像-文本对的数据集，比 LAION-400M 大 14
倍，是世界第一大规模、多模态的文本图像数据集，<strong>共80T数据</strong>，并提供了色情图片过滤、水印图片过滤、高分辨率图片、美学图片等子集和模型，供不同方向研究。</p>
<p>LAION-5B中包括23.2亿的英语，22.6亿的100+语言及12.7亿的未知语言，我们将子集分别标记为：</p>
<p><strong>●</strong> laion2B-en (应该是我们要的2B数据集)</p>
<p><strong>●</strong> <strong>laion2B-multi</strong></p>
<p><strong>●</strong> <strong>laion1B-nolang</strong></p>
<p>LAION训练了一个基于CLIP嵌入的色情内容识别模型NSFW，可以过滤3%的不适图片，NSFW准确率约96%，过滤后有子集：</p>
<p>laion2B-en-safety</p>
<p>laion2B-multi-safety</p>
<p>laion1B-nolang-safety</p>
<p>LAION训练了一个水印识别模型，过滤后有子集：</p>
<p>laion2B-en-watermark</p>
<p>laion2B-multi-watermark</p>
<p>laion1B-nolang-watermark</p>
<p>一个170M的超分辨率子集：</p>
<p>laion-high-resolution</p>
<p>一个120M的美学图片子集，可以用来做图片生成：</p>
<p>laion-aesthetic</p>
<h3 id="子集">子集</h3>
<p>LAION-5B 提供了多种子集，以满足不同的研究需求：</p>
<ul>
<li><p><strong>LAION-400M</strong>：规模较小，约 400
万个图像-文本对，方便快速实验。</p></li>
<li><p><strong>LAION-2B</strong>：包含 20
亿对，适合中等规模实验。</p></li>
<li><p><strong>LAION-5B</strong>：完整数据集。</p>
<figure>
<img src="/2025/01/04/laion-dataset/image-20250104222449781.png"
alt="image-20250104222449781" />
<figcaption aria-hidden="true">image-20250104222449781</figcaption>
</figure></li>
</ul>
<h3 id="laion-2b-hd-子集">LAION-2B-HD 子集</h3>
<ul>
<li><p><strong>图像-文本对总数</strong>：约 <strong>1.2
亿对</strong>。</p></li>
<li><p><strong>图像分辨率</strong>：</p></li>
<li><p>大多数图像的分辨率在 <strong>1024×1024
像素</strong>或更高。</p></li>
<li><p><strong>数据来源</strong>：</p></li>
<li><p>从 LAION-2B
数据集中过滤生成，确保图像清晰度高，文本描述相关性强。</p></li>
</ul>
<h4 id="特点">特点</h4>
<ul>
<li><p>高质量图像，适合图像生成、超级分辨率等任务。</p></li>
<li><p>文本描述更贴近高质量图像语义。</p></li>
</ul>
<p><strong>LAION-2B-HD</strong> 是从 LAION
数据集中筛选出的高质量、高分辨率图像子集，适合需要高分辨率图像和精确文本描述的任务。</p>
<h2 id="下载方法">下载方法：</h2>
<p>https://github.com/rom1504/img2dataset/blob/main/dataset_examples/laion-high-resolution.md</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/03/hugging-download/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/03/hugging-download/" class="post-title-link" itemprop="url">hugging-download</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-03 19:38:45" itemprop="dateCreated datePublished" datetime="2025-01-03T19:38:45+08:00">2025-01-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-01-04 22:16:12" itemprop="dateModified" datetime="2025-01-04T22:16:12+08:00">2025-01-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="hugging-face登录方式">1.Hugging face登录方式</h3>
<p>huggingface废弃用户名和密码的登录方式，用token</p>
<p>wget --header="Authorization: Bearer hf_xxx"<br />
</p>
<p>https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt</p>
<h3
id="可以下载sd1.5下载数据集却403-forbidden">2.可以下载sd1.5,下载数据集却403
forbidden</h3>
<ol type="1">
<li>403 forbidden（将token权限增加write,我全打开了）</li>
<li><figure>
<img src="/2025/01/03/hugging-download/image-20250103194111502.png"
alt="image-20250103194111502" />
<figcaption aria-hidden="true">image-20250103194111502</figcaption>
</figure></li>
</ol>
<p>https://discuss.huggingface.co/t/error-403-what-to-do-about-it/12983</p>
<h3
id="国内机器访问huggingface网络不好无法下载">3.国内机器访问huggingface网络不好，无法下载</h3>
<p>用镜像网站hf-mirror.com</p>
<figure>
<img src="/2025/01/03/hugging-download/image-20250103194152062.png"
alt="image-20250103194152062" />
<figcaption aria-hidden="true">image-20250103194152062</figcaption>
</figure>
<p>wget --header="Authorization: Bearer hf_xxx"
https://hf-mirror.com/datasets/laion/laion-high-resolution/resolve/main/.part-00000-45914064-d424-4c1c-8d96-dc8125c645fb-c000.snappy.parquet.crc</p>
<h3 id="下载laion数据集用img2dataset工具">4.
下载laion数据集用img2dataset工具</h3>
<p>注意huggingface的laion
high-resolution文件里有两种文件类型，crc是parquet的校验文件，下了没用，要下载parquet文件是数据集的metadata</p>
<h3 id="wandb超时">wandb超时</h3>
<p>export WANDB_MODE=offline</p>
<h2 id="步骤">步骤：</h2>
<ol type="1">
<li><p>下载metadata</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for i in &#123;0..127&#125;; do </span><br><span class="line"></span><br><span class="line">  wget --header=&quot;Authorization: Bearer hf_xxx&quot; https://hf-mirror.com/datasets/laion/laion-high-resolution/resolve/main/part-$(printf &quot;%05d&quot; $i)-45914064-d424-4c1c-8d96-dc8125c645fb-c000.snappy.parquet</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure></li>
</ol>
<p>2.下载原数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">export WANDB_MODE=offline</span><br><span class="line"></span><br><span class="line">img2dataset --url_list laion-high-resolution --input_format &quot;parquet&quot;\</span><br><span class="line"></span><br><span class="line">​     --url_col &quot;URL&quot; --caption_col &quot;TEXT&quot; --output_format webdataset\</span><br><span class="line"></span><br><span class="line">​      --output_folder laion-high-resolution-output --processes_count 16 --thread_count 64 --image_size 1024\</span><br><span class="line"></span><br><span class="line">​      --resize_only_if_bigger=True --resize_mode=&quot;keep_ratio&quot; --skip_reencode=True \</span><br><span class="line"></span><br><span class="line">​       --save_additional_columns &#x27;[&quot;similarity&quot;,&quot;hash&quot;,&quot;punsafe&quot;,&quot;pwatermark&quot;,&quot;LANGUAGE&quot;]&#x27; --enable_wandb True</span><br></pre></td></tr></table></figure>
<p>应该已经开始下载了</p>
<figure>
<img src="/2025/01/03/hugging-download/image-20250104221538386.png"
alt="image-20250104221538386" />
<figcaption aria-hidden="true">image-20250104221538386</figcaption>
</figure>
<h2 id="下载数据集结果">下载数据集结果</h2>
<figure>
<img src="/2025/01/03/hugging-download/image-20250104221607364.png"
alt="image-20250104221607364" />
<figcaption aria-hidden="true">image-20250104221607364</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/02/stable-diffusion-history/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/02/stable-diffusion-history/" class="post-title-link" itemprop="url">stable-diffusion-history</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-02 18:59:15" itemprop="dateCreated datePublished" datetime="2025-01-02T18:59:15+08:00">2025-01-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-01-03 19:17:30" itemprop="dateModified" datetime="2025-01-03T19:17:30+08:00">2025-01-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191320002.png"
alt="image-20250103191320002" />
<figcaption aria-hidden="true">image-20250103191320002</figcaption>
</figure>
<h3 id="第一篇文章-2015">第一篇文章-2015</h3>
<p>关于diffusion的第一篇文章来自于 <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using
Nonequilibrium Thermodynamics</a> paper published in 2015.
受到非平衡统计物理的启发，引入了一个模型，将数据的结构破坏成t
steps。这个过程叫做forward diffusion;在此之后恢复叫做reverse
diffusion</p>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191354356.png"
alt="image-20250103191354356" />
<figcaption aria-hidden="true">image-20250103191354356</figcaption>
</figure>
<p>这个前向的diffusion过程来自于马尔可夫链，就是t时刻的结果只与t-1时刻的结果相关。</p>
<p>然后是逆像的diffusion过程，通过gan预测每一步的分布。</p>
<h3
id="图像生成再次引入diffusion-2020">图像生成再次引入diffusion-2020</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">Denoising Diffusion
Probabilistic Models</a> from 2020这篇文章取得了让人印象深刻的效果</p>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191420017.png"
alt="image-20250103191420017" />
<figcaption aria-hidden="true">image-20250103191420017</figcaption>
</figure>
<p>他们证实了reparameterization的技巧可以用在reverse
diffusion的过程中形成封闭的解决方案。</p>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191436862.png"
alt="image-20250103191436862" />
<figcaption aria-hidden="true">image-20250103191436862</figcaption>
</figure>
<p>并且发现预测每一步的噪音而不是噪音背后的图像得到更好的结果。在CIFAR10数据集上进行了测试，在IS和FID得分中都取得了sota.</p>
<p>重新引入diffusion为后面的midjourney和stable
diffusion都打下了基础。</p>
<h3 id="clip">CLIP</h3>
<p>OpenAi 发布了他们的论文 <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual
Models From Natural Language Supervision</a> and a <a
target="_blank" rel="noopener" href="https://openai.com/research/clip">blog post</a>和
开源的多模态的zero-shot CLIP模型。</p>
<p>这个模型在语言的监督下学习了视觉的特征，这不是一个新概念，但是它将text
prompt匹配图像做的很好。</p>
<p>它们抓取了互联网的超过400百万张图片，他们发现将text
prompt适配到每个任务更好，相比传统的分类模型来说。比如传统的分类模型打的标签就是类别dog，但是在text
prompt里面就是a photo of dog。</p>
<p>它们的预训练包括：</p>
<ul>
<li><p>基于transformer的text encoder</p></li>
<li><p>基于vision transformer的image encoder</p></li>
<li><p>基于resnet的image encoder</p></li>
</ul>
<p>输出的向量text encoder:Ti和Image encoder Ii
然后用在它们下一步的对比预训练过程中。</p>
<p>输入是</p>
<ul>
<li><p>一个prompt用于描述图像中的物体，如a photo of big
{object}</p></li>
<li><p>包含该物体的图像</p></li>
<li><figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191507830.png"
alt="image-20250103191507830" />
<figcaption aria-hidden="true">image-20250103191507830</figcaption>
</figure></li>
</ul>
<p>text和image encoder输出的向量构成text-image对，构成一个矩阵</p>
<ul>
<li><p>矩阵元素是一个image vector和text vector的乘积，<em>Ii *
Ti</em>.</p></li>
<li><p>矩阵对角元素是matching的text-image对</p></li>
<li><p>每行非对角元素是text和image不匹配对应的</p></li>
</ul>
<p>模型目标是最大化对角线元素的值，最小化非对角线元素值。这个过程输出的是对比性表示，捕捉的是图片和文本shared的特征，因此叫_Contrastive
Language-Image_ <em>Pre-training</em>.</p>
<p>这个模型输出的是image和text的embedding表示，这些embeddings可以通过cosine相似度或者欧几里得距离来衡量相似度，这些相似度可以用在很多任务中，比如GAN
模型中的判别器或图像分类器</p>
<h1
id="guided-language-to-image-diffusion-for-generation-and-editing-glide-2021年底">Guided
Language to Image Diffusion for Generation and Editing
(GLIDE)-2021年底</h1>
<p>OPENAI发表论文_<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10741">GLIDE:
Towards Photorealistic Image Generation and Editing with Text-Guided
Diffusion Models</a>_. 运用了guided diffusion和comparing CLIP guidance
vs *Classifier-free
guidance,允许扩散模型从文本中学习。在diffusion层中使用__unet__结构。用到text-image的__数据集__，这个数据集在DALL-E中也被使用。*</p>
<h5 id="guided-diffusion">Guided diffusion</h5>
<p>使用transformer model去encode text
prompt,将最后一层embedding的输出作为diffusion模型的class-conditioning，这个class-conditioning使用了attention去关注forward/backward/denising
attention层。</p>
<h5 id="clip-guidance">Clip guidance</h5>
<p>在推理过程中的去噪扩散过程中，使用clip进行分类起指导。用clip的点积对比矩阵梯度扰乱去噪均值，将初始去噪图像移动到
CLIP 预测高文本图像匹配的方向。类似于 Google 的 dreep deam 模型。</p>
<h5 id="classifier-free-guidance">Classifier-free guidance</h5>
<p>在训练过程中同时提供没有text
guidance的example，在推理的去噪阶段，让模型预测两次噪声，一次有prompt一次没有。text
guidance会影响预测的噪音值。有text的预测noise:(<em>εθ(xt|y)</em>),没有的(<em>εθ(xt|∅)</em>)。文本造成的不同在每个时间步t会乘以一个系数s,然后加到预测的噪音上。这样推着噪音预测向正确的方向进行。因为我们在每一步都考虑了更多文本特定的噪声，从而增强了文本添加的噪声效果。</p>
<h1 id="εˆθxty-εθxt-s-εθxty-εθxt"><em>εˆθ(xt|y) = εθ(xt|∅) + s ·
(εθ(xt|y) − εθ(xt|∅))</em></h1>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191611149.png"
alt="image-20250103191611149" />
<figcaption aria-hidden="true">image-20250103191611149</figcaption>
</figure>
<p>结果发现classfier-free guidance比clip guidance效果更好</p>
<p>最后他们训练diffusion用的256x256，因为扩散过程是其运行的像素空间中非常密集的过程。因此他们也训练了一个encoder和decoder用于降采样和复原图片到1024x1024。</p>
<h3 id="latent-diffusion-2021年底">Latent diffusion-2021年底</h3>
<p>论文 the _<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752">High-Resolution
Image Synthesis with Latent Diffusion Models</a>__，
主要将diffusion过程从像素空间移到了latent
space，使得diffusion过程更加的高效_</p>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191635342.png"
alt="image-20250103191635342" />
<figcaption aria-hidden="true">image-20250103191635342</figcaption>
</figure>
<p>Latent Diffusion
模型首先训练autoEncoder模型，encoder提取与原图最相似最重要的部分到latent
space，
decoder基于latent空间的表示生成原图。然后，将unet中的扩散过程用在pretrained
auto encoder编码器的latent space
representation.他们也发现classifier-free guidance提升质量。</p>
<p>UNet
还使用交叉注意力层，允许它们用作不同类型的条件输入生成器，例如文本到图像和超分辨率。</p>
<p>autoencoder的decoder层把latent space上采样恢复到1024x1024.</p>
<h3 id="dall-e2-2022年初">DALL-E2-2022年初</h3>
<p>OpenAI发布论文 _<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.06125">Hierarchical Text-Conditional
Image Generation with CLIP
Latents</a>__，这是一个__继承__工作来自于clip,
dall-e和glide.本文的作者确实将该模型称为 unclip，因为该模型将 CLIP
模型中的文本嵌入转换回图像。_</p>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191656226.png"
alt="image-20250103191656226" />
<figcaption aria-hidden="true">image-20250103191656226</figcaption>
</figure>
<p>可以注意到虚线下方，prior model输出一个clip image
embedding，然后用于diffusion decoder的去噪过程中的condition.</p>
<ol type="1">
<li><p>take一个预训练好的clip模型，冻结权重</p></li>
<li><p>将prompt通过clip encode成一个embedding向量</p></li>
<li><p>一个基于扩散的prior model将text embedding 匹配到相应的clip image
embedding.所以prior model产生clip model会产生的图像。</p></li>
<li><p>使用调整过的glide模型，他们通过去噪/反向扩散过程以随机方式映射图像嵌入。这使得模型能够生成许多与相似视觉概念相关的可能图像。</p></li>
</ol>
<p>为什么要用prior去产生image embedding</p>
<figure>
<img
src="/2025/01/02/stable-diffusion-history/image-20250103191713661.png"
alt="image-20250103191713661" />
<figcaption aria-hidden="true">image-20250103191713661</figcaption>
</figure>
<p>比起单纯的caption或者text embedding，prior产生的image
embedding能产生更好的结果，如上图。</p>
<p>但是openai没有开源模型的权重。</p>
<h3 id="开源的latent-diffusionstable-diffusion-2022">开源的latent
diffusion(stable diffusion)-2022</h3>
<p>开源模型stable diffusion由Stability AI发布</p>
<p>https://medium.com/<span class="citation"
data-cites="vasco-dev/history-and-literature-on-latent-stable-diffusion-dbca69fd54d5">@vasco-dev/history-and-literature-on-latent-stable-diffusion-dbca69fd54d5</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2024/12/31/new_laptop_settings/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/31/new_laptop_settings/" class="post-title-link" itemprop="url">新电脑配置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-31 17:41:52" itemprop="dateCreated datePublished" datetime="2024-12-31T17:41:52+08:00">2024-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-01-02 18:54:28" itemprop="dateModified" datetime="2025-01-02T18:54:28+08:00">2025-01-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="对象">对象</h2>
<p>Mac pro m4</p>
<h3 id="终端配置iterm2oh-my-zsh">1.
终端配置<strong>iterm2+oh-my-zsh</strong></h3>
<p>要自己安装<strong>command line
tools</strong>和home-brew(https://www.jianshu.com/p/e0471aa6672d)</p>
<p>https://github.com/sirius1024/iterm2-with-oh-my-zsh</p>
<h3 id="termius">2. Termius</h3>
<p>https://termius.com/download/macos</p>
<h3 id="typora">3.typora</h3>
<p><strong>https://github.com/shuhongfan/TyporaCrack</strong></p>
<h3 id="vpn">4. vpn</h3>
<p>可乐云：https://my.coke1.link/</p>
<p>另一个：https://apk.okzapp.app/index.php#/register?code=QIFC23KC
（朋友说用了三年多了没有跑路）</p>
<p>我之前用的是DuangCloud，2024下半年华丽嘎了</p>
<h3 id="其他问题">5. 其他问题</h3>
<h4 id="vpn开启全局代理之后终端无法安装包">5.1
vpn开启全局代理之后终端无法安装包</h4>
<p>设置：</p>
<figure>
<img src="/2024/12/31/new_laptop_settings/image-20250102095721650.png"
alt="image-20250102095721650.png" />
<figcaption aria-hidden="true">image-20250102095721650.png</figcaption>
</figure>
<p>现象：终端无法访问国外资源</p>
<figure>
<img src="/2024/12/31/new_laptop_settings/image-20250102103106455.png"
alt="image-20250102103106455.png" />
<figcaption aria-hidden="true">image-20250102103106455.png</figcaption>
</figure>
<p>解决方法：查看vpn代理端口并配置终端的代理端口：</p>
<figure>
<img src="/2024/12/31/new_laptop_settings/image-20250102103359294.png"
alt="image-20250102103359294.png" />
<figcaption aria-hidden="true">image-20250102103359294.png</figcaption>
</figure>
<figure>
<img src="/2024/12/31/new_laptop_settings/image-20250102103235915.png"
alt="image-20250102103235915" />
<figcaption aria-hidden="true">image-20250102103235915</figcaption>
</figure>
<p>以上参考链接：https://blog.csdn.net/silence_xz/article/details/136669658?spm=1001.2101.3001.6650.5&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-136669658-blog-133763842.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-136669658-blog-133763842.235%5Ev43%5Epc_blog_bottom_relevance_base1&amp;utm_relevant_index=10</p>
<h4 id="npm安装不动">5.2 npm安装不动</h4>
<p>现象：</p>
<figure>
<img src="/2024/12/31/new_laptop_settings/image-20250102103717638.png"
alt="image-20250102103717638" />
<figcaption aria-hidden="true">image-20250102103717638</figcaption>
</figure>
<p>解决方法：换源</p>
<p><code>npm config set registry http://registry.npmmirror.com</code></p>
<p>参考链接：http://liuw.tech/2022/11/05/hexo-%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%90%8E%E9%87%8D%E6%96%B0%E9%83%A8%E7%BD%B2hexo/</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2024/05/27/database/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/05/27/database/" class="post-title-link" itemprop="url">database</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-05-27 12:30:55" itemprop="dateCreated datePublished" datetime="2024-05-27T12:30:55+08:00">2024-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-05-30 01:20:58" itemprop="dateModified" datetime="2024-05-30T01:20:58+08:00">2024-05-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5
id="course-httpswww.coursera.orglearnintroduction-to-relational-databasesungradedwidgetp0c8phands-on-lab-relational-model-concepts">course
https://www.coursera.org/learn/introduction-to-relational-databases/ungradedWidget/p0C8P/hands-on-lab-relational-model-concepts</h5>
<h3 id="关系模型的基本概念">关系模型的基本概念</h3>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAxMTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvTGFiJTIwLSUyMFJlbGF0aW9uYWwlMjBNb2RlbCUyMENvbmNlcHRzL2luc3RydWN0aW9uYWwtbGFicy5tZCIsInRvb2xfdHlwZSI6Imluc3RydWN0aW9uYWwtbGFiIiwiYWRtaW4iOmZhbHNlLCJpYXQiOjE3MTE2Mzg2ODN9.WVq3A1Cs9GKG5RgO8AaYx3S3ZI3iAmLe899AkdPantM">Hands-on
Lab: Relational Model Concepts</a></p>
<figure>
<img src="/2024/05/27/database/image-20240527123715098.png"
alt="image-20240527123715098" />
<figcaption aria-hidden="true">image-20240527123715098</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://labs.cognitiveclass.ai/v2/tools/datasette?ulid=ulid-5cc4e56aa12f13251ff5e1bdda1219009104b784">load
data</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.coursera.org/learn/introduction-to-relational-databases/ungradedWidget/Yuntk/course-glossary">整体复习</a></p>
<p>https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAxMTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvZ2xvc3NhcnkvbTRfY291cnNlX2dsb3NzYXJ5Lm1kIiwidG9vbF90eXBlIjoiaW5zdHJ1Y3Rpb25hbC1sYWIiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTcxMTQyNTQ3NX0.o7HjxQ6FDaJx6wmdINArHesmkZFTbPxDdIgSbawnP5c</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2024/05/23/backend/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/05/23/backend/" class="post-title-link" itemprop="url">backend</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-05-23 14:00:15" itemprop="dateCreated datePublished" datetime="2024-05-23T14:00:15+08:00">2024-05-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-05-24 19:14:29" itemprop="dateModified" datetime="2024-05-24T19:14:29+08:00">2024-05-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="week-1">Week 1</h1>
<h4 id="node-js">1. Node js</h4>
<p>Node js是javascript在服务器端的编程框架， express是其开发框架。</p>
<p>server大概有三种：database server / web server(http) / application
server</p>
<p>js的特点：Event-driven, 异步asynchronous, 非阻塞，<a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9SZWFkaW5ncy9TZXJ2ZXJTaWRlSmF2YXNjcmlwdC5tZCIsInRvb2xfdHlwZSI6Imluc3RydWN0aW9uYWwtbGFiIiwiYWRtaW4iOmZhbHNlLCJpYXQiOjE3MTE0MjY1MTZ9.6yLbDHSnA7Ffyp8kq9u1jq8zNAs1QiwSi8fIG0quUxg">js了解</a></p>
<h5 id="js-module">js module</h5>
<figure>
<img src="/2024/05/23/backend/image-20240523143047893.png"
alt="image-20240523143047893" />
<figcaption aria-hidden="true">image-20240523143047893</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523143135653.png"
alt="image-20240523143135653" />
<figcaption aria-hidden="true">image-20240523143135653</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523143215979.png"
alt="image-20240523143215979" />
<figcaption aria-hidden="true">image-20240523143215979</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523143324541.png"
alt="image-20240523143324541" />
<figcaption aria-hidden="true">image-20240523143324541</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523143354049.png"
alt="image-20240523143354049" />
<figcaption aria-hidden="true">image-20240523143354049</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523144020865.png"
alt="image-20240523144020865" />
<figcaption aria-hidden="true">image-20240523144020865</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523144848936.png"
alt="image-20240523144848936" />
<figcaption aria-hidden="true">image-20240523144848936</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523145015691.png"
alt="image-20240523145015691" />
<figcaption aria-hidden="true">image-20240523145015691</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523145042861.png"
alt="image-20240523145042861" />
<figcaption aria-hidden="true">image-20240523145042861</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9SZWFkaW5ncy9BZHZhbmNlZF9Ob2RlLmpzX21vZHVsZXMubWQiLCJ0b29sX3R5cGUiOiJpbnN0cnVjdGlvbmFsLWxhYiIsImFkbWluIjpmYWxzZSwiaWF0IjoxNzExNDI2NTUzfQ.oUQg6uIJbbEgp0GByD36iHMr_gMmReyJYVykaSrHGbA">高级js模块</a></p>
<figure>
<img src="/2024/05/23/backend/image-20240523145912229.png"
alt="image-20240523145912229" />
<figcaption aria-hidden="true">image-20240523145912229</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9sYWJzJTJGTW9kdWxlMV9JbnRyb2R1Y3Rpb25Ub1NlcnZlclNpZGVKYXZhU2NyaXB0JTJGSGFuZHNPbl9MYWJfRmlyc3RTZXJ2ZXJTaWRlU2NyaXB0Lm1kIiwidG9vbF90eXBlIjoidGhlaWEiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTcxMTQyNjUyNH0.4cNsJQ7n8QzVT8x1BO0QhMW5jb7Kp_aXW3VILCc2h9Y">lab:使用nodejs创建服务端程序</a></p>
<figure>
<img src="/2024/05/23/backend/image-20240523152832502.png"
alt="image-20240523152832502" />
<figcaption aria-hidden="true">image-20240523152832502</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9HbG9zc2FyeS8yMDAzODkuMTFfTTFfR2xvc3NhcnkubWQiLCJ0b29sX3R5cGUiOiJpbnN0cnVjdGlvbmFsLWxhYiIsImFkbWluIjpmYWxzZSwiaWF0IjoxNzExNDI2NTQ0fQ.TuD3KX8yEEewo2CIDv2AyjGmWI1EMdClt8Tdw1m5MKY">一些概念</a></p>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9DaGVhdHNoZWV0cy8yMDAzODkuMjVfTTFfQ2hlYXRTaGVldC5tZCIsInRvb2xfdHlwZSI6Imluc3RydWN0aW9uYWwtbGFiIiwiYWRtaW4iOmZhbHNlLCJpYXQiOjE3MTE0MjY1MzV9.4M92HAb93JO5_DdTXT_jL2vJa-pfHrJ3Cxw2C_HASVE">服务器端js简介</a></p>
<h1 id="week-2">Week 2</h1>
<p>nodejs通过非阻塞的方式调用所有io</p>
<figure>
<img src="/2024/05/23/backend/image-20240523154546786.png"
alt="image-20240523154546786" />
<figcaption aria-hidden="true">image-20240523154546786</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523154705545.png"
alt="image-20240523154705545" />
<figcaption aria-hidden="true">image-20240523154705545</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240523185448246.png"
alt="image-20240523185448246" />
<figcaption aria-hidden="true">image-20240523185448246</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9SZWFkaW5ncy9JbnRyb2R1Y3Rpb25Ub0FzeW5jQXdhaXQubWQiLCJ0b29sX3R5cGUiOiJpbnN0cnVjdGlvbmFsLWxhYiIsImFkbWluIjpmYWxzZSwiaWF0IjoxNzExNDI2NTU3fQ.QLu1ThZK6P3pv6Q24iyROVj8RZZnSPyZzOIx3e7Kuz0">async/await</a></p>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9sYWJzJTJGTW9kdWxlMl9Bc3luY19DYWxsYmFjayUyRkhhbmRzT25fTGFiX0FzeW5jX0NhbGxiYWNrLm1kIiwidG9vbF90eXBlIjoidGhlaWEiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTcxMTQyNjUzMX0.qV1jLSpoScLWvGOZKXv95pbhkyzEOctJ9yOy5CscDQ4">异步回调编程</a></p>
<figure>
<img src="/2024/05/23/backend/image-20240524112320066.png"
alt="image-20240524112320066" />
<figcaption aria-hidden="true">image-20240524112320066</figcaption>
</figure>
<p><a href="">术语表 - 使用回调编程的异步 I/O</a></p>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9DaGVhdHNoZWV0cy8yMDAzODkuMjVfTTJfQ2hlYXRTaGVldC5tZCIsInRvb2xfdHlwZSI6Imluc3RydWN0aW9uYWwtbGFiIiwiYWRtaW4iOmZhbHNlLCJpYXQiOjE3MTE0MjY1Mzh9.JPvmEnhXrPQHLGLyZDW-7kzx2espu7krfqxdFJp9Ao0">Asynchronous
I/O with Callback Program</a></p>
<h1 id="express网络框架">Express网络框架</h1>
<p>node js不是一个运行框架，是一个runtime
environment用于在server上面运行js code</p>
<figure>
<img src="/2024/05/23/backend/image-20240524115011622.png"
alt="image-20240524115011622" />
<figcaption aria-hidden="true">image-20240524115011622</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240524115044107.png"
alt="image-20240524115044107" />
<figcaption aria-hidden="true">image-20240524115044107</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240524115606544.png"
alt="image-20240524115606544" />
<figcaption aria-hidden="true">image-20240524115606544</figcaption>
</figure>
<h4 id="中间件和路由器">中间件和路由器</h4>
<p>本文将讨论<em>中间件</em> 和<em>路由</em>这两个术语。</p>
<p>中间件是位于应用程序、数据库或服务之间的软件，允许这些不同的技术进行通信。它在分布式系统中为终端用户创建无缝交互。</p>
<p>Express
是一个消息传递框架，用于处理路由和编写中间件。应用程序的前端使用 Express
来促进后端组件之间的通信，而前端和后端服务无需使用相同的语言。前端与中间件通信，而不是直接与后端通信。</p>
<p>像 Express 这样的消息框架通常包含 JSON、REST API
和网络服务。旧的消息框架可能包含可扩展标记语言（XML）和简单对象访问协议（SOAP），而不是
JSON 和 REST
API。消息传递框架提供了一种处理不同应用间数据传输的标准化方法。</p>
<p>网络服务器是连接网站和数据库的中间件的一个例子。网络服务器处理业务逻辑，并根据请求从数据库路由数据。<em>路由</em>
是将 GET、POST 或 DELETE 等 HTTP 请求与 URL 以及处理该 URL
的函数相关联的代码部分。路由在网络开发中用于根据浏览器 URL
确定的规则分割应用程序的用户界面。</p>
<p>路由器功能统称为 "中间件"。中间件负责响应 HTTP
请求或调用中间件链中的其他函数。Express 通过 Router 类处理路由器函数，如
Router.get()。顾名思义，Router.get() 负责处理 HTTP GET
请求。其他路由器函数包括 Router.post()、Router.put() 和
Router.delete()，使用方法基本相同。这些方法需要两个参数，一个 URL
路径和一个回调函数。</p>
<p>除了路由之外，中间件还负责通过加密和解密数据来提供服务间的安全连接，通过将流量分配到不同的服务器来管理应用程序负载，以及在数据返回客户端之前对数据进行排序或过滤。</p>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9SZWFkaW5ncy9JbnRyb2R1Y3Rpb24tdG8tQXV0aGVudGljYXRpb24ubWQiLCJ0b29sX3R5cGUiOiJpbnN0cnVjdGlvbmFsLWxhYiIsImFkbWluIjpmYWxzZSwiaWF0IjoxNzExNDI2NTU1fQ.iKf3HQ4FD9ESfd0H1cH8R1eONRumBzDQAQYVyD3SO34">身份验证和授权</a></p>
<p><a
target="_blank" rel="noopener" href="https://d3c33hcgiwev3.cloudfront.net/L1T_uyeHSHGyROFLyqZRxQ_2ee09d6aa6ba4736920c2f6a31faa6f1_Writing-RESTful-APIs-Reading.docx.pdf?Expires=1716681600&amp;Signature=IJLmbTFs~gRZ3Ij9yrXrNVeNbu5kADjvREy0SkQt7eovyMZNnOX6YaZFa7Cs2fOVg4Fg4JOsSBxmsgho3a~kMxWsDVQGpDO79Ai2NjvK1Jv5~pe0z7FlTvajvO62pgLJFpBaegWY12EM5mjzVwzMHDILZrSNF7luz~2R3gb8-3s_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A">http
method/rest api</a></p>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9sYWJzJTJGTW9kdWxlM19FeHByZXNzSlMlMkZIYW5kcy1vbl9MYWJfQ1JVRC5tZCIsInRvb2xfdHlwZSI6InRoZWlhIiwiYWRtaW4iOmZhbHNlLCJpYXQiOjE3MTE0MjY1MjZ9.30Vfc7qrnD9jo_XAW4oiq9zJ1ax_0swsPwmRIL34JLk">lab:crud
with nodejs</a></p>
<figure>
<img src="/2024/05/23/backend/image-20240524174736995.png"
alt="image-20240524174736995" />
<figcaption aria-hidden="true">image-20240524174736995</figcaption>
</figure>
<figure>
<img src="/2024/05/23/backend/image-20240524174805639.png"
alt="image-20240524174805639" />
<figcaption aria-hidden="true">image-20240524174805639</figcaption>
</figure>
<p><a
target="_blank" rel="noopener" href="https://www.coursera.org/learn/developing-backend-apps-with-nodejs-and-express/ungradedWidget/IO1Oa/glossary-express-web-application-framework">express
web术语</a></p>
<p>test-project/ node_modules/ config/ db.js //Database connection and
configuration credentials.js //Passwords/API keys for external services
used by your app models/ //For mongoose schemas items.js prices.js
routes/ //All routes for different entities in different files items.js
prices.js app.js routes.js //Require all routes in this and then require
this file in package.json</p>
<p><a
target="_blank" rel="noopener" href="https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTURldmVsb3BlclNraWxsc05ldHdvcmstQ0QwMjIwRU4tU2tpbGxzTmV0d29yay9sYWJzJTJGUHJhY3RpY2VQcm9qZWN0X0ZyaWVuZHNMaXN0X1dpdGhBdXRoJTJGaW5zdHJ1Y3Rpb25zLm1kIiwidG9vbF90eXBlIjoidGhlaWEiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTcxMTQyNjUyM30.0Rshfz3KAokqUCOpGERC0zmSL26a7aKKCtJLaZIUsKc">practice
lab</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2024/05/20/frisbee-pickup/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/05/20/frisbee-pickup/" class="post-title-link" itemprop="url">frisbee-pickup</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-05-20 23:43:40" itemprop="dateCreated datePublished" datetime="2024-05-20T23:43:40+08:00">2024-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-05-21 00:17:26" itemprop="dateModified" datetime="2024-05-21T00:17:26+08:00">2024-05-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="当参加飞盘的比赛">当参加飞盘的比赛</h2>
<p>​
距离接触飞盘已经两年了，中间受伤的一年没有如何运动，我几乎踩中了很多不利的因素：</p>
<ul>
<li>错过最好的学校的大段时间训练和有场地、伙伴练习的机会，我玩飞盘已经是研究生快毕业的时候</li>
<li>北漂互联网，在北京参加飞盘活动路上时间花费很多</li>
<li>几乎没什么体育底子</li>
</ul>
<p>​ 为什么还要玩</p>
<ul>
<li>上手容易，以及飞盘的规则是no touch，真的很不喜欢人碰我</li>
<li>锻炼身体的动力</li>
</ul>
<p>​
我不是很厉害的选手，所以没有长期训练的队伍，很多时候都在打皮卡。花了很多时间，当了很多次炮灰得到的感悟：</p>
<ul>
<li>在皮卡的小比赛要珍惜每一次拿盘的机会，要去捡盘，一定要克服被打击而畏缩的惯性。我真的看到太多太多次，在皮卡，男生让女生不要捡盘，“让我捡”、“你不要碰”，而很多女生甚至视之为理所当然，“因为自己没有能力”，“会让队伍失分”，所以我们只能寄希望于好的队伍好的团体氛围。
<ul>
<li>1.皮卡的目的是自己的进步，如果用一个优化函数定义自己的目标，那就是在短短的场上几十分钟的时间内，将自己暴露在尽可能多的尝试和失败当中。这个游戏规则其实很简单，练习越多，进步越快。在皮卡中队伍的输赢本不应该成为一个阻碍，考虑也只是均衡双方力量让比赛尽可能有竞争力的进行下去。</li>
<li>2.能力不应该成为不控盘的理由。我多么希望自己能在早期就明白这个道理，在男生让我只是传dump给他的时候，在我即使捡盘也有近处一个男生让我直接给他的时候，在初次进入一个领域知道并坚持自己享有和其他人同等竞争的机会和权利，不要轻易放弃。</li>
</ul></li>
<li>参加一个比赛，跟着一个队伍“容易成为炮灰”，这种情况很多时候表示为：
<ul>
<li>喊你的时候对方理由一般是缺人，但是后面喊的人越来越多，越多的人数分摊有限的上场机会。尤其是认识的人，往往不好“问的太多”显得很难对付。</li>
<li>如果去参加一个皮卡的队伍，应该约定好你们准备招募多少个人，可以保证多少分的上场机会。（我在这方面当炮灰太多次了，因为没有队伍，因为处于弱势，所以我一度觉得这是我不得不接受的规则。直到后来我开始怀疑这个想法，如果我不接受这件事情就不会发生。我宁愿不参加比赛也不接受不平等的忽视和上场机会。不比赛依旧有很多方法可以去训练，依然可以进步。</li>
</ul></li>
<li>国内的很多比赛都是34
43ABBA的赛制轮回，但是实际比赛我却发现真正执行到底的比赛不多。很多时候都是：“我们女生不够”让对方队伍不好拒绝，<strong>其实这种情况的解决方案本应该是女生不够的队伍可以认输</strong>，事实上每一次妥协都是在牺牲女性盘友的上场机会。因为对“人数不够”的场景的默认，虽然有这样的规则，女盘手的缺失不成为一个问题。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2020/08/18/nuscenes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/18/nuscenes/" class="post-title-link" itemprop="url">nuscenes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-18 12:19:50" itemprop="dateCreated datePublished" datetime="2020-08-18T12:19:50+08:00">2020-08-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-05-20 23:24:11" itemprop="dateModified" datetime="2024-05-20T23:24:11+08:00">2024-05-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学习建议官方的devkit，由于nuscenes按照token索引数据，不建议自己读取jason文件，而是根据官方函数使用其接口。探索数据集用Mini数据集很方便。</p>
<h3 id="item名词">1.item名词</h3>
<p>nuscenes数据集把各个类型的数据分别组织成了一个table.比如scene车移动场景、sample关键帧、instance实例、category类别等等。对于每一个数据都有一个唯一的token来索引到这个数据。所以我们得到数据的过程就是根据官方提供的接口和函数找接口取数据的过程。具体的栏目有：</p>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 77%" />
</colgroup>
<thead>
<tr>
<th>Item</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scene</td>
<td>整个数据集一共1000个场景，Mini的有10个。每20s是一个场景</td>
</tr>
<tr>
<td>sample</td>
<td>An annotated snapshot of a scene at a particular timestamp</td>
</tr>
<tr>
<td>Sample_data</td>
<td>Data collected from a particular sensor.</td>
</tr>
<tr>
<td>Instance</td>
<td>Enumeration of all object instance we observed</td>
</tr>
<tr>
<td>Category</td>
<td>Taxonomy of object categories (e.g. vehicle, human).</td>
</tr>
<tr>
<td>Attribute</td>
<td>Property of an instance that can change while the category remains
the same.</td>
</tr>
<tr>
<td>Visibility</td>
<td>可见性</td>
</tr>
<tr>
<td>Sensor</td>
<td>nuscenes有6个camera,1个lidar,5个radar.我们主要做的detection任务只涉及到camera.所以接下来只会涉及到camera.</td>
</tr>
<tr>
<td>Calibrated sensor</td>
<td>Definition of a particular sensor as calibrated on a particular
vehicle.进行坐标转换的时候主要用到的数据，上面的sensor其实只有一些基本信息，没有坐标转换参数。所以要进行坐标转换工作主要使用这类数据</td>
</tr>
</tbody>
</table>
<h3 id="nuscenes涉及的几个坐标系">2. Nuscenes涉及的几个坐标系</h3>
<ul>
<li>global坐标系：一般的bbox，ego
pose都是在全局坐标系下给出的，可以理解为人为设定的一个坐标系，具体也不知道在哪儿。</li>
<li>ego坐标系：车体坐标系，注意calibrated
sensor相机的外参是相对车体给出的。</li>
<li>摄像机坐标系</li>
</ul>
<h3 id="scene">3. Scene</h3>
<p><code>nusc.list_scenes()</code> 方法列举出所有的scene。</p>
<p><code>nusc.scene</code> 返回列表，可以根据下标去索引具体的scene.</p>
<p><img src="/2020/08/18/nuscenes/图片1.png" /></p>
<h3 id="sample">4.Sample</h3>
<p>在scenes中，每半秒（2
HZ）标注一次数据就是sample。所以sample就是那些有标注的帧。附官方定义：各个传感器有自己的时间线，一致对上的就是关键帧。</p>
<p>•We define sample as an annotated keyframe of a scene at a given
timestamp. A keyframe is a frame where the time-stamps of data from all
the sensors should be very close to the time-stamp of the sample it
points to.</p>
<p><code>my_sample = nusc.get('sample',first_sample)</code></p>
<p><img src="/2020/08/18/nuscenes/图片2.png" /></p>
<p><code>nusc.list_scene</code> 列举出所有的场景</p>
<p><code>nusc.list_sample(my_sample_token["token"])</code> 列举出related
sample_data keyframes and sample_annotation associated with a sample</p>
<h3 id="sample_data">5. Sample_data</h3>
<p>从一个sample中具体取出哪路相机数据。</p>
<p><code>my_sample['data']</code> 列举出12个token,分别是6 camera 5 radar
1 lidar</p>
<p><img src="/2020/08/18/nuscenes/图片3.png" /></p>
<p>具体取出某个sensor的数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sensor = &#x27;CAM_FRONT&#x27;</span><br><span class="line">Cam_front_data = nusc.get(&#x27;sample_data&#x27;,my_sample[&#x27;data&#x27;][sensor])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/18/nuscenes/图片4.png" /></p>
<p>根据sample_data的token取出路径、annotationh和相机内参的接口，检测任务的dataloader很好用.这个函数tutorial没有给，源代码有。传入的第二个可见性的参数是关于bbox的，具体可看源代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_path,box_list,cam_intrinsic = nusc.get_sample_data(token,visibility)</span><br></pre></td></tr></table></figure>
<p>可视化某一个sample_data:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nusc.render_sample_data(cam_front_data[&#x27;token&#x27;])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/08/18/nuscenes/图片6.png" /></p>
<h3 id="sample_annotation">6. Sample_annotation</h3>
<p>sample_annotation refers to any bounding box defining the position of
an object seen in a sample. All location data is given with respect to
the global coordinate
system.注意注释数据的位置信息都是相对于全局坐标系给出的。</p>
<p><img src="nuscenes/图片5.png" style="zoom:75%;" /></p>
<p>可视化sample_annotation:</p>
<p><img src="/2020/08/18/nuscenes/图片12.png" /></p>
<h3 id="instance">7.instance</h3>
<p>Object instance are instances that need to be detected or tracked by
an AV.</p>
<p><code>nusc.instance</code> 返回instance的列表，可以通过下标索引。</p>
<h3 id="category">8. category</h3>
<p><code>nusc.list_categories()</code></p>
<p><img src="/2020/08/18/nuscenes/图片8.png" /></p>
<h3 id="attribute">9. attribute</h3>
<p>An attribute is a property of an instance that may change throughout
different parts of a scene while the category remains the same.</p>
<p><img src="/2020/08/18/nuscenes/图片9.png" /></p>
<h3 id="visibility">10. visibility</h3>
<p>visibility可视化及描述：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">anntoken=<span class="string">&#x27;a7d0722bce164f88adf03ada491ea0ba&#x27;</span></span><br><span class="line">visibility_token = nusc.get(<span class="string">&#x27;sample_annotation&#x27;</span>,anntoken)[<span class="string">&#x27;visibility_token&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Visibility:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(nusc.get(<span class="string">&#x27;visibility&#x27;</span>,visibility_token)))</span><br><span class="line">nusc.render_annotation(anntoken)</span><br></pre></td></tr></table></figure>
<p>Visibility: {'description': 'visibility of whole object is between 80
and 100%', 'token': '4', 'level': 'v80-100'}</p>
<p><img src="/2020/08/18/nuscenes/图片10.png" /></p>
<h3 id="sensor">11. sensor</h3>
<p>1 lidar, 5 radar, 6 cameras</p>
<h3 id="calibrated_sensor">12. Calibrated_sensor</h3>
<p>calibrated_sensor consists of the definition of a particular sensor
(lidar/radar/camera) as calibrated on a particular vehicle.
注意外参是相对<strong>ego vehicle body frame</strong>得到的。</p>
<p><img src="/2020/08/18/nuscenes/图片14.png" /></p>
<p>3D bounding box坐标：X points forward, Y to the left, Z up</p>
<p>理解“with respect to ego vehicle body
frame”，就是说我相机坐标系中的坐标转换到ego vehicle，就是</p>
<p><span class="math display">\[(\left[ \begin{array}{l} x_{ego}\\\\
y_{ego}\\\\ z_{ego} \end{array} \right] = R\left[ \begin{array}{l}
x_{camera}\\\\ y_{camera}\\\\ z_{camera} \end{array} \right] +
T)\]</span></p>
<p>相应的把ego vehicle坐标系下的bbox转换到sensor coord system就是：</p>
<p><img src="/2020/08/18/nuscenes/图片15.png" /></p>
<h3 id="ego_pose">13. Ego_pose</h3>
<p>go_pose contains information about the location (encoded in
translation) and the orientation (encoded in rotation) of the ego
vehicle, with respect to the <strong>global coordinate
system</strong>.</p>
<p>ego_pose的数量和sample_data是一样的，这两者是一一对应的关系。<u>其实我有点不懂为什么每一个sample_data对应有一个ego_pose，六路相机也就是一个sample不应该对应一个ego
pose吗</u></p>
<p>•描述的是车体自己（x forward,y left,z
up）相对于全局坐标系的旋转R和平移T。这个R通过四元数给出。</p>
<p>•理解ego pose的R和T都是相对于（with respect
to）全局坐标系这句话;也就是ego坐标系中的一个坐标要变换到全局坐标系中就是</p>
<p><span class="math inline">\(\left[ \begin{array}{l} x_{global}\\\\
y_{global}\\\\ z_{global} \end{array} \right] = R\left[ \begin{array}{l}
x_{ego}\\\\ y_{ego}\\\\ z_{ego} \end{array} \right] + T\)</span></p>
<p>其中公式左边是全局坐标系中的坐标。</p>
<h3
id="nuscenes评测只涉及detection方面">14.nuscenes评测（只涉及detection方面）</h3>
<p>•Nuscenes的评测程序可以在devkit中的eval找到。</p>
<p>•README中规定了相应的sample_result的格式。prediction的结果要按照规定格式组织jason文件。sample_result的表和sample_annotation的表设计为一样的，所以完全可以用sample_annotation的工具和接口去操作sample_result.同时注意生成的结果即使没有识别出框，必须存在相应的token.</p>
<p>•虽然nuscenes训练有23类，但是评测处理similar和rare的类别，考虑到每一类的frequency,最终只归纳了10类，并且每一类有个detection
range(meters)。超过这个距离识别出来的框忽略。</p>
<p><a
target="_blank" rel="noopener" href="https://www.nuscenes.org/nuscenes#data-annotation">这个链接可以看到数据集各个类别的频率</a></p>
<h5 id="预处理">1.预处理</h5>
<p>GT和predition bboxes:</p>
<p>•超过距离的移除；</p>
<p>•Bikes和motorcycle在bike-rack里面的没有被注释也忽略</p>
<p>GT bboxes:</p>
<p>•没有Lidar和radar点的被移除，不能确保在当前frame是可见的</p>
<h5 id="评测矩阵">2.评测矩阵</h5>
<p>mAP:
一般的物体检测选择IOU来做匹配标准和评价阈值，Nuscenes选取的是xy平面bbox距离ego的距离（也就是深度距离）来作为标准，选择最小的距离匹配。选取{0.5,1,2,4}作为距离阈值（相当于原来的IOU@0.3,0.5,0.7），计算AP的时候只选取precision和recall&gt;0.1的点进行计算，当然由于没有计算&lt;0.1的部分会做一个归一化的操作。</p>
<p>TP metircs：其他因素的误差矩阵。Average of translation, velocity,
scale, orientation and attribute errors.</p>
<p>NDS(nuscenes detection score): the weighted sum of the
above。评价总分，mAP占50%，TP中的五类误差各占权重10%。其中TP
metrics统计的是误差值，到衡量得分存在一个转化关系：<code>TP score = max(1-TP_error,0.0)</code></p>
<h5 id="true-positive-metrics">3.True Positive metrics</h5>
<p>•TP衡量translation/scale/orientation/velocity/attribute errors</p>
<p>•所以TP矩阵计算匹配时都要满足center distance在2m之类</p>
<p>•最终计算平均在recall&gt;10%，达不到这个阈值某类的TP
errors所有都会被设置为1.</p>
<p>•Average Translation
error(ATE):bbox的中心到ego原点的L2范数(meters)。即公式</p>
<p><span class="math inline">\(\sqrt{ {(x1 - x2)^2} + {(y1 - y2)^2} +
{(z1 - z2)^2} }\)</span></p>
<p>. 区分于上面计算mAP作为阈值的distance:<span
class="math inline">\(\sqrt{(z1-z2)^2}\)</span></p>
<p>•Average Scale Error(ASE): 对齐中心和朝向之后计算1-IOU</p>
<p>•Average Orientation Error(AOE):最小的yaw
angle差异（radians）.一般类都是360度衡量，barriers在180度衡量，cones忽略这个指标。</p>
<p>•Average Velocity Error(AVE):absolute velocity error in
m/s.barrier和cone忽略这个指标。</p>
<p>•Average Attribute
Error(AAE):1-acc.acc是属性分类正确率。Barrier和cone忽略这个指标</p>
<p>•TP矩阵每个类分别计算，对于所有类别取均值得到Map,mASE,mAOE,mAVE,mAAE.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">64</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
