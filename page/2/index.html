<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Garfield&#39;s blog">
<meta property="og:url" content="http://77philosophia.github.io/page/2/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://77philosophia.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/05/07/serl-family/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/07/serl-family/" class="post-title-link" itemprop="url">serl-family</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-07 10:23:26" itemprop="dateCreated datePublished" datetime="2025-05-07T10:23:26+08:00">2025-05-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-12 14:21:05" itemprop="dateModified" datetime="2025-05-12T14:21:05+08:00">2025-05-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="serl-a-software-suite-for-sample-efficient-robotic-reinforcement-learning">SERL:
A Software Suite for Sample-Efficient Robotic Reinforcement
Learning</h1>
<blockquote>
<p>[!NOTE]</p>
<p>总结风格：从整体结构有个outline的感觉，每一块涉及到的具体的算法细节和推导看相关论文和代码深入。</p>
</blockquote>
<h2 id="url">URL</h2>
<p>https://arxiv.org/pdf/2401.16013</p>
<p>https://serl-robot.github.io/</p>
<p>https://github.com/rail-berkeley/serl</p>
<h2 id="tldr">TL;DR</h2>
<p>2024年1月份的文章，介绍了一个名为SERL的开源软件框架，旨在降低机器人强化学习(Robotic
Reinforcement Learning, Robotic
RL)在现实世界中的应用门槛。作者指出，尽管Robotic
RL在近年来取得了显著进展，但在实际应用中仍然存在许多挑战，特别是算法的实现细节对性能的影响往往非常大，甚至超过算法本身的选择。SERL的目标是提供一个高质量、易于使用的工具，包含样本高效的离线强化学习方法、奖励函数计算方法、环境重置自动化方法、高质量的机器人控制器以及一系列具有挑战性的示例任务。</p>
<blockquote>
<p>[!NOTE]</p>
<p>1.提供了一个可以用在真实环境的开源的强化学习框架，综合了一套学习策略，算法细节。-》实践落地的work</p>
<p>2.可扩展性也很好，不跟一个具体的robot硬件平台深绑定，鼓励在此基础上做自己的开发。</p>
</blockquote>
<h2 id="show-case">Show case</h2>
<figure>
<img src="/2025/05/07/serl-family/image-20250507171443384.png"
alt="image-20250507171443384" />
<figcaption aria-hidden="true">image-20250507171443384</figcaption>
</figure>
<h4 id="pcb">pcb:</h4>
<p>在 PCB
板上装配穿孔元件是一项常见却又充满挑战的机器人任务。电子元件的引脚极易弯曲，而孔位与引脚之间的公差非常小，要求机器人在装配时既要精准又要轻柔。通过仅仅
21 分钟的自主学习，SERL 使机器人达到了 100%
的任务完成率。即便面临如电路板位置移动或视线部分被遮挡等未知的干扰，机器人也能稳定完成装配工作。(在执行电路板元件安装任务时，机器人能够应对在训练阶段未曾遇到的各种干扰，顺利完成任务。)</p>
<figure>
<img src="/2025/05/07/serl-family/serl-pcb.gif" alt="pcb" />
<figcaption aria-hidden="true">pcb</figcaption>
</figure>
<p><img src="/2025/05/07/serl-family/pcb2.gif" /></p>
<p><img src="/2025/05/07/serl-family/pcb3.gif" /></p>
<h4 id="电缆布线">电缆布线</h4>
<p>在许多机械和电子设备的组装过程中，我们需要将电缆沿着特定的路径精确地安装到位，这一任务对精度和适应性提出了很高的要求。由于柔性电缆在布线过程中容易产生形变，而且布线过程可能会受到各种干扰，比如电缆被意外移动或者夹持器位置的变化，这导致使用传统的非学习型方法难以应对。SERL
能够在短短 30 分钟内实现 100%
的成功率。即便是在夹持器位置与训练期间不同时，机器人也能够泛化其学习到的技能，适应新的布线挑战，确保布线工作的正确执行。(机器人无需更多的专项训练也能直接把线缆穿过与训练时位置不一样的夹子里。)</p>
<p><img src="/2025/05/07/serl-family/cab1.gif" /></p>
<p><img src="/2025/05/07/serl-family/cab2.gif" /></p>
<p><img src="/2025/05/07/serl-family/cab3.gif" /></p>
<h4 id="物体抓取">物体抓取：</h4>
<p>在仓库管理或零售业中，机器人经常需要将物品从一个地方移动到另一个地方，这要求机器人能够识别并搬运特定的物品。强化学习的训练过程中，很难对欠驱动的物体进行自动的归位重置。利用
SERL 的无重置强化学习功能，机器人在 1 小时 45 分钟内同时学习两个 100/100
成功率的策略。用前向策略把物体从 A 箱放到 B 箱，再用后向策略把物体从 B
箱归为回 A 箱。(SERL
训练了两套策略，一个把物体从右边搬运到左边，一个从左边放回右边。机器人不仅在训练物体上达到
100% 成功率，就连没见过的物体也能智能搬运。)</p>
<p><img src="/2025/05/07/serl-family/grab1.gif" /></p>
<p><img src="/2025/05/07/serl-family/grab2.gif" /></p>
<p><img src="/2025/05/07/serl-family/grab3.gif" /></p>
<h2 id="modelmethod">Model&amp;Method</h2>
<h4 id="问题定义和拆解">1. 问题定义和拆解</h4>
<p>机器人的强化学习任务可以定义为一个马尔可夫决策过程：</p>
<figure>
<img src="/2025/05/07/serl-family/image-20250507171917083.png"
alt="image-20250507171917083" />
<figcaption aria-hidden="true">image-20250507171917083</figcaption>
</figure>
<blockquote>
<p>[!TIP]</p>
<p>几个字母的代表含义</p>
</blockquote>
<p>难点要考虑：</p>
<ol type="1">
<li><p>sample efficiency</p></li>
<li><p>实际场景中为了进行策略学习可能有一些难以定义的场景，比如reward
function可能依赖于视觉的观察。每次episodic task需要自动化重置。</p></li>
<li><p>Controller：一个是要精准了（从仿真-&gt;实际环境），一个是要安全</p></li>
</ol>
<h4 id="核心的算法">2.核心的算法</h4>
<h5 id="rlpd算法">2.1 RLPD算法</h5>
<p>定义了强化学习策略的损失函数、采样过程、用了layer normalization</p>
<h5 id="reward-function">2.2 reward function</h5>
<p>用了分类器判断任务是否成功（论文提到有很多种训练方法，训练一个分类器，训练gan等等，可以自由地做一些探索）</p>
<h5 id="reset-free">2.3 reset free</h5>
<p>简而言之，省去人工重置，设计了一个forward
agent执行任务，还有一个backward agent重置任务。</p>
<h4 id="软件结构">3. 软件结构</h4>
<figure>
<img src="/2025/05/07/serl-family/image-20250507173132208.png"
alt="image-20250507173132208" />
<figcaption aria-hidden="true">image-20250507173132208</figcaption>
</figure>
<h4 id="其他">4. 其他</h4>
<h5 id="控制器的设计">4.1 控制器的设计</h5>
<p>简而言之：末端控制器对任务成功率的影响很大，paper里面用了分层结构，两层控制器。不用的频率。</p>
<figure>
<img src="/2025/05/07/serl-family/image-20250507173637912.png"
alt="image-20250507173637912" />
<figcaption aria-hidden="true">image-20250507173637912</figcaption>
</figure>
<h5 id="相对坐标系作observation和action">4.2
相对坐标系作observation和action</h5>
<p>免去定义世界坐标系，以末端夹具作为起点，描述下一帧相对上一帧的相对坐标。</p>
<blockquote>
<p>[!TODO]</p>
<p>整体性的框架的一个自上而下的俯视。但是要了解具体的：</p>
<p>Todo: 1. 各个环节算法 2.代码</p>
<p>可以放到后面的分享当中。</p>
</blockquote>
<h1 id="hil-serl">hil-serl</h1>
<h2 id="url-1">URL</h2>
<p>项目主页：https://hil-serl.github.io/</p>
<p>Paper:https://hil-serl.github.io/static/hil-serl-paper.pdf</p>
<p>Code: https://github.com/rail-berkeley/hil-serl</p>
<h2 id="tldr-1">TL；DR</h2>
<p>验证了经过精细的系统设计，RL算法可以解决一系列灵巧复杂的基于视觉的manipulation任务；</p>
<p>第一个实现双臂的、image inputs、RL在真实世界处理类似组装timing
belt的任务。</p>
<blockquote>
<p>[!NOTE]</p>
<p>和serl不同的：</p>
<p>1.加入了人类的修正（learn form mistakes）</p>
<p>2.serl关注简单的任务，也没有涉及双臂的操作。hil-serl聚焦基于视觉的更复杂的manipulation.</p>
</blockquote>
<h2 id="methods">Methods</h2>
<h3 id="问题拆解和定义">1.问题拆解和定义</h3>
<figure>
<img src="/2025/05/07/serl-family/image-20250508113632106.png"
alt="image-20250508113632106" />
<figcaption aria-hidden="true">image-20250508113632106</figcaption>
</figure>
<p>涉及到的关键点是observation space S和action
space的选择，比如cameras的组合，本体的状态，相应的robot底层控制器的选择。</p>
<p>针对paper里面的任务：</p>
<p>1.作者的reward
function选择了一种稀疏的function：二分类，决定任务是否成功；</p>
<p>2.RLPD算法（sample
effenciency和整合先验数据进行训练的能力）：每次训练从prior
data和on-policy data各采一半数据。</p>
<h3 id="系统结构">2.系统结构</h3>
<ul>
<li><p>Actor process：在robot上执行policy,把数据发送给replay
buffer</p></li>
<li><p>learner process:</p></li>
<li><p>Replay buffers（2）: demo buffer存储离线的人类演示；RL
buffer存储on-policy data</p></li>
<li><p>environment:处理各种环境输入：cameras,鼠标操作、各种控制器。</p>
<p>Learner
process从两个buffers里面采样，通过RLPD算法更新策略，阶段性将策略发送给actor
process去执行。</p></li>
</ul>
<figure>
<img src="/2025/05/07/serl-family/image-20250508114359129.png"
alt="image-20250508114359129" />
<figcaption aria-hidden="true">image-20250508114359129</figcaption>
</figure>
<h3 id="系统设计">3. 系统设计</h3>
<ul>
<li>pretrained vision backbones: 对输入的图像进行encode</li>
<li>reward function:
训练了一个二分类的分类器,任务完成给奖励1，其它step奖励为0。</li>
<li>下游的robotic system:
一个是用ego-centric的坐标系描述；另外安全性方面控制器阻抗控制 +
限幅，开环控制已经足够。</li>
<li>Gripper control: 增加了一个critic
network对夹爪的动作进行离散化（open, close, stay）.简而言之，训练两个MDP
task，一个action空间用连续的action space, 一个用离散化的action
space。根据训练方法（DQN），推理的时候从M1中取出连续的actions描述，然后从critic
network里面查询离散化的M2的action.</li>
</ul>
<h3 id="人在回路的强化学习">4. 人在回路的强化学习</h3>
<h4
id="rl的样本复杂度sample-complexity">1.<strong>RL的样本复杂度（Sample
Complexity）</strong></h4>
<ul>
<li><strong>定义</strong>：学习最优策略所需的环境交互样本量，受以下因素影响：
<ol type="1">
<li><strong>状态/动作空间的维度</strong>（Cardinality of state/action
spaces）：
<ul>
<li>高维空间（如机械臂的连续状态+多传感器输入）需要更多样本覆盖。</li>
</ul></li>
<li><strong>任务时间跨度</strong>（Task horizon）：
<ul>
<li>长周期任务（如多步骤装配）需更长的轨迹探索。</li>
</ul></li>
<li><strong>探索策略</strong>（Exploration policy）：
<ul>
<li>不充分的探索会导致策略陷入局部最优。</li>
</ul></li>
</ol></li>
</ul>
<h4 id="解决方案human-in-the-loop-hitl">2.解决方案：Human-in-the-Loop
(HITL)**</h4>
<p>核心思想</p>
<ul>
<li><strong>人类干预加速学习</strong>：
通过人类提供实时反馈（如动作修正、奖励调整、示范数据），<strong>直接引导探索方向</strong>，减少无效样本。</li>
<li><strong>理论依据</strong>：
人类先验知识可缩小策略搜索空间，降低样本复杂度。</li>
</ul>
<ol start="3" type="1">
<li><p>在看一次这张图，人可以在t0到tN的中间时刻干预，用人类干预的action替换robot
policy的action.把数据同时存储在两个buffers中，但是策略转换只存储在RL
buffers中。</p>
<p>简而言之：robot表现差的时候人工就干预；</p>
<p>实验发现，开始阶段人工频繁干预，后面可以干预的越来越少。</p></li>
</ol>
<figure>
<img src="/2025/05/07/serl-family/image-20250508114359129.png"
alt="image-20250508114359129" />
<figcaption aria-hidden="true">image-20250508114359129</figcaption>
</figure>
<h4 id="训练过程">5. 训练过程</h4>
<figure>
<img src="/2025/05/07/serl-family/image-20250508140805366.png"
alt="image-20250508140805366" />
<figcaption aria-hidden="true">image-20250508140805366</figcaption>
</figure>
<ul>
<li>相机的选择：wrist cameras(基于ego-centric views的选择)+side
cameras(拍摄到整个ROI)</li>
<li>训练reward classifier: 10 trajectories x 10s/条</li>
<li>准备offline demo replay buffer： 20～30trajectories</li>
<li>训练中注意人工干预不要过度，避免过拟合</li>
</ul>
<h2 id="results">Results</h2>
<figure>
<img src="/2025/05/07/serl-family/image-20250508142437272.png"
alt="image-20250508142437272" />
<figcaption aria-hidden="true">image-20250508142437272</figcaption>
</figure>
<h5 id="任务">1. 任务</h5>
<figure>
<img src="/2025/05/07/serl-family/image-20250508142453433.png"
alt="image-20250508142453433" />
<figcaption aria-hidden="true">image-20250508142453433</figcaption>
</figure>
<ul>
<li>主板组装任务：内存插入、SSD组装、USB插入、电缆夹紧</li>
<li>宜家组装任务：侧面板组装、上面板组装</li>
<li>汽车仪表器组装、物体传递、履带组装、叠叠乐抽取、物体翻转</li>
</ul>
<h5 id="findings">2. findings</h5>
<ul>
<li><p>HIL-SERL在几乎所有任务中，在1到2.5小时的真实世界训练中实现了100%的成功率，完成任务时间也更短。</p></li>
<li><p>从零开始的RL，没有任何演示或纠正，在所有任务上成功率为0%</p></li>
<li><p>为了验证在线人类纠正的重要性，作者将SERL的离线缓冲区中的演示数量增加了十倍，从通常的20增加到200
(HIL-SERL no
itv)。然而，没有任何在线纠正，这种方法的成功率显著低于HIL-SERL，包括在复杂任务如汽车仪表板组装中的完全失败（0%成功率）。这证实了在线纠正在促进策略学习中的关键作用。这些结果证实了离线演示和策略学习指导中在策略学习中的关键作用，尤其是对于需要持续反应行为的复杂操作任务。</p></li>
</ul>
<p>Todo:</p>
<ol type="1">
<li>从hil-serl迁移到配天机器人上的总结工作</li>
<li>中间具体的算法细节</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/05/06/math-diffusion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/06/math-diffusion/" class="post-title-link" itemprop="url">math_diffusion</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-05-06 18:53:18" itemprop="dateCreated datePublished" datetime="2025-05-06T18:53:18+08:00">2025-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-07 09:49:19" itemprop="dateModified" datetime="2025-05-07T09:49:19+08:00">2025-05-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学习链接：</p>
<p>《Mathematical Foundation of Diffusion Generative Models》</p>
<p>https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/foundation-diffusion-generative-models</p>
<h2 id="background">background</h2>
<p>Diffusion generative
models是一种从distributions当中采样的方法，与其他的,比如GANS,
VAEs有很大的不同。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/05/06/http-proxy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/06/http-proxy/" class="post-title-link" itemprop="url">http_proxy</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-05-06 18:09:56 / Modified: 18:31:14" itemprop="dateCreated datePublished" datetime="2025-05-06T18:09:56+08:00">2025-05-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="代理软件测试链路畅通但是终端浏览器始终无法访问外网">代理软件测试链路畅通但是终端/浏览器始终无法访问外网</h1>
<p>测试：在终端临时设置http_proxy,https_proxy,all_proxy之后可以访问google</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">env | grep -i proxy //发现输出为空，设置的proxy没有在终端生效</span><br><span class="line">curl https://www.google.com //卡顿，不出意外也是无法访问的</span><br><span class="line"></span><br><span class="line">export http_proxy=http://127.0.0.1:7890 </span><br><span class="line">export https_proxy=http://127.0.0.1:7890</span><br><span class="line">export all_proxy=socks5://127.0.0.1:7891</span><br><span class="line">// 终端生效</span><br><span class="line"></span><br><span class="line">curl https://www.google.com //success，返回html页面</span><br><span class="line"></span><br><span class="line">unset http_proxy https_proxy all_proxy HTTP_PROXY HTTPS_PROXY ALL_PROXY //取消proxy设置重新访问google无法访问</span><br></pre></td></tr></table></figure>
<p>验证猜想之后修改配置文件让一直生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unset... //防止是终端设置一次生效</span><br><span class="line">env xxx // 确认没有了</span><br><span class="line">echo xxx ~/.zshrc //修改配置文件</span><br><span class="line">source xxx</span><br><span class="line"></span><br><span class="line">env xxx // 确认proxy生效</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/06/http-proxy/image-20250506181813284.png"
alt="image-20250506181813284" />
<figcaption aria-hidden="true">image-20250506181813284</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/04/29/linux-settings/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/29/linux-settings/" class="post-title-link" itemprop="url">linux-settings</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-04-29 10:16:52" itemprop="dateCreated datePublished" datetime="2025-04-29T10:16:52+08:00">2025-04-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-06 10:24:41" itemprop="dateModified" datetime="2025-05-06T10:24:41+08:00">2025-05-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="linux开发环境配置">linux开发环境配置</h1>
<p>1.安装ubuntu</p>
<p>2.安装Nvidia driver</p>
<p>3.zerotier用于远程连接(同时安装向日葵以防万一)</p>
<p>4.更换shell为zsh，安装oh-my-zsh,配置高亮、自动补全、autojump等的插件；将主题更改为agnoster;</p>
<p>https://www.cnblogs.com/chencarl/p/16824387.html</p>
<p>为了展示 <strong>Agnoster</strong> 主题提示符里的三角形，需要
<strong>Powerline</strong> 字体库的支持。</p>
<p>https://zhuanlan.zhihu.com/p/62419420</p>
<p>5.tmux进行管理</p>
<p>6.安装miniconda并把路径加入到zshrc的配置文件里。</p>
<figure>
<img src="/2025/04/29/linux-settings/image-20250429143912288.png"
alt="image-20250429143912288" />
<figcaption aria-hidden="true">image-20250429143912288</figcaption>
</figure>
<p>https://www.cnblogs.com/xingnie/p/16269190.html</p>
<p>完了要执行conda init，简单的让conda配置在zshrc的方法（有时候conda
init默认更新了bashrc，不用拷贝设置到zshrc）</p>
<figure>
<img src="/2025/04/29/linux-settings/image-20250429145219288.png"
alt="image-20250429145219288" />
<figcaption aria-hidden="true">image-20250429145219288</figcaption>
</figure>
<p>Conda init zsh即可(因为我已经执行过了，所以是no change)</p>
<figure>
<img src="/2025/04/29/linux-settings/image-20250429145236438.png"
alt="image-20250429145236438" />
<figcaption aria-hidden="true">image-20250429145236438</figcaption>
</figure>
<p>7.远程服务器免密码连接：</p>
<p>7.1 复制本地公钥文件到远程服务器</p>
<p>先看.ssh文件夹下面是否有生成好的密钥文件</p>
<p>ssh-copy-id -i ~/.ssh/id_ed25519.pub username@ip</p>
<p>此时可以通过ssh username@ip直接登陆不需要输入密码。</p>
<p>7.2 通过<strong><code>~/.ssh/config</code></strong> 简化连接</p>
<p>nano ~/.ssh/config</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Host server1</span><br><span class="line">    HostName 192.168.1.100</span><br><span class="line">    User root</span><br><span class="line">    IdentityFile ~/.ssh/id_ed25519</span><br><span class="line"></span><br><span class="line">Host server2</span><br><span class="line">    HostName example.com</span><br><span class="line">    User admin</span><br><span class="line">    IdentityFile ~/.ssh/id_ed25519</span><br><span class="line">    Port 2222  # 如果SSH端口不是默认的22</span><br></pre></td></tr></table></figure>
<p>之后就可以直接用别名登录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh server1  # 代替 ssh root@192.168.1.100</span><br><span class="line">ssh server2  # 代替 ssh -p 2222 admin@example.com</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/04/28/macos-install-zerotier-cannot-launch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/28/macos-install-zerotier-cannot-launch/" class="post-title-link" itemprop="url">macos-install-zerotier-cannot-launch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-04-28 18:08:46 / Modified: 18:14:13" itemprop="dateCreated datePublished" datetime="2025-04-28T18:08:46+08:00">2025-04-28</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="macos-pro安装zero-tier却无法打开">macOS
pro安装zero-tier却无法打开</h1>
<p>https://gist.github.com/Paraphraser/3c01b5f2c9e1f298751d7c255773a1cd</p>
<figure>
<img
src="/2025/04/28/macos-install-zerotier-cannot-launch/image-20250428181123099.png"
alt="image-20250428181123099" />
<figcaption aria-hidden="true">image-20250428181123099</figcaption>
</figure>
<figure>
<img
src="/2025/04/28/macos-install-zerotier-cannot-launch/image-20250428181152069.png"
alt="image-20250428181152069" />
<figcaption aria-hidden="true">image-20250428181152069</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/04/28/ubuntu-nvidia-driver-noNetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/28/ubuntu-nvidia-driver-noNetwork/" class="post-title-link" itemprop="url">ubuntu-nvidia-driver-noNetwork</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-04-28 17:56:37 / Modified: 18:08:15" itemprop="dateCreated datePublished" datetime="2025-04-28T17:56:37+08:00">2025-04-28</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="ubuntu安装nvidia显卡驱动之后没有network">ubuntu安装Nvidia显卡驱动之后没有network</h1>
<p>https://blog.csdn.net/weixin_44286126/article/details/131455726</p>
<p>排查思路：</p>
<p>1.ubuntu会保留旧的镜像，可以在引导模式(出现图标esc)切换到旧镜像，保持网络连接。然后dpkg
查看新旧镜像是否有安装包的差异。</p>
<ol start="2" type="1">
<li>原因是安装ubuntu显卡驱动的时候把内核部分包也升级了。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/04/27/VLM-VLA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/27/VLM-VLA/" class="post-title-link" itemprop="url">VLM-VLA</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-04-27 17:13:02 / Modified: 17:14:04" itemprop="dateCreated datePublished" datetime="2025-04-27T17:13:02+08:00">2025-04-27</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>大致回顾了一下VLM的concept和结构，大概就是对image和text进行向量空间表示，通过contrastive
learning让同一个事物的二者特征在embedding空间里相近。</p>
<p>just like 哲学就是哲学家的历史，从paper publication
history看一下vla/vlm的区别和发展。don't be fooled by 高大上的名词。</p>
<h1 id="vlm"><strong>VLM</strong></h1>
<h1 id="vlm的定义"><strong>VLM的定义</strong></h1>
<p>多模态模型处理一种类型以上的输入，比如vision+text
models。下面说的大概是研究多模态的一种，比如vision language
models.需要理解和处理image和text的信息。相关的任务比如image
captioning(e.g.CLIP), visual question
answering(GPT-4-vision)，text-to-image(e.g.DALL-E)，或者text-to-video
generation(sora).</p>
<h1 id="vlm的结构"><strong>VLM的结构</strong></h1>
<p>VLM的结构在于如何融合视觉和文本模态，不同的架构会在早期阶段、中间阶段或者最后阶段进行信息的融合或者对齐。有很多种，可以看看几种最常见的：</p>
<h2 id="llavalarge-language-and-vision-assistant"><strong>LLaVA(Large
Language and Vision Assistant)</strong></h2>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602a6d6e809e475597e6c83b7a5467cc1b9b1b1cf35b05de7367edf29a23485286baa512d4ecd6a23dc66c64adc2a3bc0c83.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>图像经过clip并投影到embedding
vectors，文本通过文本模型产生相应的embedding
vectors。处在相同的维度空间里，可以进一步无缝集成。</p>
<p>在训练LLaVA模型中有两个阶段：</p>
<p>1.pre-training for feature alignment.
只有投影矩阵在数据集CC3M上进行更新。</p>
<figure>
<img
src="https://alidocs.dingtalk.com/core/api/resources/img/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602aa3ba6b64951bcf7e84d17910d9e7d63213bffcf9a87c1aab68c6fa978b73ce9d9bddef934264e90602517809c9a88bd1?tmpCode=b2bef226-f447-45ac-8fa7-2a6efc08cd77"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>Fine-tuning End-to-End. 投影矩阵和LLM会在两个不同的场景进行更新：
<ol type="1">
<li>visual chat:LLaVA
根据我们生成的多模式指令跟踪数据进行了微调，以适应日常面向用户的应用。</li>
<li>Science QA:LLaVA 针对科学领域的多模态推理数据集进行了微调。</li>
</ol></li>
</ol>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602a7a5c135778ee098c4ee39340545c1810032fdf6ec961d9b8280818ad1a39f1eb6c03bec04b3171c863a771ca118fcc80.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="llava-model的组成"><strong>LLaVA model的组成</strong></h3>
<h4 id="vision-encoder"><strong>Vision Encoder</strong></h4>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602a8169d9d9ddf3b83fb68af4e589ff4560e0af6934db815dd3d4873be65aacbe1f4f69c02138fa35209a0503d9106918e3.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>visual
encoder将image划分为小的patches，比如16x16像素这样。每个patch被处理转换成数值形式，在送给image
encoder;</p>
<p>image
enconder由多个blocks组成，主要目的是加强图像的视觉表示。包括feedforward
layers,用来提取高层特征；attention layers关注图像的相关性.</p>
<p>vision
encoders的training目标是尽量减少images的向量表示和text描述的差异度。在训练期间，目标是开发一个模型，该模型能够将图像的嵌入转换为与文本紧密一致的表示。</p>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602a822951806323f8968a78147832cebbe7366e02e12c7c3c233e5af64135f1de429bae9669baa2e10300bf43c4f2a3e7a4.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4 id="embedding-models"><strong>Embedding Models</strong></h4>
<p>Embeddings认为是tokens的密集数学表示(compact numerical
representations).根源来自分布假设，相似位置的词语有相近的meanings.</p>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602aaac77b6f1abe50dfce406188c32a716b26e334a1b4675672c7cd1c938ecf5ad9e0005879e3da24024ad4d15891f4e49e.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="llava的推理"><strong>LLaVA的推理</strong></h3>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602afc228fb22ff2c4af7ef46e557ff1e118f0fc5dee6617dc42b5443ede480f28923d2b3d49d7a1732db1f0c05db8c5a083.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>image和text都转换为embeddings之后，由attention决定关注哪一部分。</p>
<h1 id="vla"><strong>VLA</strong></h1>
<p>以RT-2为例</p>
<h2 id="vla-model和vlm-model"><strong>VLA model和VLM model</strong></h2>
<p>VLM模型是一种能处理visual和自然语言的机器学习模型，VLM
是在互联网规模的图像和文本数据上进行训练的。比如RT-2中用到的base VLA
model:PaLI-X PaLM-E</p>
<p>RT-2提出了一种方法将预训练的VLA
model用在机器人轨迹数据上，其中机器人轨迹数据用文本形式进行编码。</p>
<h2 id="rt-2-model"><strong>RT-2 model</strong></h2>
<p>将机器人的轨迹数据编码为text tokens，对vla模型进行fine tune.</p>
<h2 id="效果"><strong>效果</strong></h2>
<h3 id="泛化性"><strong>泛化性</strong></h3>
<p>这项研究的一项重要发现是 RT-2
模型令人印象深刻的泛化能力。该模型在处理新物体、背景和环境时展现出显著提升的性能。它可以解读机器人训练数据中未曾出现的命令，并根据用户指令进行基本的推理。这种推理能力源于底层语言模型运用思维链推理的能力。该模型的推理能力示例包括：判断哪个物体（石头）可以用来做临时锤子，或者哪种饮料最适合疲惫的人（能量饮料）。这种程度的泛化能力是机器人控制领域的一大进步。</p>
<h3 id="新兴能力"><strong>新兴能力</strong></h3>
<p>RT-2
模型的另一个令人兴奋的方面是它展现出的涌现能力。通过利用从互联网规模预训练中获得的知识，该模型可以执行训练期间未明确传授的任务。例如，它可以重新利用已学技能，将物体放置在语义指示的位置附近，或解释物体之间的关系，以确定拾取哪个物体以及将其放置在何处。例如，诸如“捡起即将从桌子上掉下来的袋子”或“将香蕉移动到二加一的和”（均如图
1
所示）之类的命令——要求机器人对机器人数据中从未见过的物体或场景执行操作任务——需要从基于网络的数据中转化而来的知识才能运行。这些涌现能力展示了
VLA
模型在将知识从网络规模数据迁移到现实世界机器人控制方面的强大能力。</p>
<figure>
<img
src="/2025/04/27/VLM-VLA/5eecdaf48460cde589e30c1a9ad470c9891b500d8a49637075b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832e3a45c6badaef44ca130f2aa95930983137950b62161dc0f419c48d2d743c40903620870ff0d76157.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="消融实验"><strong>消融实验</strong></h3>
<p>该研究论文还将 RT-2
模型的性能与多个基线模型进行了比较。结果表明，RT-2
在泛化能力和涌现能力方面均优于基线模型。此外，论文还探讨了模型规模和训练策略对泛化性能的影响。结果发现，更大的模型以及与网络数据协同微调可以带来更好的泛化性能。</p>
<h3 id="缺点"><strong>缺点</strong></h3>
<p>虽然 RT-2 模型展现出巨大的潜力，但仍存在一些局限性。实时运行大型 VLA
模型的计算成本很高，需要开展更多研究来优化其推理速度。此外，目前可用于微调的开源
VLM 模型数量有限。未来的研究应侧重于开发实现更高频率控制的技术，并使更多
VLM 模型可用于训练 VLA 模型。</p>
<h1 id="参考文献"><strong>参考文献：</strong></h1>
<p>https://medium.com/<span class="citation"
data-cites="aydinKerem/what-are-visual-language-models-and-how-do-they-work-41fad9139d07">@aydinKerem/what-are-visual-language-models-and-how-do-they-work-41fad9139d07</span></p>
<p>https://alinlab.kaist.ac.kr/ai602_2024_fall.html</p>
<p>https://alinlab.kaist.ac.kr/ai602_2025_spring.html</p>
<p>https://medium.com/black-coffee-robotics/vision-language-action-vla-models-llms-for-robots-f60ba0b79579</p>
<p>https://medium.com/<span class="citation"
data-cites="LawrencewleKnight/how-vision-language-action-models-are-revolutionizing-robotic-control-a627bbc0c249">@LawrencewleKnight/how-vision-language-action-models-are-revolutionizing-robotic-control-a627bbc0c249</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/04/27/RL-Basic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/27/RL-Basic/" class="post-title-link" itemprop="url">RL-Basic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-04-27 11:35:41 / Modified: 11:39:52" itemprop="dateCreated datePublished" datetime="2025-04-27T11:35:41+08:00">2025-04-27</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Goal: 了解一下关于RL, VLA, VLM的basic concetps</p>
</blockquote>
<h1 id="什么是rl">什么是RL</h1>
<blockquote>
<p>[!NOTE]</p>
<p>agent的终极目标是优化在环境中的行为。开始时刻，agent在环境中可能表现糟糕；随着时间推移，和环境交互试错可以不断的提高自己的表现。</p>
<p>RL最大的美妙之处在于可以通过同一套算法让agent适应不同的未知的复杂环境。</p>
</blockquote>
<h3 id="应用场景"><strong>应用场景</strong></h3>
<ul>
<li>游戏：chess and go</li>
<li>机器人:在家里移动、搬运、完成复杂的日常任务</li>
<li>自动驾驶：自动驾驶汽车、控制直升机或无人机。</li>
</ul>
<blockquote>
<p>[!WARNING]</p>
<ul>
<li>经典books:http://incompleteideas.net/book/RLbook2020.pdfrepo:https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions</li>
</ul>
</blockquote>
<h1 id="rl-framework"><strong>RL framework</strong></h1>
<ul>
<li><strong>Agent</strong>: 目标是学习策略来优化某个process</li>
<li><strong>Environment</strong>:
Agent所在的环境，由一系列状态组成。</li>
<li>在每个时间步，agent选择一个action,会改变环境的state到新一个。agent因此会收到一个feedback评估这次choice的好坏。这个feedback叫做<strong>reward</strong>，会表示为数值形式。</li>
<li>根据reward,agent随时间逐渐学习到优化的策略去最大化总的reward值。</li>
</ul>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832ee62a2c00f2506431df37dcacffbf0d3f06ebb3f9035c60659af27edee83c29e87bf84bc12faa752b.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>当前环境的state和当前agent的action可能导致不同的prob让new
state会有不同的reward,对应相应的概率。</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602aaac77b6f1abe50dfd9389e0fb3ccd3fdf8f7f83186eccbd32f5473c651d1e4afe7af616c5e1a828a868db6307fa1a316.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="reward种类"><strong>Reward种类</strong></h1>
<p>为了定义一个长过程中总的reward(也叫做return),有几种形式。</p>
<h2 id="简化版本"><strong>简化版本：</strong></h2>
<p>Ri表示i时间戳agent收到的reward,</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832ea5a092a0869b9a49251f76a8d51566dc2517977dcc83dd7f5fa09420059731be2d8a6999f18e0eb0.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2
id="discounted累计reward"><strong>Discounted累计reward:</strong></h2>
<p>大多数时候会在reward前面乘以一个削减系数（0～1）。</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a822951806323f8960fa2fb72871b3bf7d5de161273e1e25b88aa4a62aeff924909b53ed0480f68080407fa5ac0f59a0a.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>根本原因是agent的优化行为是更考虑短期的rewards。最终可以表示为一种递归的公式形式。</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832e2752ec8e99bb261e3289026cdc391e0e12c49ec7a8e14847077fa8e1d7bd4e0e6c8dcf03add25814.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="任务的类型"><strong>任务的类型</strong></h1>
<h2 id="episodic-tasks"><strong>episodic tasks</strong></h2>
<p>agent和environment的交互可以包含一系列独立的episodes.这些偶然事件独立于其他并且起始状态从状态分布中采样。</p>
<p>举个例子，我们想让agent玩一个游戏。为了达到那个目的，我们会让robot玩很多独立的游戏，或输或赢。收到的rewards会逐渐影响robot接下来在游戏中的策略。</p>
<p>episodes也叫trials.</p>
<h2 id="continuing-tasks"><strong>continuing tasks</strong></h2>
<p>不是所有的tasks都是episodic，一些任务可以是连续性的，意味着没有一个最终的状态。比如（win
or lose）。这种情况下，timestamp是无限的，因此计算cummulative
return可能不可行。</p>
<h1 id="策略policies和value-functions"><strong>策略(policies和value
functions)</strong></h1>
<p>在强化学习中，<strong>Policy（策略）</strong>和<strong>Value
Function（价值函数）</strong>是两个最核心的概念，它们共同指导智能体的决策，但侧重点不同。它们的关系可以通过<strong>“导航”</strong>的类比直观理解：</p>
<h2 id="policy"><strong>policy</strong></h2>
<h3 id="policy策略-怎么做"><strong>Policy（策略）——
‘怎么做’</strong></h3>
<ul>
<li><strong>直观理解</strong>：策略是智能体的<strong>行为准则</strong>，直接决定它在每个状态下应该采取什么动作。
<ul>
<li><strong>比如</strong>：
<ul>
<li>自动驾驶的Policy可能是：“看到红灯→刹车；绿灯→加速”。</li>
<li>游戏AI的Policy可能是：“敌人靠近→攻击；血量低→逃跑”。</li>
</ul></li>
<li><strong>数学表示</strong>：
<ul>
<li><strong>确定性策略</strong>：π(s)=a<em>π</em>(<em>s</em>)=<em>a</em>
（状态 s<em>s</em> 下固定选择动作 a<em>a</em>）。</li>
<li><strong>随机性策略</strong>：π(a∣s)<em>π</em>(<em>a</em>∣<em>s</em>)
（状态 s<em>s</em> 下选择动作 a<em>a</em> 的概率）。</li>
</ul></li>
</ul></li>
</ul>
<p>策略π是从状态s ∈ S 采取各种可能的action到达概率p的mapping.</p>
<p>如果一个agent遵从策略π，那么agent从状态s采取一个action的概率p(a|s)=π(s).</p>
<p>任何policy可以表示成一个表大小为|s|x|A|</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a822951806323f896986c6ef4fe897b0ad315db4181a814c9ef21b11a6c8d3bfbfaa770e7d0955ef5b53ca2eef1412a53.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>最简单的策略就是random:机器人走每一步的概率是一样的，对应上面右边的表格。</p>
<p>展示的表格也可以是episode
task的一个例子，到达终止状态后会收获一个reward。一个新独立的game可以被初始化。</p>
<p>除了policy之外，也会用value
functions描述agent从一个当前state采取某种action或者在给定的state的expected
reward(选择的好坏).</p>
<h2 id="value-function价值函数-有多好"><strong>Value
Function（价值函数）—— ‘有多好’</strong></h2>
<ul>
<li><strong>直观理解</strong>：价值函数是<strong>对状态的长期收益评估</strong>，回答“当前状态/动作对未来有多有利”。
<ul>
<li><strong>比如</strong>：
<ul>
<li>下棋时，某个棋盘状态的价值函数值高，说明胜率大。</li>
<li>股票市场中，当前持仓的价值函数值低，说明未来可能亏损。</li>
</ul></li>
<li><strong>两种类型</strong>：</li>
</ul></li>
</ul>
<ol type="1">
<li><strong>状态价值函数</strong> <strong>Vπ(s)Vπ(s)</strong>：
<ul>
<li>表示在状态 s<em>s</em> 下，<strong>遵循策略</strong>
<strong>ππ</strong> 能获得的期望回报。
<ul>
<li>公式：Vπ(s)=Eπ[Gt∣St=s]*V**π<em>(</em>s<em>)=E</em>π*[<em>G<strong>t<em>∣</em>S</strong>t</em>=<em>s</em>]。</li>
</ul></li>
<li><strong>动作价值函数</strong> <strong>Qπ(s,a)Qπ(s,a)</strong>：
<ul>
<li>表示在状态 s<em>s</em> 下<strong>执行动作</strong>
<strong>aa</strong>，之后遵循策略 π<em>π</em> 的期望回报。</li>
<li>公式：Qπ(s,a)=Eπ[Gt∣St=s,At=a]*Q**π<em>(</em>s<em>,</em>a<em>)=E</em>π*[<em>G<strong>t<em>∣</em>S</strong>t</em>=<em>s</em>,*A**t<em>=</em>a*]。</li>
</ul></li>
</ul></li>
</ol>
<ul>
<li><strong>Policy</strong>是“行动派”，<strong>Value
Function</strong>是“预言家”。</li>
<li>Policy告诉智能体“该做什么”，Value
Function告诉它“这么做有多好”。</li>
<li>两者协同工作：Value
Function评估和改进Policy，而Policy的变动又会影响Value
Function的估计。</li>
<li>结合两者的方法（如Actor-Critic）通常能更高效地解决复杂问题。</li>
</ul>
<h3 id="state-value-function"><strong>state-value
function:</strong></h3>
<p>state-value function
v(s)或者简单点说叫v-function建立了映射，从环境的某种state,根据agent采取的策略<em>π</em>，到预期会收到的累积的expected
reward.</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832e7a8d57df376b356d171e81b79de6e337372cb5694f27174e4dce4c500338c37bf95253e51abbc896.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>举个例子：</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832e162b031b56b6a1724b18e3afdf1b55bd225aac08a1dc52adf33423bef2f6b67708c426ff155be740.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>终止状态的v-value是0.</p>
<h3 id="action-value-function"><strong>action-value
function</strong></h3>
<p>与V-function类似，也考虑agent在某个policy下考虑某个可能的action.</p>
<p>Action-value function
q(s,a),或者可以说是Q-function是建立一种映射，从环境的一种状态s ∈ S
和可能的action a ∈ A,遵循某种策略<em>π</em> 收到的expected reward.</p>
<p>Q-function可以表示为|S|x|A|的表格。</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a15464a86392b1bf540e224cfc1df371c8d39a8d332128710072622a09d4cf92d8988b562c3a960af81055eb0250cfadf.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602aa3ba6b64951bcf7e35c7986ae2f87dd0b9fc1b7de33af5df9f5bcb9b3feba58e6341dabe337268f2d87ed2ce7eecb005.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>state和action functions的不同点在于action-value
function采用了在当前state agent要采用的action的信息，state
function只考虑了当前的state并没有考虑到agent的下一步action.</p>
<p>V-和Q-functions都是从agent经验中学到的。</p>
<h4 id="v--and-q-的subtility之处"><strong>V- and
Q-的subtility之处</strong></h4>
<p>为什么q(s,a)不等于v(s')。i.e.为什么agent在state s采取了action
a去到了下一个state s'的expected reward不等于agent在s'的expected
reward?</p>
<p>答案是虽然在一个当前状态s采取一个action a会确定性leads
to一个新状态s',agent收到的reward是Q-function考虑的而不是V-function.q(s,a)计算是<em>Rₜ
+ αRₜ₊₁</em> … . ,v(s')是<em>Rₜ₊₁ + αRₜ₊₂ + …</em> .不包括Rt的。</p>
<p>另外一个值得注意的是，在某些状态s采取行为a可能导致多个可能的下一个状态。比如迷宫游戏agent往右走可能有20%随机遇到障碍被自动移到别的位置。因此，agent收到的rewards即使是同样的action和state也可能不一样。这是另一个导致两者不对等的原因。</p>
<h2 id="bellman-equation"><strong>Bellman equation</strong></h2>
<p>Bellman
equation是RL领域的一个基础equation.简单地来说，它递归建立起state/act
ion的function values在当前和下个timestamp.</p>
<h3 id="v-function"><strong>V-function</strong></h3>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a7a5c135778ee098c401725b5201f203435270a1f77f4a98be0ae5e992dfa74328a3abbe119771052b7e74677826d1bd8.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="q-function"><strong>Q-function</strong></h3>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a8169d9d9ddf3b83fcb03b239e3eceb242c9c971f5314e56e72c2fde54030076fabfad3ef98bc3edb99d190f84f42e1c0.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a7a5c135778ee098cfe750a74b532a501f6cf218ac414754e2c700d28f0f271a505b79623335ef7e55c3d7f4a3444c437.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="optimal-policy"><strong>Optimal policy</strong></h1>
<p>定义不同策略的比较</p>
<p>π₁&gt;π2 当对于所有状态s ∈ S.π₁的reward都不小于π₂的reward</p>
<p>policy π⁎是最优的，当不小于其他所有策略</p>
<h2 id="bellman-optimality-equation"><strong>Bellman optimality
equation</strong></h2>
<p>解Bellman quations的优化问题，求max</p>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602a15464a86392b1bf55353e8fbce6a782389a0a7c7836bb44f878c060e95ba8434fcb00ab44b734ff3277216822a7d6743.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img
src="/2025/04/27/RL-Basic/5eecdaf48460cde589e30c1a9ad470c985c7bb96130a977175b8339e1c4c24831f739168b2e59d878d68742cd653602aaac77b6f1abe50df69045fd03b3ab51cef15cc3b2788fdda90cb3bc153f71cf66390746302e9a53c842d333d629689de.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>求解了V-* function或者Q<em>-function，最优策略</em>π⁎
就可以很容易被计算。*</p>
<p>实际运用时，states的数量非常庞大，因此数学求解这个问题非常困难。因此，RL
learning要用计算量和memory更低的近似的策略替代。</p>
<h1 id="参考文献"><strong>参考文献：</strong></h1>
<p>https://readmedium.com/reinforcement-learning-introduction-and-main-concepts-48ea997c850c</p>
<p>books and repo:</p>
<p>http://incompleteideas.net/book/RLbook2020.pdf</p>
<p>https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/04/21/stable-diffusion-history/F%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/21/stable-diffusion-history/F%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-04-21 17:35:22" itemprop="dateCreated datePublished" datetime="2025-04-21T17:35:22+08:00">2025-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="f智能问答">F智能问答</h1>
<p>数据库的优缺点、异同：</p>
<p>Es,mysql,redis</p>
<p>bm25</p>
<p>TF-IDF</p>
<p>milvus向量数据库</p>
<p>Ginas.ai</p>
<h1 id="lightrag">lightRAG</h1>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/03/14/stable-diffusion-history/problem%201/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/14/stable-diffusion-history/problem%201/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-14 15:32:46" itemprop="dateCreated datePublished" datetime="2025-03-14T15:32:46+08:00">2025-03-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="problem-1">problem 1</h1>
<p>array max sum连续子数组 return max sum</p>
<p>Ex: [-2,1,-3,4,-1,2,1,-5,4] -&gt;6</p>
<p>定义：s[i]以index i为结尾的最长子数组的max sum</p>
<p>s[i] = if s[i-1] &gt; 0 s[i-1]+s[i]. Or s[i]</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">maxLianXuSum</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; array)</span> </span>&#123;</span><br><span class="line">  std::vector&lt;<span class="type">int</span>&gt; array_s;</span><br><span class="line">  <span class="type">int</span> max_sum = <span class="number">-9999</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i&lt; array.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span>(i &gt; <span class="number">0</span> &amp;&amp;s[i<span class="number">-1</span>] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      array_s[i] = array[i] + array_s[i<span class="number">-1</span>];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      array_s[i] = array[i]</span><br><span class="line">    &#125;</span><br><span class="line">    max_sum = <span class="built_in">max</span>(max_sum, array_s[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> max_sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Pro 2:</p>
<p>活动start end</p>
<p>Max number activities no overlap -&gt; retun num</p>
<p>[1,3] 2.5 4,7. 6,8. -&gt; 2</p>
<p>1,3 -&gt; &gt;3 -&gt; 4,7and 6.8 -&gt; add 4,7
-&gt;找到的就是以1,3开头尽可能多的活动</p>
<p>2,5 -&gt; 5-&gt; 6,8 -&gt; add 6,8 ....2,5</p>
<p>4,7 没有必要因为1,3的尽可能多的会包括4,7</p>
<p>6,8 开头 -&gt;add 6,8 -&gt; 1</p>
<p>4,7 -&gt; 4,7 -&gt;1</p>
<p>2,5 -&gt; add 6, 8. Ans[2,5] = ans[6,8]+1 1, 3 -&gt;</p>
<p>6,8 -&gt; 2,5</p>
<p>4,7 -&gt;1,3</p>
<p>2,5 -&gt;</p>
<p>1,3</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Activity</span> &#123;</span><br><span class="line">  <span class="type">int</span> start_time;</span><br><span class="line">  <span class="type">int</span> end_time;</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">findMaxNumActivities</span><span class="params">(<span class="type">const</span> std::vector&lt;Activity&gt;&amp; act)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> n = act.<span class="built_in">size</span>();</span><br><span class="line">  <span class="comment">// 1. 按照start time排序</span></span><br><span class="line">  std::vector&lt;Activity&gt; act_by_start_time;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j = act.<span class="built_in">size</span>()<span class="number">-1</span>; j &gt; <span class="number">0</span>; --j) &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i&lt; j; ++i) &#123;</span><br><span class="line">    	<span class="keyword">if</span>(act[i].start_time&lt;act[i<span class="number">-1</span>].start_time) <span class="built_in">swap</span>(act, i, i<span class="number">-1</span>);</span><br><span class="line">  	&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 2.数组ans的定义：记录以活动i开头的max num act</span></span><br><span class="line">  std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">  <span class="type">int</span> has_compared_min_end_time;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = n; i &gt; <span class="number">0</span>; --i) &#123;</span><br><span class="line">		ans[i] = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 找start time大于当前的end time的活动</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j = i+<span class="number">1</span>; j &lt; n; ++j) &#123;</span><br><span class="line">      <span class="keyword">if</span>(act[j].start_time &gt; <span class="type">end_t</span>) &#123;</span><br><span class="line">        ans[i] = <span class="built_in">max</span>(ans[i], ans[j] +<span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;        </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 3. return max element in ans as return num</span></span><br><span class="line">  <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; ans.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    res = <span class="built_in">max</span>(res, ans[i]);</span><br><span class="line">  &#125;</span><br><span class="line">	<span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Pro 3:</p>
<p>Array longest递增子序列的长度</p>
<p>10,9,2,5,3,7,101,18 -&gt; 4</p>
<p>定义ans_array: index i表示以array[i]结束的子序列的最长的长度</p>
<p>Ans[0] =1;</p>
<p>Ans[i] = max(ans[i-k]...)+1 which array[i-k] &lt; array[i]</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">findLongestSequence</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; array)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(array.<span class="built_in">size</span>() &lt; <span class="number">2</span>) <span class="keyword">return</span> array.<span class="built_in">size</span>();</span><br><span class="line">  std::vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">  ans[<span class="number">0</span>] = array[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt; array.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    ans[i] = <span class="number">1</span>;</span><br><span class="line"> 		<span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; i; ++j) &#123;</span><br><span class="line">      <span class="keyword">if</span>(array[j] &lt; array[i]) ans[i] = <span class="built_in">max</span>(ans[i], ans[j]+<span class="number">1</span>);</span><br><span class="line">      </span><br><span class="line">    &#125;   </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// find max element in ans</span></span><br><span class="line">  <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; ans.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    res = <span class="built_in">max</span>(res, ans[i]);</span><br><span class="line">  &#125;</span><br><span class="line">	<span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>n堆硬币 正整数</p>
<p>任意一堆硬币的顶部取一个硬币</p>
<p>取k个硬币硬币面值和的最大值</p>
<p>Ex1: 1,100,3. 7,8,9. k=2 101</p>
<p>Ex2:</p>
<p>100</p>
<p>100</p>
<p>100</p>
<p>100</p>
<p>100</p>
<p>100</p>
<p>1，1，1，1，1，1，700</p>
<p>k=7</p>
<p>706</p>
<p>Ans:列数是n（硬币的堆数），行数 max_coin_num = 7</p>
<p>Ans: 每一列从上到下1，101， 104.。。</p>
<p>k = 1, vector = 100,100, ... ,1</p>
<p>k=2,</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
