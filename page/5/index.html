<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Garfield&#39;s blog">
<meta property="og:url" content="http://77philosophia.github.io/page/5/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://77philosophia.github.io/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/07/comfyui-env/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/07/comfyui-env/" class="post-title-link" itemprop="url">comfyui环境搭建+问题记录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-07 19:15:27 / Modified: 19:19:41" itemprop="dateCreated datePublished" datetime="2025-01-07T19:15:27+08:00">2025-01-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="环境安装">环境安装</h3>
<ol type="1">
<li><p>import torchaudio的时候报错“undefined symbol”</p>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191646057.png"
alt="image-20250107191646057" />
<figcaption aria-hidden="true">image-20250107191646057</figcaption>
</figure></li>
</ol>
<p>推测还是版本不兼容，按照下面这个博客的版本进行安装</p>
<p><strong>pip install torch==2.1.0</strong></p>
<p><strong>pip install torchvision==0.16.0</strong></p>
<p><strong>pip install torchaudio==2.1.0</strong></p>
<p><strong>https://blog.csdn.net/qq_45599476/article/details/140207176</strong></p>
<p>再次启动就好了</p>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191710761.png"
alt="image-20250107191710761" />
<figcaption aria-hidden="true">image-20250107191710761</figcaption>
</figure>
<ol start="2" type="1">
<li><p>在点击Queue Prompt出图的时候报错</p>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191754873.png"
alt="image-20250107191754873" />
<figcaption aria-hidden="true">image-20250107191754873</figcaption>
</figure></li>
</ol>
<p>现这个工程中<strong>xformers</strong>也没有成功运行。</p>
<p><strong>https://github.com/comfyanonymous/ComfyUI/issues/4870</strong></p>
<p><strong>The torch repo has a release wheel of xformers that works
with torch 2.4.1</strong></p>
<p><strong>pip3 install -U xformers --index-url</strong>
<strong>https://download.pytorch.org/whl/cu124</strong> <strong>should
pull</strong></p>
<p>*<strong>0.0.28.post1*</strong></p>
<p><strong>https://pytorch.org/get-started/previous-versions/</strong></p>
<p>解决还是变包：</p>
<p><strong>pip install torch==2.4.1</strong></p>
<p><strong>Pip install torchvision==0.19.1</strong></p>
<p><strong>Pip install torchaudio==2.4.1</strong></p>
<p><strong>pip3 install -U xformers --index-url</strong>
<strong>https://download.pytorch.org/whl/cu124</strong> <strong>should
pull</strong></p>
<p>*<strong>0.0.28.post1*</strong></p>
<p>成功出图</p>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191854078.png"
alt="image-20250107191854078" />
<figcaption aria-hidden="true">image-20250107191854078</figcaption>
</figure>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191903222.png"
alt="image-20250107191903222" />
<figcaption aria-hidden="true">image-20250107191903222</figcaption>
</figure>
<h3 id="测试">测试：</h3>
<p>安装和sd跑同一个prompt</p>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191927650.png"
alt="image-20250107191927650" />
<figcaption aria-hidden="true">image-20250107191927650</figcaption>
</figure>
<figure>
<img src="/2025/01/07/comfyui-env/image-20250107191936508.png"
alt="image-20250107191936508" />
<figcaption aria-hidden="true">image-20250107191936508</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/07/B-Lora-reproduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/07/B-Lora-reproduce/" class="post-title-link" itemprop="url">复现B-Lora算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-07 19:09:30 / Modified: 19:14:01" itemprop="dateCreated datePublished" datetime="2025-01-07T19:09:30+08:00">2025-01-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="用官方提供的推理">1. 用官方提供的推理</h3>
<figure>
<img src="/2025/01/07/B-Lora-reproduce/image-20250107191054805.png"
alt="image-20250107191054805" />
<figcaption aria-hidden="true">image-20250107191054805</figcaption>
</figure>
<h3 id="训练出现nan">2. 训练出现nan</h3>
<p>需要调节学习率<strong>heckpoint=1100, lr=5e-6</strong></p>
<figure>
<img src="/2025/01/07/B-Lora-reproduce/image-20250107191222548.png"
alt="image-20250107191222548" />
<figcaption aria-hidden="true">image-20250107191222548</figcaption>
</figure>
<h4 id="其他问题">其他问题：</h4>
<p>训练过程中每次都会有<strong>bug report</strong></p>
<p><strong>Welcome to bitsandbytes. For bug reports, please submit your
error trace to:</strong>
<strong>https://github.com/TimDettmers/bitsandbytes/issues</strong></p>
<p><strong>For effortless bug reporting copy-paste your error into this
form:</strong></p>
<p>https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link</p>
<p>找到一个<strong>issue</strong>
<strong>https://github.com/bitsandbytes-foundation/bitsandbytes/issues/168</strong></p>
<figure>
<img src="/2025/01/07/B-Lora-reproduce/image-20250107191329941.png"
alt="image-20250107191329941" />
<figcaption aria-hidden="true">image-20250107191329941</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/07/lora-scripts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/07/lora-scripts/" class="post-title-link" itemprop="url">lora-scripts</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-07 00:47:35 / Modified: 10:58:07" itemprop="dateCreated datePublished" datetime="2025-01-07T00:47:35+08:00">2025-01-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure>
<img src="/2025/01/07/lora-scripts/image-20250107004758199.png"
alt="image-20250107004758199" />
<figcaption aria-hidden="true">image-20250107004758199</figcaption>
</figure>
<h3 id="准备lora-scripts环境">1. 准备lora-scripts环境</h3>
<p>https://github.com/Akegarasu/lora-scripts</p>
<blockquote>
<p>[!CAUTION]</p>
<p>需要用python3.9，按照仓库代码指示进行安装，安装成功可以启动ui页面。</p>
</blockquote>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107104846559.png"
alt="image-20250107104846559" />
<figcaption aria-hidden="true">image-20250107104846559</figcaption>
</figure>
<h3 id="准备数据集">2. 准备数据集</h3>
<ol type="1">
<li>lora训练脚本图片默认大小是1024x1024,如果训练图片大小不够，可以对图片进行高清放大：</li>
</ol>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105006386.png"
alt="image-20250107105006386" />
<figcaption aria-hidden="true">image-20250107105006386</figcaption>
</figure>
<h3 id="打标筛选">3. 打标+筛选</h3>
<p>打标的基本步骤：用<strong>webui</strong>的<strong>tagger</strong>打标工具自动打标，生成<strong>txt</strong>文件，人工修正标签。</p>
<p>在这个过程里面我用到的一些人工打标的原则：</p>
<blockquote>
<p>[!NOTE]</p>
<ol type="1">
<li>过滤多人交互的复杂图片，不确定的可以只留单人图。凑足<strong>100</strong>张图片</li>
<li>打标风格标签用简单的无意义的单个词，如sznanpinds。（我一般用小说前两个拼音首字母+nanpinds/nvpinds）</li>
<li>筛选标签
<ol type="1">
<li>合并删除重复、矛盾的标签，比如shirt,white shirt-&gt;white shirt;
gray hair, brown hair -&gt; brown hair</li>
<li>删除识别错误的标签</li>
</ol></li>
</ol>
</blockquote>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105300989.png"
alt="image-20250107105300989" />
<figcaption aria-hidden="true">image-20250107105300989</figcaption>
</figure>
<h4 id="修改训练脚本train.sh">4. 修改训练脚本train.sh</h4>
<ol type="1">
<li><p>底膜设置、数据集文件夹、输出文件设置</p>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105409825.png"
alt="image-20250107105409825" />
<figcaption aria-hidden="true">image-20250107105409825</figcaption>
</figure></li>
<li><p>训练数据集地址，
，数据集文件夹的命名是<strong>repeat</strong>次数**_<strong>含义，比如我在训练风格</strong>lora**的时候我希望</p>
<p>每张图片训练的时候<strong>repeat
20</strong>次<strong>,</strong>我的数据集文件夹命名就是<strong>20_xxxStyle</strong></p></li>
</ol>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105459729.png"
alt="image-20250107105459729" />
<figcaption aria-hidden="true">image-20250107105459729</figcaption>
</figure>
<p>但是在脚本里填数据集位置的时候不能直接写这里</p>
<p>的<strong>/xxx/datasets/large_filterMultiPeople_addOld_datasets/12-</strong></p>
<p><strong>SheZhengWang/output_x2/training/20_shezhengwang</strong>，要写到上一级父目录，因此数据集地址</p>
<p>是<strong>/xxx/datasets/large_filterMultiPeople_addOld_datasets/12-</strong></p>
<p><strong>SheZhengWang/output_x2/training</strong>，这个目录下只有<strong>20_xxx</strong>一个文件夹。</p>
<ol start="3" type="1">
<li>模型输出文件夹<strong>output_dir_name</strong>，log目录，避免新run覆盖旧的模型文件。</li>
</ol>
<h4 id="测试">5. 测试</h4>
<p>在输出文件夹会有一些模型文件，可以选择一轮的模型复制到<strong>webui</strong>的<strong>models/LoRA</strong>目录下（或者comfyui相应的目录下面），刷新加载选中之后用提示词<strong>+lora</strong>模型就可以出图了。注意<strong>lora</strong>训练一般都会过拟合，一般<strong>10</strong>～<strong>20</strong>个<strong>epoch</strong>就可以，之后的<strong>epoch</strong>虽然还会损失下降，但是已经过拟合了。</p>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105655243.png"
alt="image-20250107105655243" />
<figcaption aria-hidden="true">image-20250107105655243</figcaption>
</figure>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105706660.png"
alt="image-20250107105706660" />
<figcaption aria-hidden="true">image-20250107105706660</figcaption>
</figure>
<p>测试图可以勾选Hires.fix高清放大和ADetailer面部修复得到更好的出图效果。如我在训练动漫数据用到的参数如下：</p>
<figure>
<img src="/2025/01/07/lora-scripts/image-20250107105804953.png"
alt="image-20250107105804953" />
<figcaption aria-hidden="true">image-20250107105804953</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/06/lora-training-background/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/lora-training-background/" class="post-title-link" itemprop="url">一些lora训练的背景知识</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-06 23:23:29" itemprop="dateCreated datePublished" datetime="2025-01-06T23:23:29+08:00">2025-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-01-07 19:25:27" itemprop="dateModified" datetime="2025-01-07T19:25:27+08:00">2025-01-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="understanding-stable-diffusion-models">1. Understanding stable
diffusion models</h3>
<ul>
<li><p>Stable diffusion 怎样理解concepts</p>
<ul>
<li>Stable
diffusion已经有了大量的知识，如果我们训练lora,非常重要的就是理解和区分"New
concepts"和"modified concepts"</li>
<li>New concepts:这些是模型之前没遇到的或者是未充分体现</li>
<li>Modified concepts:调整或者完善模型的现有理解</li>
</ul></li>
<li><p>lora种类：只用关注标准的就行。</p></li>
<li><p>理解sd模型</p>
<ul>
<li><p>Latent
space:一张512x512像素的图片大小是4x512x512，计算很耗费。因此一个解决方法是latent
space.将数学上复杂的对象比如image转换为简单压缩的形式。</p></li>
<li><p>一个做这种压缩过程的就是VAE,将图像压缩到它的latent
space然后将其重建为原来的形式。</p>
<blockquote>
<p>[!NOTE]</p>
<p>In the case of Stable Diffusion,for an image of size 512x512, the VAE
compresses it to 64x64x4 =16,384 values, making the process extremely
more efficient.</p>
</blockquote></li>
<li><p>Text encoder, tokenizer and Embeddings</p>
<ul>
<li><p>kenizer将输入分词</p></li>
<li><p>Text encoder将token编码成数字向量</p></li>
<li><p>编码后的向量叫做embedding</p>
<blockquote>
<p>[!NOTE]</p>
<p>Example:"Poiuytrezay",is encoded by the Text Encoder into "po
#628","iu#14292","y #88","Tre #975"，“z #89”, "ay</w> #551",where the
numbers represent the embedding ids within stable diffusion.</p>
</blockquote></li>
</ul></li>
<li><p>Unet</p>
<ul>
<li><p>unet是一个关键部分，它mix embedding和latent-encoded图像into a
mathematical soup 输出一个"noise" prediction.然后这个noise
prediction会被removed
from图像从而"denoise"it.训练lora的时候这也是train的主要部分，因为it is
the one making prediction.</p>
<figure>
<img
src="/2025/01/06/lora-training-background/image-20250106235503406.png"
alt="image-20250106235503406" />
<figcaption aria-hidden="true">image-20250106235503406</figcaption>
</figure></li>
</ul></li>
</ul></li>
</ul>
<h3 id="训练的准备">2. 训练的准备</h3>
<ul>
<li><p>数据集</p>
<ul>
<li>“A bad apple spoils the bunch”</li>
<li>Activation tags:特殊的触发词，
prompt模型产生或者强调相应的视觉元素。</li>
<li>The activation tag should be a unique tag that represents your
concept/character/style and should be present as the FIRST element in
the caption files of your dataset.</li>
</ul></li>
<li><p>Training script/UI</p>
<ul>
<li><p>各种训练脚本/ui都有相同的参数设置，只是会在部分features上有区别。</p></li>
<li><p>理解Lora,Dreambooth和TI(text inversion)</p>
<ul>
<li>sd的模型用传统的fine-tune的技术可能会耗费大量资源，下面是一些替代的技术。
<ul>
<li>DreamBooth:与常规微调没有太大区别，但通常针对单个概念。与常规微调的主要区别在于事先保存。</li>
<li>Textual inversion:创新新的嵌入帮助模型学习新关联</li>
<li>LoRA:这是一种只修改模型权重的一小部分的技术。</li>
<li>Pivotal Tuning:</li>
<li>hypernetwork</li>
</ul></li>
</ul></li>
<li><p>Base model for training</p>
<ul>
<li>总是用bf16/fp16 pruned模型用于训练</li>
<li>用四种base model：NAI, SD1.5, SD2.1 SDXL
<ul>
<li>真实性：SD1.5，SD2.1, or SDXL(your choice)</li>
<li>Anime/cartoon:NSI,SDXL</li>
</ul></li>
</ul></li>
<li><p>Dreambooth</p>
<ul>
<li><p>简单解释：Dreambooth是一种微调AI模型部分内容（特别是unet和文本编码器）的方法，使用稀有标记和正则化图像来维护模型的现有知识。生成的Dreambooth模型是一个检查点。</p></li>
<li><p>详细解释：文本到图像模型（如稳定扩散）最初在大量数据集上进行训练，以学习广泛的概念。这种初始训练形成了模型的“先验”<strong>——</strong>关于各种物体和想法（如不同的动物、车辆等）的基本知识。在这种情况下，引入新概念可能会很棘手。传统的微调（涉及根据新数据调整模型参数）可能会使模型忘记一些原始训练（称为“语言漂移”或“灾难性遗忘”).<strong>Dreambooth</strong>
提供了一种解决方案，可以在添加新概念的同时，使用特定于类的先验保存和稀有标记来减轻模型先验知识的丢失。</p></li>
<li><p>Regularization images/Class-specific prior preservation</p>
<p>Dreambooth使用一种特殊方法来保持模型的原始知识不变。它涉及使用与新概念属于同一“类别”但已经是模型知识一部分的图像来训练模型。在这里，“类别”是指实体（如对象、概念或数据点）所属的广泛类别或组。</p>
<p>例如，如果您正在教它关于特定类型的狗（例如金毛猎犬）的知识，您还包括使用初始模型生成的狗的一般图像。这有助于模型在学习新的特定类型时记住其关于狗的原始训练。</p></li>
</ul></li>
<li><p>Rare tokens</p>
<ul>
<li>在引入新概念时，<strong>Dreambooth</strong>
建议使用不常见或罕见的标记 <strong>-</strong>
模型不会与已知内容紧密关联的唯一标识符。由于它们先前的关联较弱，它将使语言漂移的影响较小（因为它将失去可能毫无意义的关联）
。这可以防止新训练干扰模型的现有功能。此类标记的示例可能是：“<strong>olis</strong>”、“<strong>bnha</strong>”或“<strong>hta</strong>”。要用罕见标记表示新概念，您可以将罕见标记与类一起使用（例如“一个
<strong>olis</strong> 女孩”，其中 <strong>olis</strong>
可以是您想要训练的任何新角色） 。</li>
</ul></li>
<li><p>How Dreambooth apply to lora</p>
<ul>
<li><strong>LoRA</strong> 需要与现有的微调方法协同工作。最常用的方法是
<strong>Dreambooth</strong> 和文本反转。但是，由于 <strong>LoRA</strong>
与文本反转不兼容，因此它主要与 <strong>Dreambooth</strong> 结合使用。将
<strong>LoRA</strong> 与<strong>Dreambooth</strong>
结合使用的主要区别在于它如何改进模型：<strong>LoRA</strong>
减少了需要调整的参数总数，并专注于微调模型的注意力层。这意味着它不会改变模型的整个单元和文本编码器。实际上，指导
<strong>Dreambooth</strong> 的原则也适用于 <strong>LoRA</strong>。在
<strong>LoRA</strong>
训练中省略罕见的标记或正则化图像可能会导致“语言漂移”<strong>——</strong>模型开始失去其原始的训练准确性。虽然包含这些元素不是强制性的，但在训练期间注意这个潜在问题很重要。</li>
</ul></li>
</ul>
<p>使用<strong>LoRA</strong> 时的一个关键挑战是有效地整合多个概念。一些
<strong>LoRA</strong>
模型最终会生成类似于其训练数据集中不同元素的拼贴图像（又称“拼贴画”）
。最好的 <strong>LoRA</strong>
不仅会生成高质量的图像，还会保留原始模型的广泛功能。例如，如果您要针对特定名人调整模型，则目标是实现该名人的变化（例如头发颜色变化）
，而不会失去模型准确识别或描绘其他名人的能力。</p></li>
</ul>
<h3 id="lora">Lora</h3>
<p>单纯引入模型，很多时候并不会触发LoRA的效果，因为LoRA在训练时基本都会加入若干个特殊的触发词，所以使用时也需要在提示词中输入触发词才能激活LoRA的效果。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>存在一些特殊的Lora不需要触发词，只要引入模型就能生效。</p>
</blockquote>
<p>理论上引入的LoRA不限个数（只是插件限制了），但同时引入多个LoRA模型时，同类的LoRA尽量不要重复。例如一个影响服饰、一个影响画风，它们之间是不会冲突的，还能起到很好的互补作用。但是如果两个
<strong>LoRA</strong>
都是影响容貌的，就有可能冲突，这时可以适当调节它们之间的权重，以其中一个为主、另一个则稍微起到调味作用就可以了。</p>
<h4 id="lora的应用场景">lora的应用场景：</h4>
<ol type="1">
<li><p>描绘特定人物形象</p>
<blockquote>
<p>[!NOTE]</p>
<p>大部分LoRA为了确保泛用性，训练使用的底膜都是SD大模型，所以一般情况下LoRA不挑模型，尤其是参考图上没有标注模型信息的LoRA,放心切换大模型就好。</p>
</blockquote>
<p>其实除了这些针对“某一个人”的LoRA之外，还有针对“某一类人”的LoRA模型——不局限于某一个人、而是实现了一个大方向的整体美化，例如：</p>
<ul>
<li>Fashion Girl: 使用时尚女性照片训练的模型</li>
<li>Cute Girl: 使用可爱女性照片训练的模型</li>
<li>AsianMale: 使用亚洲型男面孔训练的模型</li>
</ul>
<p>现在追加一个AsianMale的LoRA,权重不需要设置太高（免得喧宾夺主）：&lt;lora:Lora-Custom-ModelLiXian:0.3&gt;,1man</p></li>
<li><p>描绘特定画风</p></li>
<li><p>描绘特定概念</p></li>
<li><p>穿着特定服饰</p></li>
<li><p>添加特定元素</p>
<p>特定元素其实算是服饰类的延伸，只不过它更小、更专。搭配局部重绘使用。</p></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/06/train-vpred/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/train-vpred/" class="post-title-link" itemprop="url">基于noobai-vpred0.65底膜的训练</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-06 23:14:28 / Modified: 23:20:42" itemprop="dateCreated datePublished" datetime="2025-01-06T23:14:28+08:00">2025-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="comfyui工作流加载">1. comfyui工作流加载</h2>
<p>需要增加节点，直接使用官方提供的工作流即可。</p>
<figure>
<img src="/2025/01/06/train-vpred/image-20250106231614399.png"
alt="image-20250106231614399" />
<figcaption aria-hidden="true">image-20250106231614399</figcaption>
</figure>
<h2 id="webui运行">2.webui运行</h2>
<p>根据https://huggingface.co/Laxhar/noob_sdxl_v_pred_test（一个月以前的说明文档），无法在webui里运行。</p>
<p>根据https://huggingface.co/Laxhar/noobai-XL-Vpred-0.65s?not-for-all-audiences=true</p>
<figure>
<img src="/2025/01/06/train-vpred/image-20250106231748876.png"
alt="image-20250106231748876" />
<figcaption aria-hidden="true">image-20250106231748876</figcaption>
</figure>
<figure>
<img src="/2025/01/06/train-vpred/image-20250106231801221.png"
alt="image-20250106231801221" />
<figcaption aria-hidden="true">image-20250106231801221</figcaption>
</figure>
<figure>
<img src="/2025/01/06/train-vpred/image-20250106231816557.png"
alt="image-20250106231816557" />
<figcaption aria-hidden="true">image-20250106231816557</figcaption>
</figure>
<h2 id="lora脚本训练">3.lora脚本训练</h2>
<p>将parameterization=1 --v_parameterization --zero_terminal_snr
--scale_v_pred_loss_like_noise_pred --debiased_estimation_loss<br />
</p>
<p>在https://civitai.com/models/833294/noobai-xl-nai-xl
下面找到分享提示要将noise相关的都让之不生效 包括offset_noise之类的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/06/from-transformer-to-llm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/from-transformer-to-llm/" class="post-title-link" itemprop="url">从Transformer到LLM:Architecture,Training and Usage</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-06 22:11:16 / Modified: 23:09:12" itemprop="dateCreated datePublished" datetime="2025-01-06T22:11:16+08:00">2025-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="nlp-basic-concepts">NLP basic concepts</h3>
<h4 id="两个nlp的主要问题">1. 两个NLP的主要问题</h4>
<p>表示和建模：</p>
<p>a.Representation:将语言表示为机器语言：如BERT,Openai embedding</p>
<ol start="2" type="a">
<li>Modeling:用统计方法建模，GPT, chatGPT</li>
</ol>
<ol type="1">
<li><p>怎么表示words</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106225834152.png"
alt="image-20250106225834152" />
<figcaption aria-hidden="true">image-20250106225834152</figcaption>
</figure></li>
</ol>
<p>将单词分散到向量空间：将token映射到向量空间，向量空间(vector space)
是用来表示文本数据的一种高纬空间。在这个空间中，每个token或文本单元都被表示为一个向量。这些向量捕捉了文本单元的语义信息，并使得数学计算（如距离计算、相似度计算）成为可能。</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230157598.png"
alt="image-20250106230157598" />
<figcaption aria-hidden="true">image-20250106230157598</figcaption>
</figure>
<ol start="2" type="1">
<li><p>建模</p>
<p>语言模型LLM: 语言模型预测任何单词序列在给定语言语料库中的可能性</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230245416.png"
alt="image-20250106230245416" />
<figcaption aria-hidden="true">image-20250106230245416</figcaption>
</figure></li>
</ol>
<ul>
<li><p>怎么学习语言模型</p>
<ul>
<li><p>自回归模型Autogressive LM:</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230342821.png"
alt="image-20250106230342821" />
<figcaption aria-hidden="true">image-20250106230342821</figcaption>
</figure></li>
<li><p>Bi-gram/N-gram model:</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230354621.png"
alt="image-20250106230354621" />
<figcaption aria-hidden="true">image-20250106230354621</figcaption>
</figure></li>
<li><p>Marked LM:</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230523067.png"
alt="image-20250106230523067" />
<figcaption aria-hidden="true">image-20250106230523067</figcaption>
</figure></li>
<li><p>几种语言模型的学习方式</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230534541.png"
alt="image-20250106230534541" />
<figcaption aria-hidden="true">image-20250106230534541</figcaption>
</figure></li>
<li><p>为什么语言模型有比较好的表示</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230544997.png"
alt="image-20250106230544997" />
<figcaption aria-hidden="true">image-20250106230544997</figcaption>
</figure></li>
<li><p>总结</p>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230557957.png"
alt="image-20250106230557957" />
<figcaption aria-hidden="true">image-20250106230557957</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="attention机制和transformer">attention机制和transformer</h3>
<h4 id="attention来源翻译任务seq2seq">1.
attention来源：翻译任务seq2seq</h4>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230706080.png"
alt="image-20250106230706080" />
<figcaption aria-hidden="true">image-20250106230706080</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230717889.png"
alt="image-20250106230717889" />
<figcaption aria-hidden="true">image-20250106230717889</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230732617.png"
alt="image-20250106230732617" />
<figcaption aria-hidden="true">image-20250106230732617</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230745379.png"
alt="image-20250106230745379" />
<figcaption aria-hidden="true">image-20250106230745379</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230757155.png"
alt="image-20250106230757155" />
<figcaption aria-hidden="true">image-20250106230757155</figcaption>
</figure>
<h4 id="结构">2. 结构</h4>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230817984.png"
alt="image-20250106230817984" />
<figcaption aria-hidden="true">image-20250106230817984</figcaption>
</figure>
<figure>
<img
src="/2025/01/06/from-transformer-to-llm/image-20250106230830430.png"
alt="image-20250106230830430" />
<figcaption aria-hidden="true">image-20250106230830430</figcaption>
</figure>
<h3 id="训练lm">训练LM</h3>
<h3 id="pretrained-model用法微调和prompt">Pretrained
model用法（微调和prompt）</h3>
<h3 id="transformer除语言外其他应用">transformer除语言外其他应用</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/06/style-transfer-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/06/style-transfer-paper/" class="post-title-link" itemprop="url">style-transfer-paper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-06 00:21:27 / Modified: 21:25:32" itemprop="dateCreated datePublished" datetime="2025-01-06T00:21:27+08:00">2025-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="风格">风格</h1>
<p>AIGC领域中的一项重要子任务就是对图像进行风格化处理，<strong>一般涉及到对图像视觉外观和纹理进行编辑（被视为风格信息），同时保留其底层对象、结构和概念不变（被视为是内容信息）</strong>。为了达到这种编辑效果，就需要实现对图像中风格和内容进行分离。现有的方法通常需要训练专门的分离模型或者需要进行大量的优化，使用成本较高。</p>
<p>查看相关paper的网址：</p>
<p>https://www.paperdigest.org/article/?id=style_transfer</p>
<h3
id="南京航空航天大学diffusestunleashing-the-capability-of-the-diffusion-model-for-style-transfer">2024/10/19南京航空航天大学《DiffuseST:Unleashing
the Capability of the Diffusion Model for Style Transfer》</h3>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106101004946.png"
alt="image-20250106101004946" />
<figcaption aria-hidden="true">image-20250106101004946</figcaption>
</figure>
<p>提出了一种training-free的风格转移方法，balancing文本，图像style和图像content的特征结合起来。具体方法：</p>
<p>对于一张内容图像Ic,和一张风格图像Is</p>
<p>a.使用BLIP Diffusion中的BLIP-2的encoder产生Is的text-aligned
embedding的表示。</p>
<p>b.为了提取空间特征，在DDIM
inverse的过程中content和style分支保留U-net的feature</p>
<p>c.在diffusion模型的step-by-step过程中分离content和style的注入。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106101632557.png"
alt="image-20250106101632557" />
<figcaption aria-hidden="true">image-20250106101632557</figcaption>
</figure>
<h3 id="text-representation-extraction">Text representation
extraction</h3>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106105738256.png"
alt="image-20250106105738256" />
<figcaption aria-hidden="true">image-20250106105738256</figcaption>
</figure>
<p>风格的特征很难用text描述，而image features又很少作为sd模型的condition
input.</p>
<p>因此，先采用Blip-diffusion里的blip-2 encoder将style
image的特征表示为文本表示。此外，我们用clip的到content
image的embedding信息。</p>
<h3 id="spatial-representation-extraction">Spatial representation
extraction</h3>
<p>用DDIM算法提取图像特征。每一层u-net有一个残差块，一个self-attention模块去加强特征表示，一个cross-attention模块和text
condition交互。关注前两个blocks，调查发现主要保持图像空间的语义和结构特征。</p>
<h4 id="content和style的injection">content和style的injection</h4>
<p>用相应注入层unet的两层参数替换到target branch.</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106110415815.png"
alt="image-20250106110415815" />
<figcaption aria-hidden="true">image-20250106110415815</figcaption>
</figure>
<h3 id="implicit-style-content-separation-using-b-lora">《Implicit
Style-Content Separation using B-LoRA》</h3>
<h4 id="主要工作">主要工作：</h4>
<p>将LoRA(低秩适应)机制引入到图像编辑领域，提出了一种称为B-LoRA的框架，该框架可以隐式分离单个图像中的风格和内容组件，同时继承了LoRA的各种优势，包括轻量化训练和即插即用等功能。此外，作者通过深度分析现有流行扩散模型（Stable
Diffusion XL,
SDXL）的内部架构，发现仅需要联合设置两个B-LoRA块即可以实现图像内容和风格的分离，从而显著的提升各种下游图像风格化任务的性能和效果。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106112237612.png"
alt="image-20250106112237612" />
<figcaption aria-hidden="true">image-20250106112237612</figcaption>
</figure>
<p>提出了一种称为B-LoRA的风格转换框架，如上图所示，由于B-LoRA继承了原始LoRA的优势，具有高度的任务灵活性，同时不容易出现过拟合（仅优化模型注意层中新加入的低秩权重，预训练模型的参数保持冻结）。<strong>通过对SDXL内部结构进行分析，作者发现仅需要对两个特定的transformer层设置B-LoRA块就可以实现对图像内容和风格的分离</strong>。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106112514801.png"
alt="image-20250106112514801" />
<figcaption aria-hidden="true">image-20250106112514801</figcaption>
</figure>
<p>B-LoRA的另一个优点是即插即用的灵活性，它可以作为单独的组件应用到各种下游图像编辑任务中，而不需要任何额外的训练或微调。例如上图展示的风格迁移、文本引导的风格操作和条件图像生成等任务。</p>
<h4 id="方法">方法：</h4>
<h5 id="对sdxl结构分析">对sdxl结构分析</h5>
<p>sdxl是一个基于扩散的文本到图像生成模型，其主干网络采用了一个大型unet架构，由70个注意力层组成，这些注意力层可以被分成11个transformer块，前两个和最后三个块分别包含4个和6个注意力层，中间6个块各包含10个注意力层，细节如下图所示。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106161123542.png"
alt="image-20250106161123542" />
<figcaption aria-hidden="true">image-20250106161123542</figcaption>
</figure>
<p>SDXL可以接受文本作为条件进行生成，具体来说，给定文本提示，首先使用OpenClip
ViT-bigG和CLIP
ViT-L两个模型对其进行编码，然后将两个编码拼接起来作为最终的文本条件，随后将通过交叉注意力层馈入到网络中。由于本文的目标是将输入图像的风格和内容解耦为单独的信号再进行处理，因而需要对sdxl每个层对生成图像的风格或内容的贡献进行判定。</p>
<p>判定方法非常简单，即将不同的文本提示注入到每个sdxl
transformer块的交叉注意力层中，随后计算这些提示与生成图像之间的语义相似度。当只改变第i个块对应的输入提示时，如果观察到生成图像的变化较为明显，则表明该块对图像质量变化占主导地位。在实际操作时，作者重点检查了sdxl的6个中间transformer块，并且定义了两组随机的文本提示content和style,其中前者通过修改对象类别来定义内容，后者通过修改颜色来定义风格，然后使用CLIP来计算生成图像的变化程度。对于一对提示px,p，作者通过将变化提示px的嵌入注入到i块中，同时将原始p的嵌入注入到其它层中来生成新图像。对6个transformer块均执行后可以得到6幅图像，可以计算得到每一对提示的变化相似度得分：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106162626224.png"
alt="image-20250106162626224" />
<figcaption aria-hidden="true">image-20250106162626224</figcaption>
</figure>
<p>作者总共挑选了400对内容和风格提示进行了实验，实验结果表明sdxl模型中的第2，4个transformer块对生成内容的影响最大，而第5个块对生成风格的影响最大，如下图所示：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106175938180.png"
alt="image-20250106175938180" />
<figcaption aria-hidden="true">image-20250106175938180</figcaption>
</figure>
<h4 id="基于lora的内容和风格分离">基于lora的内容和风格分离</h4>
<p>基于上述发现，作者认为仅需要对第2，4和第5个块进行优化就可以实现隐层特征的解耦，而无需对整体模型微调。作者引入了Lora模块[1]来对这两部分进行单独优化，另w0表示预训练sdxl模型的冻结权重，令delta{w_i}表示每个块的低秩适应矩阵，优化过程主要分为两部分，第一部分优化delta{w_2}delta{w_5}，第二部分优化delta{w_4}delta{w_5}。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106180723828.png"
alt="image-20250106180723828" />
<figcaption aria-hidden="true">image-20250106180723828</figcaption>
</figure>
<p>优化过程和生成结果如上图所示，可以看出，delta{w_2}delta{w_4}更倾向于控制图像中内容信息，且delta{w_4}可以更好的捕捉到图像中的细节信息。作者将这种解耦方式称为B-LoRA,因为其只对两个Transformer块进行了Lora微调，这样可以节省70%的显存占用。</p>
<h4 id="b-lora的风格化操作">B-lora的风格化操作</h4>
<p>在实验图像内容和风格的解耦后，作者重点对delta{w_4}和delta{w_5}两层进行微调，其中delta{w_4}捕获内容，delta{w_5}捕获风格，通过微调他们的参数来实现图像的风格化操作，整体过程如下图所示：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106190120481.png"
alt="image-20250106190120481" />
<figcaption aria-hidden="true">image-20250106190120481</figcaption>
</figure>
<p>给定一个内容图像Ic和一个风格图像Is,分别学习它们对应的B-LoRA权重delta{w_4}和delta{w_5}.然后将这两个B-LoRA权重组合到预训练的sdxl模型中，就可以将Ic的内容与Is的风格进行融合，来生成一个新的风格化图像（如上图1所示）。为了实现文本为条件的图像风格化效果（如上图2所示），只要使用内容图像Ic对应的B-LoRA权重delta{w_4},将其与用户输入的文本提示进行融合就可以实现对图像风格的编辑，这样可以很好的保留Ic的内容特征。此外还可以通过排除delta{w_4}仅使用delta{w_5}的方式来调整模型仅关注图像Is中的特定风格，这样允许用户通过输入不同的文本来单独控制生成内容（如上图3所示）。</p>
<h4 id="效果">效果：</h4>
<p>（1）图像风格迁移：给定一个内容图像和一个风格图像，通过组合两个B-LoRA的权重实现风格迁移。</p>
<p>（2）基于文本的图像风格编辑：仅使用内容图像的B-LoRA权重，加上文本提示实现对图像风格的编辑。</p>
<p>（3）一致的风格生成：使用风格图像的B-LoRA权重，生成具有相同风格的新图像。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106200150033.png"
alt="image-20250106200150033" />
<figcaption aria-hidden="true">image-20250106200150033</figcaption>
</figure>
<p>其中前两行展示了图像风格迁移的效果，即要求模型迁移style图像中的风格，同时保留content图像中的内容。可以看到，本文的方法相比其他方法更加稳定。此外，第三行图像展示了基于文本的图像风格编辑的效果，可以看到本文方法对输入对象的内容进行了良好的保留。</p>
<h4 id="限制">限制：</h4>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106200410031.png"
alt="image-20250106200410031" />
<figcaption aria-hidden="true">image-20250106200410031</figcaption>
</figure>
<p>对一些风格和内容紧密结合的图像来说，风格信息对目标身份起到了决定性作用，因此当对这种图像中的内容进行风格化处理时，很容易丢失目标的身份信息，如上图(a)(b)所示。此外，B-LoRA在面对一些复杂场景时也会出现难以准确捕获场景结构的情况，如上图(c)所示。作者表明这些局限性可以通过进一步探索LoRA解耦的属性来解决，例如在解耦时考虑更加细粒度的结构、形状、颜色、纹理等属性。</p>
<h3
id="harnessing-the-latent-diffusion-model-for-training-free-image-style-transfer">《Harnessing
the latent Diffusion Model for Training-Free Image Style Transfer》</h3>
<p>先看一下效果：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106200730275.png"
alt="image-20250106200730275" />
<figcaption aria-hidden="true">image-20250106200730275</figcaption>
</figure>
<p>在unet做风格迁移时代，方法是finetune模型，计算量很大；作者借鉴了当时做style
transfer的一种方法，AdaIn;但是直接用到现在的diffusion
model里面受限于channels的数量而不能有很好的效果。作者提出了一种STRDP的算法，变化了LDM的去噪过程，将AdaIN用一种不同的方式重复应用到lantent
diffusion
model的unet结构里。这个算法是training-free的，而且可以保留原图的颜色。</p>
<h4 id="related-work">Related work</h4>
<ul>
<li>Direct Image Optimization:基于vgg
features设计style和content的loss,优化像素值。</li>
<li>Image Feature
Transformation:将一个content图像和一个image图像提取特征，然后在content
features里面添加style feature最后做个decode
AdaIN的方法。主要就是将风格数据转移到图像特征当中。也提出了一种网络结构，将图像特征的均值和方差替换掉content特征的均值和方差。</li>
<li>Diffusion Models:
<ul>
<li>重点是可控制性</li>
<li>Guidance:guided
diffusion得到一个提供的梯度作为去噪过程中的指导。需要附加的模型和额外的训练。</li>
<li>Additive control:Controlnet包含了一份克隆的diffusion
model和新的finuetune的参数。用额外的输入比如line
arts,depth去finetune这些新参数。</li>
<li>Tuning.通过fine-tune提供预训练LDM的可控制性。比如有的方法优化text
embedding包含style信息。</li>
</ul></li>
</ul>
<h3 id="背景知识">背景知识</h3>
<ul>
<li>Diffusion
Models:将数据x加噪到高斯，去噪过程让网络预测噪声从而恢复数据x</li>
<li>Latent Diffusion Models:LDM将数据x映射到latent
space中对应z，从而降低计算量。这种表示方法也有一个问题，传统的给diffusion
model加控制的方法，如果不经过额外的训练，无法直接用到LDM中。</li>
<li>Adaptive Instance Normalization:</li>
<li><figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106202947906.png"
alt="image-20250106202947906" />
<figcaption aria-hidden="true">image-20250106202947906</figcaption>
</figure></li>
</ul>
<p>仿照之前的AdaIn的方法，在提取过程中用风格的均值和方差替换content
feature的均值和方差，按照上面的公式，这个框架的缺陷就是要重新训练一个decoder去将输出转换为image.</p>
<h4 id="我们的方法">我们的方法</h4>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106204716157.png"
alt="image-20250106204716157" />
<figcaption aria-hidden="true">image-20250106204716157</figcaption>
</figure>
<p>方法：</p>
<p>首先style image和content image都会被转换到latent
space,然后都会经过前向的diffusion process,通过加噪产生一系列的noisy
latent
representations.再反向的diffusion过程中，不断运用AdaIN集成style和content的隐变量。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212104981.png"
alt="image-20250106212104981" />
<figcaption aria-hidden="true">image-20250106212104981</figcaption>
</figure>
<p>对降噪过程中u-net的参数执行AdaIn算法。</p>
<blockquote>
<p>[!NOTE]</p>
<p>均值和方差可以很大程度的影响风格转换效果，那么我想改变一张图片的风格，如果先对其进行去风格化，再进行风格嵌入，效果是不是会出奇的好呢？</p>
</blockquote>
<p>AdaIN全称为Adaptive Instance
Normalization,是一种图像处理技术，用于实现风格迁移。它的计算公式如下：</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212330208.png"
alt="image-20250106212330208" />
<figcaption aria-hidden="true">image-20250106212330208</figcaption>
</figure>
<h4 id="对比效果">对比效果</h4>
<p>保留了原图的颜色</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212355074.png"
alt="image-20250106212355074" />
<figcaption aria-hidden="true">image-20250106212355074</figcaption>
</figure>
<h5 id="参数控制">参数控制：</h5>
<p>S参数来控制在应用style
transfer之前我们对原始的content和noise加多少噪声。如果S=1，原图像加噪成高斯噪声，是无法重构出原图的。因此我们要控制好s的值。</p>
<figure>
<img src="/2025/01/06/style-transfer-paper/image-20250106212530108.png"
alt="image-20250106212530108" />
<figcaption aria-hidden="true">image-20250106212530108</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/hires-fix/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/hires-fix/" class="post-title-link" itemprop="url">hires-fix</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-05 23:53:34" itemprop="dateCreated datePublished" datetime="2025-01-05T23:53:34+08:00">2025-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-01-06 00:19:40" itemprop="dateModified" datetime="2025-01-06T00:19:40+08:00">2025-01-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://medium.com/rendernet/using-hires-fix-to-upscale-your-stable-diffusion-images-8d8e2826593e</p>
<p>https://medium.com/<span class="citation"
data-cites="realfabianw/taking-a-closer-look-at-the-highres-fix-function-of-automatic1111-to-optimize-image-quality-d9bbf369e3f">@realfabianw/taking-a-closer-look-at-the-highres-fix-function-of-automatic1111-to-optimize-image-quality-d9bbf369e3f</span></p>
<p>放大图片除了使用A1111's的图生图/extras选项，还可以使用Hires.Fix。后者可以在generate过程中增大分辨率，得到更好的效果。</p>
<h3 id="upscaler">Upscaler</h3>
<p>Latent, R-ESRGAN 4x+ and R-ESRGAN 4x+ Anime6B都可以尝试一下。</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250105235616276.png"
alt="image-20250105235616276" />
<figcaption aria-hidden="true">image-20250105235616276</figcaption>
</figure>
<figure>
<img src="/2025/01/05/hires-fix/image-20250105235627073.png"
alt="image-20250105235627073" />
<figcaption aria-hidden="true">image-20250105235627073</figcaption>
</figure>
<h3 id="hires-steps">Hires steps:</h3>
<p>Hires
steps在初试采样后refine图片质量，他们发生在每一步采样步之后的上采样过程，总的steps是sampling
steps + Hires steps.</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250106001527600.png"
alt="image-20250106001527600" />
<figcaption aria-hidden="true">image-20250106001527600</figcaption>
</figure>
<p>Hires steps的值是0-150，0的时候Hires steps = sampling
steps.所以如果你的sampling steps是20，hires
steps是0，那么总步数是40.选择合适的数值很重要，过低或者过高都会使得图片质量变差。一般10-15根据经验会得到更好的选择。但是如果采样步数大于50，hires
steps可以设置为sampling steps的一半。</p>
<h3 id="denoising-strength">Denoising strength</h3>
<p>对图片质量有很大的影响，也会改变图片内容。</p>
<p>设置为0对图片没有影响，设置成1会很大改变图片内容。</p>
<p>webui设置的默认值是0.7，但是在大多数case里这个设置都太强了，初始建议设置到0.3到0.5然后从上往下调整。</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250106001856856.png"
alt="image-20250106001856856" />
<figcaption aria-hidden="true">image-20250106001856856</figcaption>
</figure>
<p>用webui的xyz工具进行测试选出合适的scaler</p>
<figure>
<img src="/2025/01/05/hires-fix/image-20250106001935542.png"
alt="image-20250106001935542" />
<figcaption aria-hidden="true">image-20250106001935542</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/improve-rag/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/improve-rag/" class="post-title-link" itemprop="url">提高rag的技巧</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 23:48:54 / Modified: 23:51:07" itemprop="dateCreated datePublished" datetime="2025-01-05T23:48:54+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://readmedium.com/these-are-the-3-langchain-functions-i-used-to-improve-my-rag-00413ccb7094</p>
<ol type="1">
<li>Multi Query Retriever</li>
<li>Long Context Recorder</li>
<li>Contextual Compression</li>
</ol>
<figure>
<img src="/2025/01/05/improve-rag/image-20250105235015001.png"
alt="image-20250105235015001" />
<figcaption aria-hidden="true">image-20250105235015001</figcaption>
</figure>
<h3 id="long-context-reorder">Long-Context Reorder</h3>
<p>https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/long_context_reorder/</p>
<p>无论你的模型架构如何，当包含超过 <strong>10</strong>
个检索到的文档时，性能都会显著下降。简而言之：当模型必须在长上下文中访问相关信息时，它们往往会忽略提供的文档。</p>
<p>策略：</p>
<p>将不太相关的文档放在列表的中间，而更相关的文档放在开头和结尾。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/05/prompt-engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/05/prompt-engineering/" class="post-title-link" itemprop="url">Prompt Design and Engineering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-05 23:12:42 / Modified: 23:47:01" itemprop="dateCreated datePublished" datetime="2025-01-05T23:12:42+08:00">2025-01-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>[!TIP]</p>
<p>Prompt engineering:指令+questions+input+example</p>
</blockquote>
<h3 id="llms的limitations">LLMs的limitations</h3>
<ul>
<li>短暂记忆</li>
<li>输出不一致性</li>
<li>过期信息</li>
<li>内容捏造</li>
<li>领域专业性</li>
</ul>
<h3 id="一些建议和技巧">一些建议和技巧</h3>
<ul>
<li>CoT prompting</li>
<li>让模型通过其他方式输出真实的东西</li>
<li>直接结束prompt</li>
<li>强势的态度</li>
<li>让ai自我纠正</li>
<li>产生不同的观点</li>
<li>保持状态+角色扮演（gpt的web端维护了一个session，api没有实现）</li>
<li>教算法</li>
<li><strong>LLMs的本质是往前读并补全文本，因此examples的顺序和prompt的顺序很重要。</strong></li>
<li>Affordances:达到一定条件触发的函数</li>
</ul>
<h3 id="高级建议和技巧">高级建议和技巧</h3>
<ul>
<li>CoT
<ul>
<li>Zero-shot: lets think step by step...</li>
<li>Manual-Cot:</li>
</ul></li>
<li>ToT(Tree of thought)</li>
</ul>
<h3 id="工具">工具：</h3>
<ul>
<li>ART</li>
<li>通过自一致性增强依赖性</li>
<li>反思</li>
<li>专家prompt：prompt chainer</li>
<li>Streamlining Complex Tasks with Chains</li>
<li>引导性的输出with rails</li>
<li>流水线的Prompt设计with自动的prompt engineering</li>
</ul>
<h3 id="argmenting-llms-through-external-knowledge--rag">Argmenting LLMs
through External Knowledge -RAG</h3>
<ul>
<li>RAG-aware Prompting Techniques</li>
</ul>
<h3 id="llm-agents">LLM Agents</h3>
<h4 id="agents">Agents</h4>
<ul>
<li>Reasoning without Observation</li>
<li>Reason And Act</li>
<li>Dialog-Enabled Resolving Agents</li>
</ul>
<h3 id="工具和框架">工具和框架</h3>
<ul>
<li>Langchain</li>
<li>Semantic Kernel</li>
<li>The Guidance</li>
<li>Nemo Guardrails</li>
<li>LlamaIndex</li>
<li>FastRAG</li>
<li>Auto-GPT</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">67</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
