<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="下面是来自medium的一篇文章的几乎翻译  [!NOTE] 1.目前的vla本质还是数据采样模仿学习，因此环境条件、物体状态等一变准确率就急速下降；与此相对应的腕部相机表现强劲，因为腕部相机更能克服干扰。要提高泛化就得堆数据。 2.lang token在其中几乎不起到语义理解作用，language变化和指令变化几乎不影响动作行为。vla没有语言和现实机器人操作对应起来的理解能力。 https:&#x2F;">
<meta property="og:type" content="article">
<meta property="og:title" content="vla in robot鲁棒性探索">
<meta property="og:url" content="http://77philosophia.github.io/2026/01/22/vla-valuable/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:description" content="下面是来自medium的一篇文章的几乎翻译  [!NOTE] 1.目前的vla本质还是数据采样模仿学习，因此环境条件、物体状态等一变准确率就急速下降；与此相对应的腕部相机表现强劲，因为腕部相机更能克服干扰。要提高泛化就得堆数据。 2.lang token在其中几乎不起到语义理解作用，language变化和指令变化几乎不影响动作行为。vla没有语言和现实机器人操作对应起来的理解能力。 https:&#x2F;">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*PBizSwIa2GxGOKh9C38s4A.png">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*eJcsziTFJlMEEWqqbxl3xQ.png">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*PsC0_dkqtw_AYNnasuvx3Q.png">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*Xv8KhRSINYgNNPuLsCh0hw.png">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*yzWMERMGBRHWMB66ayyyVg.png">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*kGUB7dCH1RNNANU9CLEn1g.png">
<meta property="og:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*-baS0o5UJO-zm-Re7P-R_A.png">
<meta property="article:published_time" content="2026-01-22T05:38:40.000Z">
<meta property="article:modified_time" content="2026-01-22T06:07:04.148Z">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://77philosophia.github.io/2026/01/22/vla-valuable/1*PBizSwIa2GxGOKh9C38s4A.png">

<link rel="canonical" href="http://77philosophia.github.io/2026/01/22/vla-valuable/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>vla in robot鲁棒性探索 | Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/22/vla-valuable/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          vla in robot鲁棒性探索
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-22 13:38:40 / Modified: 14:07:04" itemprop="dateCreated datePublished" datetime="2026-01-22T13:38:40+08:00">2026-01-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>下面是来自medium的一篇文章的几乎翻译</p>
<blockquote>
<p>[!NOTE]</p>
<p>1.目前的vla本质还是数据采样模仿学习，因此环境条件、物体状态等一变准确率就急速下降；与此相对应的腕部相机表现强劲，因为腕部相机更能克服干扰。要提高泛化就得堆数据。</p>
<p>2.lang
token在其中几乎不起到语义理解作用，language变化和指令变化几乎不影响动作行为。vla没有语言和现实机器人操作对应起来的理解能力。</p>
<p>https://www.alphaxiv.org/zh/overview/2510.03827</p>
<p>https://www.alphaxiv.org/zh/overview/2510.13626</p>
</blockquote>
<p>vla的几种方案：</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.09246">自回归方法：将机器人动作离散化为标记，并在大规模演示（如Openvla</a>和<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10900471/">Tinyvla）</a>上训练端到端策略；</li>
<li>基于扩散的模型：通过生成式扩散专家（例如 pi
序列）生成连续轨迹（https://www.pi.website/blog）</li>
<li>强化学习方法：超越监督式微调，例如<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18719">Vla-rl</a>。</li>
</ul>
<p>然而，这些训练好的VLA模型仅在非常熟悉的场景和环境中展现出一定的性能，缺乏真正的泛化能力。VLA模型的迁移性非常低，同时我们也注意到，现有的基准测试缺乏对分布变化下模型性能的全面评估，这凸显了在多样化条件下进行系统、细粒度鲁棒性评估的必要性。</p>
<p>最近，我偶然发现了一篇名为 LIBERO-plus
的论文，它通过系统性的参数变化和各种扰动维度，对当前的 VLA
模型进行了详细的脆弱性分析，深入剖析了模型在不同条件下的性能。如果您感兴趣，请访问<a
target="_blank" rel="noopener" href="https://sylvestf.github.io/LIBERO-plus/">https://sylvestf.github.io/LIBERO-plus/查看更多详情。</a></p>
<p>这项工作并非创建更多任务，而是主要集中于引入扰动因子，以研究不同的扰动因子如何影响
VLA 的性能，并将其应用于测试案例/阶段：</p>
<h2 id="扰动因子">扰动因子：</h2>
<ul>
<li><strong>对象布局</strong>：添加混淆对象（随机向任务场景中添加 n
个额外的未见对象）和/或改变目标对象的位置（x、y、z）和方向（俯仰、偏航、滚动）。</li>
<li><strong>背景纹理</strong>：改变环境的场景纹理（例如，从彩绘墙到砖墙），并随机改变工作表面的纹理（例如，桌面或地板）。</li>
<li><strong>光照条件</strong>：改变光照强度、方向、颜色和阴影图案，以影响场景风格。</li>
<li><strong>相机视角</strong>：改变第三人称相机的视角/姿态和视野——改变相机距离、球形位置和相机方向。</li>
<li><strong>机器人初始状态</strong>：改变机械臂的初始关节位置（qpos）。</li>
<li><strong>语言说明</strong>：重写任务说明，以增加语言的丰富性和复杂性，例如重写为更长、更口语化的形式，其中包含额外的但与任务无关的上下文线索，或涉及推理的复杂性。</li>
<li><strong>传感器噪声：</strong>注入光度失真，以评估在输入质量下降的情况下的鲁棒性，包括运动模糊、高斯模糊、变焦模糊、玻璃模糊以及雾霾。</li>
</ul>
<p>上述扰动是通过修改场景 XML 定义文件或修改任务描述文件 (BDDL)
来实现的。</p>
<h2 id="已测试的vla模型概述">已测试的VLA模型概述</h2>
<ul>
<li><strong>OpenVLA（Kim 等，2024）和 OpenVLA-OFTs（Kim
等，2025）</strong>：该系列 VLA 基于 Prismatic-7B VLM 构建——VLM 是一个由
SigLIP 和 DINOv2 组成的视觉编码器，其输出与 Llama2-7B
语言骨干的输入空间融合，该语言骨干通过交叉注意力机制整合视觉和文本输入。为了使
VLM 骨干适应机器人控制，连续的机器人动作被离散化为每个维度 256
个区间，并以标记的形式表示在 LLM 词汇表中。Llama 分词器中最不常用的 256
个标记被替换为动作标记，训练过程采用应用于动作序列的标准下一个标记预测目标（因此训练损失为交叉熵）。OpenVLA
在 Open X-Embodiment (OpenX) 数据集上进行了预训练。</li>
<li><strong>π0（Black 等人）和 π0-fast（Pertsch
等人，2025）</strong>：核心模型包含一个用于语义理解多模态输入（多张 RGB
图像、语言指令和本体感觉状态）的 VLM
基类（PaliGemma），并将动作标记投影并路由到一个更小的动作专家模型。该框架类似于
Transfusion 框架，后者使用多个目标函数训练单个
Transformer：连续输出标记使用流匹配损失，离散标记使用交叉熵损失。</li>
<li><strong>Nora（Hung et al.,
2025）</strong>：它采用Qwen-2.5-VL-3B多模态模型作为骨干网络，该模型具有强大的视觉语义理解能力。它使用FAST+分词器将连续的动作标记离散化，从而输出离散的动作序列。此外，它还在Open
X-Embodiment数据集上进行了预训练。</li>
<li><strong>WorldVLA（Cen等人，2025）：</strong>一种自回归动作-世界模型，它将视觉-语言-动作建模和世界建模统一到一个集成框架中。其核心思想是联合学习用于动作生成的策略模型和用于未来状态预测的世界模型，从而使这两个组件能够相互增强。该模型基于Chameleon初始化，Chameleon是一个统一的图像理解和生成模型。它采用了三个分词器：一个基于VQ-GAN的图像分词器、一个基于BPE的文本分词器，以及一个动作分词器，该动作分词器将连续机器人动作的每个维度离散化为256个区间。所有模态（文本、图像、动作）都被离散化为词元，并在统一的序列中进行自回归建模。在训练过程中，动作建模数据训练模型，使其能够根据语言指令和图像观测历史生成动作块，损失函数仅基于动作词元计算。世界建模数据用于训练模型，使其能够根据当前图像和动作预测下一帧图像，损失函数仅基于
图像标记计算。这种联合训练策略鼓励学习共享表征：世界模型获取环境物理信息以辅助生成与任务相关的动作，而动作模型则增强视觉理解以支持准确的帧预测。</li>
<li><strong>UniVLA（Li等人，2025）：</strong>该架构基于预训练的Prismatic-7B
VLM模型构建。其关键创新在于扩展了LLM的词汇表，引入了特殊的动作标记来表示量化的潜在动作。该模型以视觉观察和语言指令作为输入，并自回归地预测一系列潜在动作标记。</li>
<li><strong>RIPT-VLA（Brohan 等人，2022）：</strong>其基础模型为
OpenVLA-OFT。这项工作通过添加一个轻量级的辅助头来增强模型，该辅助头用于预测动作分布的尺度参数
σθ，使其与强化学习 (RL) 兼容。在标准的预训练和监督微调 (SFT)
阶段之后，它引入了强化交互式后训练 (RIPT)
的第三阶段。该策略的核心是动态采样留一法后训练优化 (LOOP)
框架——一种动态拒绝机制，它过滤掉所有 K
次展开都获得相同奖励（全部成功或全部失败）的上下文样本，从而确保训练批次包含有意义的学习信号。</li>
</ul>
<h2 id="基准建设">基准建设</h2>
<p><a
target="_blank" rel="noopener" href="https://libero-project.github.io/main.html">LIBERO</a>最初包含 40
个评估任务，分为四个泛化子任务（空间、物体、目标、长距离）。这项工作为每个子任务生成了
500 个实例，涵盖七个扰动维度，因此总共有 500 * 4 * 7 = 14,000
个任务。之后，为了避免天花板效应，我们移除了所有模型或绝大多数模型都能解决的任务。然后，作者进一步平衡了剩余任务在各个增强子维度上的分布，以防止偏差。最终的测试基准数据集包含
10,030 个任务，涵盖所有七个维度。</p>
<p>按回车键或点击查看完整尺寸的图片</p>
<figure>
<img src="/2026/01/22/vla-valuable/1*PBizSwIa2GxGOKh9C38s4A.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="/2026/01/22/vla-valuable/1*eJcsziTFJlMEEWqqbxl3xQ.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="结果">结果</h2>
<p>研究结果共同揭示了当前甚大阵列（VLA）泛化能力的显著缺陷。如图所示，即使是微小的扰动也会导致性能急剧下降。</p>
<p>按回车键或点击查看完整尺寸的图片</p>
<figure>
<img src="/2026/01/22/vla-valuable/1*PsC0_dkqtw_AYNnasuvx3Q.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>发现
1：在各种输入扰动下，性能会显著下降，尤其是在相机视角和机器人初始状态发生变化时。</li>
<li>发现2：模型最容易受到相机视角和机器人初始状态变化的影响，这需要对空间几何和本体感觉有较高的理解水平。相比之下，模型对光照和背景变化表现出相对的适应能力，因为这些变化属于更表面、更低层次的视觉变化。</li>
<li>发现
3：语言扰动导致大多数模型的平均性能下降幅度第二小（-25.3），这表明模型可能比预期的更少依赖语言指令，并可能利用视觉环境中的任务线索。</li>
<li>发现 4：与仅依赖第三人称视角的模型（例如
OpenVLA-OFT_w）相比，采用第一人称腕部摄像头的模型（例如
OpenVLA-OFT）展现出更优异的泛化能力，尤其是在应对摄像头视角变化时。此外，强调多样性和协同训练的训练策略（例如
π0,π0-fast）能够持续产生在多种扰动类型下都更稳健的模型，这凸显了接触不同数据分布的重要性。</li>
<li>发现
5：虽然模型表现出能够忽略干扰物体的能力，但当目标物体发生位移时，它们却无法进行泛化，这表明它们依赖于记忆的位置线索，而不是学习不变的物体语义。</li>
<li>发现
6：在光照扰动下性能的相对稳定性主要归功于腕部摄像头的近距离视角，它提供了与光照无关的几何线索。缺乏腕部摄像头输入的模型对光照变化表现出显著更高的敏感性。</li>
<li>发现7：VLA模型不具备强大的跨对象指令执行泛化能力。在目标被替换的任务中，模型的成功率几乎降至零。例如，原任务指令“<em>拿起字母汤”</em>被替换为<em>“拿起番茄酱”。</em></li>
<li>发现 8：VLA
模型似乎更依赖于固定的视觉-动作映射，而不是充分利用语言信号进行任务决策。</li>
</ul>
<p>另一篇论文也得出了同样的结论，即
libero-pro，https://zxy-mllab.github.io/LIBERO-PRO-Webpage/</p>
<p>以下是其研究结果，与上述内容类似：</p>
<ul>
<li>该模型能否推广到新对象？</li>
</ul>
<figure>
<img src="/2026/01/22/vla-valuable/1*Xv8KhRSINYgNNPuLsCh0hw.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>当用一个不相关的物品（例如，字母
汤，第三行）替换沙拉酱时，模型仍然执行相同的动作轨迹，试图拿起这个与指令不符的新物品。此外，它在语义无关的物品上的失败
也凸显了其无法正确地将语言指称与物品语义相匹配。</p>
<ul>
<li>该模型是否适用于各种指令？</li>
</ul>
<figure>
<img src="/2026/01/22/vla-valuable/1*yzWMERMGBRHWMB66ayyyVg.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>当指令被替换为无意义的输入（例如，“xxx”）时，模型仍然产生相同的动作序列，再次检索并执行将沙拉酱放入篮子的模式。因此，模型在释义指令和无意义指令下表现出的一致性，不应被视为成功泛化到语义扰动的结果，而应被视为其无法理解或解释指令的证据，表明其过度依赖于机械的模式执行。</p>
<ul>
<li>该模型对物体放置位置的敏感度如何？</li>
</ul>
<figure>
<img src="/2026/01/22/vla-valuable/1*kGUB7dCH1RNNANU9CLEn1g.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>该图显示了 VLA
对物体初始位置的高度敏感性——将杯子放在稍微不同的位置可能会导致任务完全失败。</p>
<p>总之，当我们看到这些令人印象深刻的指标时，我们应该停下来问问自己——它们真的那么有效吗？还是只是过度拟合死记硬背的结果？</p>
<p>按回车键或点击查看完整尺寸的图片</p>
<figure>
<img src="/2026/01/22/vla-valuable/1*-baS0o5UJO-zm-Re7P-R_A.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>摘自论文：https://zxy-mllab.github.io/LIBERO-PRO-Webpage/</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2026/01/22/no-look-back/" rel="prev" title="no-look-back">
      <i class="fa fa-chevron-left"></i> no-look-back
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%B0%E5%8A%A8%E5%9B%A0%E5%AD%90"><span class="nav-number">1.</span> <span class="nav-text">扰动因子：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%B2%E6%B5%8B%E8%AF%95%E7%9A%84vla%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="nav-number">2.</span> <span class="nav-text">已测试的VLA模型概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E5%87%86%E5%BB%BA%E8%AE%BE"><span class="nav-number">3.</span> <span class="nav-text">基准建设</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C"><span class="nav-number">4.</span> <span class="nav-text">结果</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description">我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
