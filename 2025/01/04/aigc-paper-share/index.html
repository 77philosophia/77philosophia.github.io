<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Stable diffusion 原理 从data产生noise很容易，从noise产生data是生成   image-20250104222656242  一致性保持目标：   image-20250104222713620  解法： StoryDiffusion：做保持   image-20250104222809450  产出这些漫画的研究出自南开大学、字节跳动等机构。在《StoryDiff">
<meta property="og:type" content="article">
<meta property="og:title" content="AIGC论文reading">
<meta property="og:url" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:description" content="Stable diffusion 原理 从data产生noise很容易，从noise产生data是生成   image-20250104222656242  一致性保持目标：   image-20250104222713620  解法： StoryDiffusion：做保持   image-20250104222809450  产出这些漫画的研究出自南开大学、字节跳动等机构。在《StoryDiff">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222656242.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222713620.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222809450.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222859537.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222920431.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222948149.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223125232.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223145816.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223207497.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223217116.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223238466.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223305875.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223331480.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223346380.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223537655.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223550213.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223626463.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223722467.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223810670.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223829556.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223847906.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223924036.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104223944051.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224024890.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224046351.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224101887.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224109475.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224123560.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224131380.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224146486.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224202197.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224209991.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224217340.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224244780.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224306153.png">
<meta property="og:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104224313553.png">
<meta property="article:published_time" content="2025-01-04T14:26:20.000Z">
<meta property="article:modified_time" content="2025-01-04T14:43:16.651Z">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://77philosophia.github.io/2025/01/04/aigc-paper-share/image-20250104222656242.png">

<link rel="canonical" href="http://77philosophia.github.io/2025/01/04/aigc-paper-share/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AIGC论文reading | Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/01/04/aigc-paper-share/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AIGC论文reading
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-01-04 22:26:20 / Modified: 22:43:16" itemprop="dateCreated datePublished" datetime="2025-01-04T22:26:20+08:00">2025-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="stable-diffusion-原理">Stable diffusion 原理</h2>
<p>从data产生noise很容易，从noise产生data是生成</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222656242.png"
alt="image-20250104222656242" />
<figcaption aria-hidden="true">image-20250104222656242</figcaption>
</figure>
<h2 id="一致性保持目标">一致性保持目标：</h2>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222713620.png"
alt="image-20250104222713620" />
<figcaption aria-hidden="true">image-20250104222713620</figcaption>
</figure>
<h1 id="解法">解法：</h1>
<h2 id="storydiffusion做保持">StoryDiffusion：做保持</h2>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222809450.png"
alt="image-20250104222809450" />
<figcaption aria-hidden="true">image-20250104222809450</figcaption>
</figure>
<p>产出这些漫画的研究出自南开大学、字节跳动等机构。在《StoryDiffusion：Consistent
Self-Attention for long-range image and video
generation》这篇论文中，该研究团队提出了一种名为 StoryDiffusion
的新方法，用于生成一致的图像和视频以讲述复杂故事。</p>
<ul>
<li><p>论文地址：https://arxiv.org/pdf/2405.01434v1</p></li>
<li><p>项目主页：https://storydiffusion.github.io/</p></li>
</ul>
<p>如上图所示，使用Consistent
Self-Attention生成的图像成功保持了身份和服装的一致性，这对于讲故事至关重要。</p>
<h4 id="主要亮点一致性图像生成">主要亮点：一致性图像生成</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222859537.png"
alt="image-20250104222859537" />
<figcaption aria-hidden="true">image-20250104222859537</figcaption>
</figure>
<h4 id="总结">总结：</h4>
<p>应用场景：小批次人物服装、身份的保持</p>
<h2
id="omg多人物图像生成的工具垫图-区域分离重绘">OMG：多人物图像生成的工具（垫图
+ 区域分离重绘）</h2>
<p>主页: https://kongzhecn.github.io/omg-project/</p>
<p>代码: https://github.com/kongzhecn/OMG/</p>
<h4 id="背景">背景：</h4>
<p>单概念的定制化生成，Textual Inversion，
LORA，InstanceID等方法已经比较成熟了。而对于多概念的定制化生成，如果直接使用LORA融合等方法，主要挑战在于不同的概念信息之间的信息泄漏，造成属性混乱。比如想要生成一个特定的黑色长发男角色，一个特定的紫色短发女角色，在单图出单角色时正常，但是在单图出双角色时，很有可能发型发色信息就混乱了，比如可能出现紫色长发女角色。比较容易想到的做法就是垫图+掩码。</p>
<p>难点：个性化保持、空间遮挡</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222920431.png"
alt="image-20250104222920431" />
<figcaption aria-hidden="true">image-20250104222920431</figcaption>
</figure>
<p>多概念个性化（multi-concept
personalization）指的是在同一个图像生成过程中，同时整合和表达多个不同的概念或主题。这种方法要求在生成图像时，能够准确保留和表现每个概念的独特特征，同时保证它们之间的协调和和谐。例如，在生成一个包含多个不同人物、背景和物体的图像时，必须确保每个元素都能清晰地展现其独特性，并且整体图像具有一致性和美感。</p>
<h4 id="先看效果">先看效果：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104222948149.png"
alt="image-20250104222948149" />
<figcaption aria-hidden="true">image-20250104222948149</figcaption>
</figure>
<h4 id="即插即用">即插即用</h4>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr>
<th><strong>OMG +</strong> <strong>LoRA</strong> <strong>(ID with
multiple images)</strong></th>
<th><img src="/2025/01/04/aigc-paper-share/image-20250104223125232.png"
alt="image-20250104223125232" /></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OMG + InstantID (ID with single image)</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223145816.png"
alt="image-20250104223145816" /></td>
</tr>
<tr>
<td><strong>OMG + ControlNet (Layout Control )</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223207497.png"
alt="image-20250104223207497" /></td>
</tr>
<tr>
<td><strong>OMG + style LoRAs (Style Control)</strong></td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223217116.png"
alt="image-20250104223217116" /></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="怎么做到">怎么做到：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223238466.png"
alt="image-20250104223238466" />
<figcaption aria-hidden="true">image-20250104223238466</figcaption>
</figure>
<p>OMG是一种两阶段的生成方法，一阶段先用无角色信息的文生图模型垫图生成布局，然后提取垫图全面的视觉信息（mask
和 attention map），二阶段将各角色的特定概念信息作用于对应的 mask
区域，避免信息泄露，属性错乱，并复用一阶段的 attention
map，维持构图布局不变。这里的概念信息可以是 LoRA 也可以是
InstantID。</p>
<h5
id="一阶段生成构图布局并提取视觉信息">1.一阶段：生成构图布局并提取视觉信息</h5>
<p>首先用一个全局的prompt p在文生图模型T2I上生成一张垫图</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223305875.png"
alt="image-20250104223305875" />
<figcaption aria-hidden="true">image-20250104223305875</figcaption>
</figure>
<p>这里要注意的是，没有任何角色信息(lora/InstanceId)</p>
<p>第一阶段，我们要提取垫图的视觉信息，这里的视觉信息有两项：生成过程中每一时间步每一层的注意力图
attention map
和每个角色的分割掩码。注意力图在生图过程中记得保存即可，而角色的掩码图，则是基于角色类别的基本词（man、woman），使用开集检测分割的模型（如
GroundingDION+SAM、YoloWorld+EfficientSAM）来进行分割。</p>
<h5 id="第二阶段多概念定制化去噪">第二阶段：多概念定制化去噪</h5>
<p>为了避免概念信息泄露，造成角色属性错乱，OMG
在进行第二阶段多概念定制化生成时不进行 LoRA
融合，而是使用多个单概念的模型分别进行推理，并作用于一阶段得到的各自掩码区域中。即一个
LoRA 只负责一个角色区域的生成。这称为概念噪声混合（Concept Noise
Blending）。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223331480.png"
alt="image-20250104223331480" />
<figcaption aria-hidden="true">image-20250104223331480</figcaption>
</figure>
<h5 id="不同时间步开启混合">不同时间步开启混合：</h5>
<p>下图对比了第二阶段中不同时间步开启概念噪声混合对最终生成结果的影响，最左侧是从一开始，第
50 步（因为用了 DDIM 采样器）就开启概念噪声混合，最右侧表示第 0
步才开启，相当于没开启，即完全等于第一阶段的结果，中间是 0-50
步之间开启。可以看到，概念噪声混合开启得越早，概念特征保持得越好。并且，开启得越早对图像构图布局的影响也越大，开启的越晚，构图布局与第一阶段结果越相近。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223346380.png"
alt="image-20250104223346380" />
<figcaption aria-hidden="true">image-20250104223346380</figcaption>
</figure>
<h4 id="总结-1">总结：</h4>
<p>想要在有交叠的情况下，精确地控制多个概念的属性特征，基本思路是 ”垫图
+ 区域分离重绘“
的方案，但是这种方法会有个问题：如果多个概念的基本语义类是一样的，比如两个
woman，这时候 zero-shot
分割模型怎么工作，对于相同语义类的多概念很难进行可控的个性化生成</p>
<h2
id="threatergen多轮对话llmimage-generation">ThreaterGen：多轮对话（LLM+image
generation）</h2>
<p><strong>https://howe140.github.io/theatergen.io/</strong></p>
<h4 id="背景-1">背景：</h4>
<ol type="1">
<li>语义一致性——现有方法在处理复杂描述（如空间关系、数量或指代表达“它们”等）时遇到困难，导致生成的图像在语义上与用户请求不一致；(2)
上下文一致性——在多轮对话中的图像生成过程中，同一实体经常难以在不同轮次中保持一致特征，甚至可能被遗忘。例如，同一只狗在不同轮次中看起来不同而没有用户编辑。</li>
</ol>
<h4 id="先看效果-1">先看效果：</h4>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 69%" />
</colgroup>
<thead>
<tr>
<th>讲故事</th>
<th><img src="/2025/01/04/aigc-paper-share/image-20250104223537655.png"
alt="image-20250104223537655" /></th>
</tr>
</thead>
<tbody>
<tr>
<td>多轮编辑</td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223550213.png"
alt="image-20250104223550213" /></td>
</tr>
<tr>
<td>定量：在自己的数据集上测试</td>
<td><img src="/2025/01/04/aigc-paper-share/image-20250104223626463.png"
alt="image-20250104223626463" /></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="怎么做到-1">怎么做到：</h4>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223722467.png"
alt="image-20250104223722467" />
<figcaption aria-hidden="true">image-20250104223722467</figcaption>
</figure>
<p>本文提出的 ppl 如图，可以分为三个阶段</p>
<p>1.第一阶段是 LLM 的角色设计：用 LLM 完成角色设计，包括角色的外观描述
+ layout 信息。</p>
<p>2.第二阶段用 T2I 完成第一阶段角色的图片生成和 latent 提取，作为
reference img</p>
<p>3.第三阶段整合前两个阶段的信息，生成符合用户输入 prompt 的图片。</p>
<hr />
<h5 id="第一阶段llm-角色设计">1.第一阶段，LLM 角色设计</h5>
<p>在这一阶段，利用 LLM，根据用户输入的要求，把对应 prompt
的信息做格式化，格式化的输出包含了背景 prompt、neg
prompt，另外最核心的部分是各个主体的 prompt。每一个主体 prompt 都是一个
triplet 对，是一个包含了 id、prompt、layout（bbox 格式）的三元组。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223810670.png"
alt="image-20250104223810670" />
<figcaption aria-hidden="true">image-20250104223810670</figcaption>
</figure>
<h5 id="第二阶段reference-img-生成">2.第二阶段，reference img 生成</h5>
<p>根据第一阶段给出的主体 prompt，用 T2I
模型生成一张对应角色的参考图片，这张参考图后续会通过 ip-adapter
来注入模型。有了 reference img，就可以针对各个 prompt
生成对应的图片，步骤如下：</p>
<p>a）根据 ref img + 待生成
prompt，可以生成一张（只）包含该主体的图片</p>
<p>b）对上述图片做分割，得到前景的物体。因为一条 prompt
里面可能包含多个物体，所以需要对每一个出现的物体都执行第一、二步。</p>
<p>c）根据 LLM 给出的 layout
信息，将第二部去除背景的主体贴在对应位置上（需要做 scale 的适配）。</p>
<p>d）然后对第三步得到的图片，计算 canny 图，另外经过一次 SD
的前传得到每一步的 feature 作为生成的 guidance（称为 latent
guidance）。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223829556.png"
alt="image-20250104223829556" />
<figcaption aria-hidden="true">image-20250104223829556</figcaption>
</figure>
<h5 id="第三阶段信息整合图片生成">第三阶段，信息整合+图片生成</h5>
<p>在这一阶段，会把上述格式化的 prompt 合并，作为 SD 输入的 text
prompt。canny 信息通过 controlnet 注入 SD。而 lantent guidance
注入方式如下，会选择在一定的 step
范围内注入而不是全过程都加。注入的时候，主体 mask 区域内用 latent
guidance，mask外则用 SD 本身生成的内容。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223847906.png"
alt="image-20250104223847906" />
<figcaption aria-hidden="true">image-20250104223847906</figcaption>
</figure>
<h4 id="总结-2">总结：</h4>
<p>1.利用 LLM + T2I 的能力来做连续故事的 ip 保持。通过 LLM
保证生成故事上下文的连续性，以及人物的关联性；通过 LLM
来生成主体的特征描述，将这个特征描述送给 T2I
模型生成该主体的一张参考图。LLM 还被用来生成图片的
layout，用来控制每一个主体的生成位置。</p>
<p>2.多轮生成过程中，允许用户通过外部输入来改变生成结果，允许用户交互</p>
<h2
id="autostudio在多轮交互式图像生成中打造一致的主体">AutoStudio：在多轮交互式图像生成中打造一致的主体</h2>
<h4 id="背景-2">背景：</h4>
<p>文本到图像(TGI)生成模型在生成单张图像方面已经表现出色,更具挑战性的任务——多轮交互式图像生成开始吸引相关研究社区的注意.这个任务要求模型在多个回合中与用户互动，以生成连贯的图像序列。然而，由于用户可能频繁切换主体，目前的努力难以在生成多样化图像的同时保持主体一致性。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223924036.png"
alt="image-20250104223924036" />
<figcaption aria-hidden="true">image-20250104223924036</figcaption>
</figure>
<h4 id="效果">效果：</h4>
<h5
id="量化指标autostudio在所有指标上都明显优于之前的方法">1.量化指标：AutoStudio<strong>在所有指标上都明显优于之前的方法</strong>。</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104223944051.png"
alt="image-20250104223944051" />
<figcaption aria-hidden="true">image-20250104223944051</figcaption>
</figure>
<h5 id="可视化">2.可视化：</h5>
<p>Theatergen无法处理人物之间复杂的互动（如拥抱和接吻），而MiniGemini则难以保持主体的一致性。</p>
<p>Intelligent
Grimm和StoryDiffusion无法在多回合互动中保持多个角色之间的一致性，并表现出有限的编辑效果。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224024890.png"
alt="image-20250104224024890" />
<figcaption aria-hidden="true">image-20250104224024890</figcaption>
</figure>
<h4 id="怎么做到-2">怎么做到：</h4>
<p>他们的目标是引入一个多功能、可扩展的框架，通过多智能体协作，可以将任何所需的LLM架构和扩散骨干结合到框架中，以满足用户多轮生成的多样化需求。</p>
<p>具体而言，AutoStudio包括三个基于LLM的智能体：</p>
<ul>
<li><p><strong>主题管理器</strong>解释对话，识别不同的主题，并为其分配适当的上下文；-》ID，prompt</p></li>
<li><p><strong>布局*<em>*</em>生成器</strong>为每个主题生成部分级别的边界框，以控制主题的位置；-》coarse
layout</p></li>
<li><p><strong>监督员</strong>为布局生成器提供布局改进和修正的建议。</p></li>
</ul>
<p>最后，<strong>绘制器</strong>基于扩散模型完成基于改进布局的图像生成。</p>
<p>此外，研究人员在绘制器中引入了一个<strong>并行*<em>*</em>UNet</strong>（P-UNet），它具有一种新颖的架构，利用两个并行的交叉注意力模块分别增强文本和图像嵌入的潜在主题特征。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224046351.png"
alt="image-20250104224046351" />
<figcaption aria-hidden="true">image-20250104224046351</figcaption>
</figure>
<h5 id="主题管理器">主题管理器：</h5>
<p>1.历史输入 2.预定义prompt</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224101887.png"
alt="image-20250104224101887" />
<figcaption aria-hidden="true">image-20250104224101887</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224109475.png"
alt="image-20250104224109475" />
<figcaption aria-hidden="true">image-20250104224109475</figcaption>
</figure>
<h5 id="layout管理器">layout管理器：</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224123560.png"
alt="image-20250104224123560" />
<figcaption aria-hidden="true">image-20250104224123560</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224131380.png"
alt="image-20250104224131380" />
<figcaption aria-hidden="true">image-20250104224131380</figcaption>
</figure>
<h5 id="supervisor管理器">supervisor管理器</h5>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224146486.png"
alt="image-20250104224146486" />
<figcaption aria-hidden="true">image-20250104224146486</figcaption>
</figure>
<h5 id="图像生成">图像生成：</h5>
<p>有了好的布局，就能生成好的具有一致多主体表现的吸引人图像吗？</p>
<p>现在的：模型不微调，图像可控-》depth、open pose-》controlnet</p>
<p>1.主体初始化生成：</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224202197.png"
alt="image-20250104224202197" />
<figcaption aria-hidden="true">image-20250104224202197</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224209991.png"
alt="image-20250104224209991" />
<figcaption aria-hidden="true">image-20250104224209991</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224217340.png"
alt="image-20250104224217340" />
<figcaption aria-hidden="true">image-20250104224217340</figcaption>
</figure>
<p>2.PUnet</p>
<p>将去噪过程中任意UNet层的输入潜在特征表示为Z。我们将UNet层的原始交叉注意力模块拆分为两个并行的文本和图像交叉注意力模块（分别表示为PTCA和PICA）来优化ZZZ。这两个模块具有相同的架构，其关键思想是计算ZZZ与每个主体文本/图像嵌入之间的特征相似性。</p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224244780.png"
alt="image-20250104224244780" />
<figcaption aria-hidden="true">image-20250104224244780</figcaption>
</figure>
<h4 id="总结-3">总结：</h4>
<p>1.工程化应用和控制很好，总-分的思路去控制生成。</p>
<p>2.引入监督器形成反馈链路。</p>
<p>关于generator《-》supervisor，考试-评分的耦合结构关系，</p>
<p>阿里数学竞赛ai最高分：</p>
<p><a
target="_blank" rel="noopener" href="https://blog.richardstu.com/solution-sharing-and-some-thoughts-about-alibaba-mathematical-competition-for-ai">Solution
Sharing and Some Thoughts about Alibaba Global Mathematical Competition
for AI</a></p>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224306153.png"
alt="image-20250104224306153" />
<figcaption aria-hidden="true">image-20250104224306153</figcaption>
</figure>
<figure>
<img src="/2025/01/04/aigc-paper-share/image-20250104224313553.png"
alt="image-20250104224313553" />
<figcaption aria-hidden="true">image-20250104224313553</figcaption>
</figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/01/04/laion-dataset/" rel="prev" title="laion数据集">
      <i class="fa fa-chevron-left"></i> laion数据集
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/01/04/langchain-robot/" rel="next" title="用lanchain做ai robot">
      用lanchain做ai robot <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#stable-diffusion-%E5%8E%9F%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">Stable diffusion 原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7%E4%BF%9D%E6%8C%81%E7%9B%AE%E6%A0%87"><span class="nav-number">2.</span> <span class="nav-text">一致性保持目标：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%A3%E6%B3%95"><span class="nav-number"></span> <span class="nav-text">解法：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#storydiffusion%E5%81%9A%E4%BF%9D%E6%8C%81"><span class="nav-number">1.</span> <span class="nav-text">StoryDiffusion：做保持</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E4%BA%AE%E7%82%B9%E4%B8%80%E8%87%B4%E6%80%A7%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">1.0.1.</span> <span class="nav-text">主要亮点：一致性图像生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.0.2.</span> <span class="nav-text">总结：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#omg%E5%A4%9A%E4%BA%BA%E7%89%A9%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%9A%84%E5%B7%A5%E5%85%B7%E5%9E%AB%E5%9B%BE-%E5%8C%BA%E5%9F%9F%E5%88%86%E7%A6%BB%E9%87%8D%E7%BB%98"><span class="nav-number">2.</span> <span class="nav-text">OMG：多人物图像生成的工具（垫图
+ 区域分离重绘）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">2.0.1.</span> <span class="nav-text">背景：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%88%E7%9C%8B%E6%95%88%E6%9E%9C"><span class="nav-number">2.0.2.</span> <span class="nav-text">先看效果：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B3%E6%8F%92%E5%8D%B3%E7%94%A8"><span class="nav-number">2.0.3.</span> <span class="nav-text">即插即用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E5%81%9A%E5%88%B0"><span class="nav-number">2.0.4.</span> <span class="nav-text">怎么做到：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%80%E9%98%B6%E6%AE%B5%E7%94%9F%E6%88%90%E6%9E%84%E5%9B%BE%E5%B8%83%E5%B1%80%E5%B9%B6%E6%8F%90%E5%8F%96%E8%A7%86%E8%A7%89%E4%BF%A1%E6%81%AF"><span class="nav-number">2.0.4.1.</span> <span class="nav-text">1.一阶段：生成构图布局并提取视觉信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%E5%A4%9A%E6%A6%82%E5%BF%B5%E5%AE%9A%E5%88%B6%E5%8C%96%E5%8E%BB%E5%99%AA"><span class="nav-number">2.0.4.2.</span> <span class="nav-text">第二阶段：多概念定制化去噪</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%97%B6%E9%97%B4%E6%AD%A5%E5%BC%80%E5%90%AF%E6%B7%B7%E5%90%88"><span class="nav-number">2.0.4.3.</span> <span class="nav-text">不同时间步开启混合：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">2.0.5.</span> <span class="nav-text">总结：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#threatergen%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9Dllmimage-generation"><span class="nav-number">3.</span> <span class="nav-text">ThreaterGen：多轮对话（LLM+image
generation）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%8C%E6%99%AF-1"><span class="nav-number">3.0.1.</span> <span class="nav-text">背景：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%88%E7%9C%8B%E6%95%88%E6%9E%9C-1"><span class="nav-number">3.0.2.</span> <span class="nav-text">先看效果：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E5%81%9A%E5%88%B0-1"><span class="nav-number">3.0.3.</span> <span class="nav-text">怎么做到：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5llm-%E8%A7%92%E8%89%B2%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.0.3.1.</span> <span class="nav-text">1.第一阶段，LLM 角色设计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5reference-img-%E7%94%9F%E6%88%90"><span class="nav-number">3.0.3.2.</span> <span class="nav-text">2.第二阶段，reference img 生成</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%E4%BF%A1%E6%81%AF%E6%95%B4%E5%90%88%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90"><span class="nav-number">3.0.3.3.</span> <span class="nav-text">第三阶段，信息整合+图片生成</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number">3.0.4.</span> <span class="nav-text">总结：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#autostudio%E5%9C%A8%E5%A4%9A%E8%BD%AE%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E4%B8%AD%E6%89%93%E9%80%A0%E4%B8%80%E8%87%B4%E7%9A%84%E4%B8%BB%E4%BD%93"><span class="nav-number">4.</span> <span class="nav-text">AutoStudio：在多轮交互式图像生成中打造一致的主体</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%8C%E6%99%AF-2"><span class="nav-number">4.0.1.</span> <span class="nav-text">背景：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%88%E6%9E%9C"><span class="nav-number">4.0.2.</span> <span class="nav-text">效果：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8F%E5%8C%96%E6%8C%87%E6%A0%87autostudio%E5%9C%A8%E6%89%80%E6%9C%89%E6%8C%87%E6%A0%87%E4%B8%8A%E9%83%BD%E6%98%8E%E6%98%BE%E4%BC%98%E4%BA%8E%E4%B9%8B%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.0.2.1.</span> <span class="nav-text">1.量化指标：AutoStudio在所有指标上都明显优于之前的方法。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">4.0.2.2.</span> <span class="nav-text">2.可视化：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E5%81%9A%E5%88%B0-2"><span class="nav-number">4.0.3.</span> <span class="nav-text">怎么做到：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86%E5%99%A8"><span class="nav-number">4.0.3.1.</span> <span class="nav-text">主题管理器：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#layout%E7%AE%A1%E7%90%86%E5%99%A8"><span class="nav-number">4.0.3.2.</span> <span class="nav-text">layout管理器：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#supervisor%E7%AE%A1%E7%90%86%E5%99%A8"><span class="nav-number">4.0.3.3.</span> <span class="nav-text">supervisor管理器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">4.0.3.4.</span> <span class="nav-text">图像生成：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-3"><span class="nav-number">4.0.4.</span> <span class="nav-text">总结：</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
