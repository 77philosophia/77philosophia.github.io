<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"77philosophia.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
<meta property="og:type" content="website">
<meta property="og:title" content="Garfield&#39;s blog">
<meta property="og:url" content="http://77philosophia.github.io/index.html">
<meta property="og:site_name" content="Garfield&#39;s blog">
<meta property="og:description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="philosophia">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://77philosophia.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Garfield's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Garfield's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/22/vla-valuable/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/22/vla-valuable/" class="post-title-link" itemprop="url">vla in robot鲁棒性探索</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-22 13:38:40 / Modified: 14:09:37" itemprop="dateCreated datePublished" datetime="2026-01-22T13:38:40+08:00">2026-01-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>下面是来自medium的一篇文章的几乎翻译</p>
<blockquote>
<p>[!NOTE]</p>
<p>1.目前的vla本质还是数据采样模仿学习，因此环境条件、物体状态等一变准确率就急速下降；与此相对应的腕部相机表现强劲，因为腕部相机更能克服干扰。要提高泛化就得堆数据。</p>
<p>2.lang
token在其中几乎不起到语义理解作用，language变化和指令变化几乎不影响动作行为。vla没有语言和现实机器人操作对应起来的理解能力。</p>
<p>https://www.alphaxiv.org/zh/overview/2510.03827</p>
<p>https://www.alphaxiv.org/zh/overview/2510.13626</p>
<p>3.要提高准确率都得微调。</p>
</blockquote>
<p>vla的几种方案：</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.09246">自回归方法：将机器人动作离散化为标记，并在大规模演示（如Openvla</a>和<a
target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10900471/">Tinyvla）</a>上训练端到端策略；</li>
<li>基于扩散的模型：通过生成式扩散专家（例如 pi
序列）生成连续轨迹（https://www.pi.website/blog）</li>
<li>强化学习方法：超越监督式微调，例如<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.18719">Vla-rl</a>。</li>
</ul>
<p>然而，这些训练好的VLA模型仅在非常熟悉的场景和环境中展现出一定的性能，缺乏真正的泛化能力。VLA模型的迁移性非常低，同时我们也注意到，现有的基准测试缺乏对分布变化下模型性能的全面评估，这凸显了在多样化条件下进行系统、细粒度鲁棒性评估的必要性。</p>
<p>最近，我偶然发现了一篇名为 LIBERO-plus
的论文，它通过系统性的参数变化和各种扰动维度，对当前的 VLA
模型进行了详细的脆弱性分析，深入剖析了模型在不同条件下的性能。如果您感兴趣，请访问<a
target="_blank" rel="noopener" href="https://sylvestf.github.io/LIBERO-plus/">https://sylvestf.github.io/LIBERO-plus/查看更多详情。</a></p>
<p>这项工作并非创建更多任务，而是主要集中于引入扰动因子，以研究不同的扰动因子如何影响
VLA 的性能，并将其应用于测试案例/阶段：</p>
<h2 id="扰动因子">扰动因子：</h2>
<ul>
<li><strong>对象布局</strong>：添加混淆对象（随机向任务场景中添加 n
个额外的未见对象）和/或改变目标对象的位置（x、y、z）和方向（俯仰、偏航、滚动）。</li>
<li><strong>背景纹理</strong>：改变环境的场景纹理（例如，从彩绘墙到砖墙），并随机改变工作表面的纹理（例如，桌面或地板）。</li>
<li><strong>光照条件</strong>：改变光照强度、方向、颜色和阴影图案，以影响场景风格。</li>
<li><strong>相机视角</strong>：改变第三人称相机的视角/姿态和视野——改变相机距离、球形位置和相机方向。</li>
<li><strong>机器人初始状态</strong>：改变机械臂的初始关节位置（qpos）。</li>
<li><strong>语言说明</strong>：重写任务说明，以增加语言的丰富性和复杂性，例如重写为更长、更口语化的形式，其中包含额外的但与任务无关的上下文线索，或涉及推理的复杂性。</li>
<li><strong>传感器噪声：</strong>注入光度失真，以评估在输入质量下降的情况下的鲁棒性，包括运动模糊、高斯模糊、变焦模糊、玻璃模糊以及雾霾。</li>
</ul>
<p>上述扰动是通过修改场景 XML 定义文件或修改任务描述文件 (BDDL)
来实现的。</p>
<h2 id="已测试的vla模型概述">已测试的VLA模型概述</h2>
<ul>
<li><strong>OpenVLA（Kim 等，2024）和 OpenVLA-OFTs（Kim
等，2025）</strong>：该系列 VLA 基于 Prismatic-7B VLM 构建——VLM 是一个由
SigLIP 和 DINOv2 组成的视觉编码器，其输出与 Llama2-7B
语言骨干的输入空间融合，该语言骨干通过交叉注意力机制整合视觉和文本输入。为了使
VLM 骨干适应机器人控制，连续的机器人动作被离散化为每个维度 256
个区间，并以标记的形式表示在 LLM 词汇表中。Llama 分词器中最不常用的 256
个标记被替换为动作标记，训练过程采用应用于动作序列的标准下一个标记预测目标（因此训练损失为交叉熵）。OpenVLA
在 Open X-Embodiment (OpenX) 数据集上进行了预训练。</li>
<li><strong>π0（Black 等人）和 π0-fast（Pertsch
等人，2025）</strong>：核心模型包含一个用于语义理解多模态输入（多张 RGB
图像、语言指令和本体感觉状态）的 VLM
基类（PaliGemma），并将动作标记投影并路由到一个更小的动作专家模型。该框架类似于
Transfusion 框架，后者使用多个目标函数训练单个
Transformer：连续输出标记使用流匹配损失，离散标记使用交叉熵损失。</li>
<li><strong>Nora（Hung et al.,
2025）</strong>：它采用Qwen-2.5-VL-3B多模态模型作为骨干网络，该模型具有强大的视觉语义理解能力。它使用FAST+分词器将连续的动作标记离散化，从而输出离散的动作序列。此外，它还在Open
X-Embodiment数据集上进行了预训练。</li>
<li><strong>WorldVLA（Cen等人，2025）：</strong>一种自回归动作-世界模型，它将视觉-语言-动作建模和世界建模统一到一个集成框架中。其核心思想是联合学习用于动作生成的策略模型和用于未来状态预测的世界模型，从而使这两个组件能够相互增强。该模型基于Chameleon初始化，Chameleon是一个统一的图像理解和生成模型。它采用了三个分词器：一个基于VQ-GAN的图像分词器、一个基于BPE的文本分词器，以及一个动作分词器，该动作分词器将连续机器人动作的每个维度离散化为256个区间。所有模态（文本、图像、动作）都被离散化为词元，并在统一的序列中进行自回归建模。在训练过程中，动作建模数据训练模型，使其能够根据语言指令和图像观测历史生成动作块，损失函数仅基于动作词元计算。世界建模数据用于训练模型，使其能够根据当前图像和动作预测下一帧图像，损失函数仅基于
图像标记计算。这种联合训练策略鼓励学习共享表征：世界模型获取环境物理信息以辅助生成与任务相关的动作，而动作模型则增强视觉理解以支持准确的帧预测。</li>
<li><strong>UniVLA（Li等人，2025）：</strong>该架构基于预训练的Prismatic-7B
VLM模型构建。其关键创新在于扩展了LLM的词汇表，引入了特殊的动作标记来表示量化的潜在动作。该模型以视觉观察和语言指令作为输入，并自回归地预测一系列潜在动作标记。</li>
<li><strong>RIPT-VLA（Brohan 等人，2022）：</strong>其基础模型为
OpenVLA-OFT。这项工作通过添加一个轻量级的辅助头来增强模型，该辅助头用于预测动作分布的尺度参数
σθ，使其与强化学习 (RL) 兼容。在标准的预训练和监督微调 (SFT)
阶段之后，它引入了强化交互式后训练 (RIPT)
的第三阶段。该策略的核心是动态采样留一法后训练优化 (LOOP)
框架——一种动态拒绝机制，它过滤掉所有 K
次展开都获得相同奖励（全部成功或全部失败）的上下文样本，从而确保训练批次包含有意义的学习信号。</li>
</ul>
<h2 id="基准建设">基准建设</h2>
<p><a
target="_blank" rel="noopener" href="https://libero-project.github.io/main.html">LIBERO</a>最初包含 40
个评估任务，分为四个泛化子任务（空间、物体、目标、长距离）。这项工作为每个子任务生成了
500 个实例，涵盖七个扰动维度，因此总共有 500 * 4 * 7 = 14,000
个任务。之后，为了避免天花板效应，我们移除了所有模型或绝大多数模型都能解决的任务。然后，作者进一步平衡了剩余任务在各个增强子维度上的分布，以防止偏差。最终的测试基准数据集包含
10,030 个任务，涵盖所有七个维度。</p>
<p>按回车键或点击查看完整尺寸的图片</p>
<figure>
<img src="/2026/01/22/vla-valuable/1*PBizSwIa2GxGOKh9C38s4A.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="/2026/01/22/vla-valuable/1*eJcsziTFJlMEEWqqbxl3xQ.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="结果">结果</h2>
<p>研究结果共同揭示了当前甚大阵列（VLA）泛化能力的显著缺陷。如图所示，即使是微小的扰动也会导致性能急剧下降。</p>
<p>按回车键或点击查看完整尺寸的图片</p>
<figure>
<img src="/2026/01/22/vla-valuable/1*PsC0_dkqtw_AYNnasuvx3Q.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>发现
1：在各种输入扰动下，性能会显著下降，尤其是在相机视角和机器人初始状态发生变化时。</li>
<li>发现2：模型最容易受到相机视角和机器人初始状态变化的影响，这需要对空间几何和本体感觉有较高的理解水平。相比之下，模型对光照和背景变化表现出相对的适应能力，因为这些变化属于更表面、更低层次的视觉变化。</li>
<li>发现
3：语言扰动导致大多数模型的平均性能下降幅度第二小（-25.3），这表明模型可能比预期的更少依赖语言指令，并可能利用视觉环境中的任务线索。</li>
<li>发现 4：与仅依赖第三人称视角的模型（例如
OpenVLA-OFT_w）相比，采用第一人称腕部摄像头的模型（例如
OpenVLA-OFT）展现出更优异的泛化能力，尤其是在应对摄像头视角变化时。此外，强调多样性和协同训练的训练策略（例如
π0,π0-fast）能够持续产生在多种扰动类型下都更稳健的模型，这凸显了接触不同数据分布的重要性。</li>
<li>发现
5：虽然模型表现出能够忽略干扰物体的能力，但当目标物体发生位移时，它们却无法进行泛化，这表明它们依赖于记忆的位置线索，而不是学习不变的物体语义。</li>
<li>发现
6：在光照扰动下性能的相对稳定性主要归功于腕部摄像头的近距离视角，它提供了与光照无关的几何线索。缺乏腕部摄像头输入的模型对光照变化表现出显著更高的敏感性。</li>
<li>发现7：VLA模型不具备强大的跨对象指令执行泛化能力。在目标被替换的任务中，模型的成功率几乎降至零。例如，原任务指令“<em>拿起字母汤”</em>被替换为<em>“拿起番茄酱”。</em></li>
<li>发现 8：VLA
模型似乎更依赖于固定的视觉-动作映射，而不是充分利用语言信号进行任务决策。</li>
</ul>
<p>另一篇论文也得出了同样的结论，即
libero-pro，https://zxy-mllab.github.io/LIBERO-PRO-Webpage/</p>
<p>以下是其研究结果，与上述内容类似：</p>
<ul>
<li>该模型能否推广到新对象？</li>
</ul>
<figure>
<img src="/2026/01/22/vla-valuable/1*Xv8KhRSINYgNNPuLsCh0hw.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>当用一个不相关的物品（例如，字母
汤，第三行）替换沙拉酱时，模型仍然执行相同的动作轨迹，试图拿起这个与指令不符的新物品。此外，它在语义无关的物品上的失败
也凸显了其无法正确地将语言指称与物品语义相匹配。</p>
<ul>
<li>该模型是否适用于各种指令？</li>
</ul>
<figure>
<img src="/2026/01/22/vla-valuable/1*yzWMERMGBRHWMB66ayyyVg.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>当指令被替换为无意义的输入（例如，“xxx”）时，模型仍然产生相同的动作序列，再次检索并执行将沙拉酱放入篮子的模式。因此，模型在释义指令和无意义指令下表现出的一致性，不应被视为成功泛化到语义扰动的结果，而应被视为其无法理解或解释指令的证据，表明其过度依赖于机械的模式执行。</p>
<ul>
<li>该模型对物体放置位置的敏感度如何？</li>
</ul>
<figure>
<img src="/2026/01/22/vla-valuable/1*kGUB7dCH1RNNANU9CLEn1g.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>该图显示了 VLA
对物体初始位置的高度敏感性——将杯子放在稍微不同的位置可能会导致任务完全失败。</p>
<p>总之，当我们看到这些令人印象深刻的指标时，我们应该停下来问问自己——它们真的那么有效吗？还是只是过度拟合死记硬背的结果？</p>
<p>按回车键或点击查看完整尺寸的图片</p>
<figure>
<img src="/2026/01/22/vla-valuable/1*-baS0o5UJO-zm-Re7P-R_A.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>摘自论文：https://zxy-mllab.github.io/LIBERO-PRO-Webpage/</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/22/no-look-back/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/22/no-look-back/" class="post-title-link" itemprop="url">no-look-back</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-22 11:22:45 / Modified: 13:37:26" itemprop="dateCreated datePublished" datetime="2026-01-22T11:22:45+08:00">2026-01-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>想到姥姥去世的时候，我听到消息，都没有哭。</p>
<p>我更加繁忙的工作和锻炼，不知道说着什么样的话，和什么样的人寒暄。</p>
<p>我容易生出很多感情。</p>
<p>一遇到分离，我就无法淡定和继续生活。</p>
<p>我有了一套处理离别的方法，去和所有重要的分离与死亡做预演。</p>
<p>那就是假装无事发生。</p>
<p>继续起床、锻炼、吃饭、睡觉，假装不曾出现。</p>
<p>我要足够坚强和冷漠。</p>
<p>但是有时候我就忍不住盼望，春天什么时候到来</p>
<p>这个冬天实在太漫长了</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/21/sam-family/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/21/sam-family/" class="post-title-link" itemprop="url">sam-family</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-21 16:52:07 / Modified: 16:53:39" itemprop="dateCreated datePublished" datetime="2026-01-21T16:52:07+08:00">2026-01-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="sam-family">sam-family</h3>
<table>
<colgroup>
<col style="width: 35%" />
<col style="width: 33%" />
<col style="width: 14%" />
<col style="width: 10%" />
<col style="width: 5%" />
</colgroup>
<tbody>
<tr>
<td>sam</td>
<td>针对图像的promptable分割模型</td>
<td>prompt:点，框，MASK,文字</td>
<td>搭配DINO/CLIP使用</td>
<td></td>
</tr>
<tr>
<td>SAM2对象选择和调整： SAM 2 扩展了 SAM
基于提示的对象分割功能，使其也能用于跨视频帧的对象跟踪。对陌生视频的鲁棒分割：该模型具备零样本泛化能力。这意味着它可以分割训练过程中未见过的领域中的对象、图像和视频，从而在实际应用场景中具有很强的适应性。实时交互：SAM
2 采用流式内存架构，一次处理一帧视频，从而实现实时交互式应用。</td>
<td>针对图像和视频的PROMPTABLE分割模型，可用于实时的TRACKING</td>
<td>同上，可以在任意一帧选取</td>
<td></td>
<td>SA-V数据集</td>
</tr>
<tr>
<td>grounded sam(2024.1)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Grounded sam2</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>sam3</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="一.sam-23年4月-meta"><strong>一.sam <Segment anything> (23年4月
meta)</strong></h2>
<p><strong>2023.4 META的AI研究部门</strong></p>
<h3
id="urlhttpswww.alphaxiv.orgzhoverview2304.02643v1"><strong>url:</strong>https://www.alphaxiv.org/zh/overview/2304.02643v1</h3>
<p>官方DEMO：https://sam2.metademolab.com/demo</p>
<h4 id="tldr"><strong>TL;DR</strong></h4>
<p>Segment Anything
项目引入了一个图像分割的基础模型，能够根据各种提示生成图像中任何对象的分割掩码。这项工作还提出了包含超过
10 亿个掩码的 SA-1B 数据集，使 SAM
能够在各种分割任务中进行零样本迁移并泛化到未见数据。</p>
<h3 id="task-method"><strong>task &amp; method</strong></h3>
<ul>
<li><strong>TASK: 可提示分割任务</strong></li>
</ul>
<figure>
<img
src="/2026/01/21/sam-family/5eecdaf48460cde50b01b09d07620a014fd28e0d50a13f5d75b8339e1c4c24831f739168b2e59d878d68742cd653602a15464a86392b1bf523a2ed422fd60fe6ab49563521b589d5a1a59a8a0c147ef57c63bba423c1a2e949354bde2c5591fc.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li><strong>METHOD:</strong></li>
</ul>
<p>图像用预训练的VIT,
提示编码器分为稀疏的和密集的(mask),有不同的处理方法；mask decoder:
一个Transformer模块，结合图像嵌入和提示嵌入来预测分割掩码。</p>
<figure>
<img
src="/2026/01/21/sam-family/5eecdaf48460cde50b01b09d07620a014fd28e0d50a13f5d75b8339e1c4c24831f739168b2e59d878d68742cd653602a473a1bc8310192c0953517fc0aa2bc6752720836a6fef9839629f99cd45d9f5392887e963d4a1eaee421e8207ac6e15e.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>数据飞轮</li>
</ul>
<p>迭代过程，模型不断帮助注释者创建分割掩码，产生的数据用于改进模型。这种方法产生了SA-1b数据集，其中包含来自1100万张图像10亿个掩码。</p>
<figure>
<img
src="/2026/01/21/sam-family/5eecdaf48460cde50b01b09d07620a014fd28e0d50a13f5d75b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832ec99544b97b8571163685e8122a6c3bff7f10f34ca21351103acdf64b8ba5d6f088e560432a2bfe6f.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="results"><strong>RESULTS:</strong></h3>
<figure>
<img
src="/2026/01/21/sam-family/5eecdaf48460cde50b01b09d07620a014fd28e0d50a13f5d75b8339e1c4c24831f739168b2e59d878d68742cd653602a7a5c135778ee098c80c36eb973c40a26a697fba25eac5d95fd8c909f6f565e2cb6a5bf0635fe81ee77013136ec04c284.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="二sam2-24年8月meta"><strong>二、sam2
(24年8月，meta)</strong></h1>
<p>Demo: <a
target="_blank" rel="noopener" href="https://sam2.metademolab.com/">https://sam2.metademolab.com</a></p>
<p>Code: https://github.com/facebookresearch/sam2</p>
<p>Website: https://ai.meta.com/sam2</p>
<h3 id="tldr-1"><strong>TL;DR</strong></h3>
<p>SAM 2 通过将原始的 Segment Anything Model (SAM)
的功能从静态图像扩展到动态视频内容，代表了视觉分割领域的一项重大进步。这项来自
Meta FAIR
的工作引入了一个统一的基础模型，该模型能够通过单一架构在图像和视频上执行可提示的视觉分割(图像看成MEMORY是空的视频分割)。</p>
<p>这项研究解决了计算机视觉中的一个基本挑战：尽管原始 SAM
在可提示图像分割方面表现出色，但视频的动态特性带来了额外的复杂性，包括物体运动、变形、遮挡和质量变化。SAM
2 通过三项核心贡献来应对这些挑战：定义了一种新的可提示视觉分割 (PVS)
任务，开发了一个同时处理图像和视频的统一模型架构，并通过创新的数据收集方法创建了迄今为止最大的视频分割数据集。</p>
<h3 id="methods"><strong>METHODS:</strong></h3>
<figure>
<img
src="/2026/01/21/sam-family/5eecdaf48460cde50b01b09d07620a014fd28e0d50a13f5d75b8339e1c4c24831f739168b2e59d878d68742cd653602a27905087190dd0b9c2b05c89e89719b6e08444384d16f27a23030e591b96b25304520958eda7793ff90a9d6a51a24108.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img
src="/2026/01/21/sam-family/5eecdaf48460cde50b01b09d07620a014fd28e0d50a13f5d75b8339e1c4c24831f739168b2e59d878d68742cd653602a0d50ecb3f30f832e498ecb22001e57a1af349b5f5d9d1c629c5d3eebc18f2af9aa50c099d068e146dcaad8c64c9192c9.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>图像编码器使用经过 MAE 预训练的
Hiera（分层视觉变换器），处理每个视频帧以生成无条件特征嵌入。这种分层设计提供了多尺度特征，这对于详细的掩码解码至关重要，同时比原始
SAM 编码器实现了 6 倍的速度提升。</li>
<li>一种新颖的<strong>记忆注意力</strong>机制构成了 SAM 2
视频处理能力的核心。该组件由堆叠的变换器块组成，通过对记忆库进行交叉注意力，将当前帧特征与来自过去帧和先前交互的信息进行条件化。记忆库存储了先前观察到的帧的表示，并维护着捕获被跟踪物体高级语义信息的“物体指针”。</li>
<li>处理物体消失或者遮挡：掩码解码器在 SAM
的设计基础上进行了关键增强以适应视频：来自分层图像编码器的高分辨率细节整合的跳跃连接，以及一个新的<strong>遮挡预测头</strong>，用于确定每个帧中物体的可见性</li>
</ul>
<h5 id="内存管理系统"><strong>内存管理系统：</strong></h5>
<p>记忆编码器通过将预测的掩码与无条件的图像特征融合来生成记忆，并将它们存储在一个
FIFO
队列中，该队列维护固定容量的近期帧信息以及被提示的帧。时间位置编码嵌入到近期帧记忆中以建模短期运动模式，而物体指针则在更长的时间跨度上提供语义连续性。</p>
<h5 id="数据集"><strong>数据集</strong></h5>
<p>SAM 2
的一项关键创新是开发了一个迭代数据引擎，解决了大规模、多样化视频分割数据集稀缺的问题。Segment
Anything Video (SA-V) 数据集中的掩码数量比现有视频分割数据集增加了 53
倍。</p>
<h3 id="results-1"><strong>RESULTS:</strong></h3>
<h3 id="交互式视频分割性能"><strong>交互式视频分割性能</strong></h3>
<p>在 9 个密集标注的零样本视频数据集上进行的模拟交互设置中，SAM 2
显著优于强基线（SAM+XMem++ 和
SAM+Cutie），在实现更高分割精度的同时，用户交互次数减少了 3
倍以上。所需点击次数的显著减少代表了交互式视频标注工作流程中用户体验的巨大提升。</p>
<h3 id="半监督视频对象分割"><strong>半监督视频对象分割</strong></h3>
<p>当仅使用首帧提示在常规半监督 VOS 基准上进行评估时，SAM 2 在 17
个不同的视频数据集上实现了最先进的准确性。即使使用点击或框输入而不是真实掩码，该模型也始终优于
XMem++ 和 Cutie 等专门的 VOS 方法。在最新引入的 SA-V
验证和测试集上，性能提升尤为显著，展示了该模型在具有挑战性的开放世界场景中“分割任何事物”的能力。</p>
<h3 id="增强的图像分割"><strong>增强的图像分割</strong></h3>
<p>SAM 2 改进了原始 SAM 的图像分割性能，在 23 个 SA 评估数据集上，其 1
次点击 mIoU 达到 58.9%，而 SAM 为 58.1%，并且运行速度快 6
倍。在混合图像和视频数据上进行训练进一步将性能提升至 61.9%
mIoU，尤其在被评估为视频的数据集上取得了显著的增益。</p>
<h3 id="实时处理能力"><strong>实时处理能力</strong></h3>
<p>该模型以实时速度运行，在单个 A100 GPU 上达到 43.8 FPS (Hiera-B+) 和
30.2 FPS (Hiera-L)，使其适用于需要交互式视频处理的实际应用。</p>
<h1 id="sam2.1"><strong>SAM2.1</strong></h1>
<p>https://encord.com/blog/sam-2.1-explained/</p>
<h3 id="tldr-2"><strong>TL;DR</strong></h3>
<p>SAM
2在图像分割模型领域已是强有力的竞争者，它能够“分割任何物体”，适用于各种图像类型和领域。然而，任何尖端技术都有其不足之处，SAM
2
也不例外。在处理视觉上相似的物体、小型物体以及遮挡（即物体部分被遮挡）的情况时，SAM
2 会遇到一些挑战。</p>
<p>SAM 2.1 最重要的更新之一是提高了分割 SAM 2 难以分割的对象的能力。</p>
<ul>
<li>处理视觉相似和小尺寸物体：SAM 2.1
集成了额外的数据增强技术来模拟复杂环境。这些技术训练模型识别和区分外观相似或尺寸非常小的物体。实际上，这使得
SAM 2.1
在医学成像或自动驾驶车辆导航等对分割精度要求极高的实际任务中表现更加可靠。</li>
<li>遮挡处理：遮挡（即物体部分被遮挡）一直是图像分割的一大挑战。SAM 2.1
通过使用更长的帧序列进行训练来解决这个问题，从而为模型提供更多上下文信息，使其能够理解部分可见的物体。这项更新使得
SAM 2.1 能够更好地重建和预测物体边界，即使物体的部分区域被遮挡。</li>
<li>位置编码调整：为了提高对空间关系和物体指针的记忆能力，SAM 2.1 对其<a
target="_blank" rel="noopener" href="https://www.sciencedirect.com/topics/computer-science/positional-encoding"><strong>位置编码系统</strong></a>进行了调整。这项改进有助于模型更有效地跨帧跟踪物体，尤其是在动态或杂乱的场景中。</li>
</ul>
<h1 id="grounded-sam2024.1"><strong>Grounded sam(2024.1)</strong></h1>
<p>IDEA研究院Code &amp; Demo:
https://github.com/IDEA-Research/Grounded-Segment-Anything</p>
<h3 id="tldr-3"><strong>TL；DR</strong></h3>
<p>由国际数字经济研究院（IDEA）研究人员开发的Grounded
SAM，将用于开放集对象检测的Grounding
DINO与用于可提示分割的SAM集成，从而能够根据自然语言输入检测和分割任意对象。这个组合系统在SGinW零样本基准测试中达到了48.7
mAP，超越了之前的统一模型。</p>
<h3 id="methods-1"><strong>METHODS:</strong></h3>
<p>Grounded SAM 基于两个最近开发的基础模型：</p>
<ol type="1">
<li><strong>Grounding
DINO</strong>：一种开放集对象检测器，可以根据自然语言描述在图像中定位对象。它将文本描述转换为视觉特征，以帮助识别图像中相应的区域。</li>
<li><strong>Segment Anything Model
(SAM)</strong>：一种可提示分割模型，在大型 SAM-1B
数据集上训练，能够在提供点或边界框等提示时，为任何对象生成精确的掩码注释。</li>
</ol>
<p>Grounded SAM
的关键创新在于有效地结合这些模型，以利用它们的互补优势。Grounding DINO
擅长查找文本描述的对象，而 SAM 擅长为这些对象生成精确的分割掩码。</p>
<h3 id="results-2"><strong>RESULTS:</strong></h3>
<h2 id="开放词汇分割的性能"><strong>开放词汇分割的性能</strong></h2>
<p>Grounded SAM 在先前的统一开放集分割模型上表现出显着的性能提升。在
Segmentation in the Wild (SGinW) 零样本基准上评估时，Grounded SAM
实现了最先进的结果。</p>
<p>研究人员测试了 Grounded SAM 的多种配置，结合了不同版本的 Grounding
DINO（B 代表 Base，L 代表 Large）和 SAM（B 代表 Base，H 代表
Huge）。他们表现最佳的模型 Grounded SAM (B+H) 达到了 48.7 的平均精度均值
(mAP)，优于之前的模型，如 UNINEXT 和 OpenSeeD。</p>
<p>这一性能表明，专门模型的组合可以有效地解决开放集分割的挑战，超越了尝试用单个统一模型解决问题的方法。</p>
<h2 id="与其他模型的集成"><strong>与其他模型的集成</strong></h2>
<p>Grounded SAM
的主要优势之一是它能够与其他开放世界模型集成，以完成更复杂的视觉任务。该论文展示了几个成功的集成案例：</p>
<ol type="1">
<li><strong>RAM-Grounded-SAM</strong>：将 Recognize Anything Model (RAM)
与 Grounded SAM 集成，以实现自动图像标注。RAM 生成图像标签，然后将其用作
Grounded SAM 的文本提示，以分割相应的对象。</li>
<li><strong>Grounded-SAM-SD</strong>：将 Grounded SAM 与 Stable
Diffusion 结合，以实现可控的图像编辑。Grounded SAM
根据文本提示识别和分割区域，而 Stable Diffusion
根据其他指令修改这些区域。</li>
<li><strong>Grounded-SAM-OSX</strong>：将 Grounded SAM 与 OSX（一种 3D
人体重建模型）集成，以进行可提示的人体运动分析。Grounded SAM
根据文本描述检测特定的人，而 OSX 估计这些人的 3D 身体姿势和形状。</li>
</ol>
<p>这些集成证明了 Grounded SAM
作为更复杂的视觉理解系统基础的多功能性。通过组合专门的模型，生成的管道继承并扩展了各个组件的功能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/21/pi05/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/21/pi05/" class="post-title-link" itemprop="url">pi05</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-21 11:45:23 / Modified: 14:16:33" itemprop="dateCreated datePublished" datetime="2026-01-21T11:45:23+08:00">2026-01-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Paper:https://www.alphaxiv.org/zh/overview/2504.16054</p>
<p>类似于smolVLA,对输入pi05也分为两部分，第一部分对图像和语言进行编码，第二部分对动作输入进行加噪。</p>
<h5 id="对输入图像和lang处理">对输入图像和lang处理</h5>
<ul>
<li><p>对图像处理：</p>
<ul>
<li>(Batch,3,224,224)-&gt;image backbone-&gt;(Batch, num_img_embeds=256,
2048)</li>
<li>对应的pad masks:(batch=4, 256)</li>
<li>Att_masks: [0,0,0...(256个)， 0，0，0，...(256个)]</li>
</ul></li>
<li><p>对language处理</p>
<ul>
<li>(Batch, 200) -&gt; (Batch, 200, 2048)</li>
<li>Pad_masks:(batch=4,200)</li>
<li>Att_masks: [0,0,...,0(200个)]</li>
</ul>
<hr />
<p>最终输入：</p>
<p>Ebbs:[img(4,256,2048), img(4,256,2048), lang(4,200,2048)]</p>
<p>Pad_masks:[(4,256), (4,256), (4,200)]</p>
<p>Att_masks:[0,0,...(256个)， 0,0,...(256个)，
0，0，...(200个)]</p></li>
</ul>
<h4 id="对action处理">对action处理</h4>
<ul>
<li>Action：(batch比如=4, chunk_size=50,
action_dim=32)-&gt;nn.Linear()-&gt;(batch=4, 50, 1024)</li>
<li>对time进行采样：(batch=4)-&gt;embedding-&gt; time_embed:(batch=4,
1024)</li>
<li>类似于smolvla的操作对action进行加噪，得到noised_actions:(batch=4,50,1024)</li>
<li>最后返回：
<ul>
<li>Embs:[action_emb(batch=4,50,1024)]</li>
<li>Pad_mask:(batch=4,50). 全1</li>
<li>Att_mask: [1,0,0,...0] 后面49个0</li>
<li>Adam_cond: (batch=4,1024). 就是time_emb</li>
</ul></li>
<li>接下来的网络主要对两部分的信息进行融合</li>
</ul>
<h3 id="网络结构">网络结构</h3>
<figure>
<img src="/2026/01/21/pi05/image-20260121141619827.png"
alt="image-20260121141619827" />
<figcaption aria-hidden="true">image-20260121141619827</figcaption>
</figure>
<figure>
<img src="/2026/01/21/pi05/image-20260121141633087.png"
alt="image-20260121141633087" />
<figcaption aria-hidden="true">image-20260121141633087</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/20/smolvla/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/20/smolvla/" class="post-title-link" itemprop="url">smolvla</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2026-01-20 16:34:13" itemprop="dateCreated datePublished" datetime="2026-01-20T16:34:13+08:00">2026-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2026-01-21 11:36:34" itemprop="dateModified" datetime="2026-01-21T11:36:34+08:00">2026-01-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>论文：https://www.alphaxiv.org/zh/overview/2506.01844</p>
<h3 id="smolvla主要架构">smolVLA主要架构</h3>
<pre><code>[Paper]()

Designed by Hugging Face.
┌──────────────────────────────┐
│                 actions      │
│                    ▲         │
│ ┌─────────┐      ┌─|────┐    │
│ |         │────► │      │    │
│ |         │ kv   │      │    │
│ |         │────► │Action│    │
│ |   VLM   │cache │Expert│    |
│ │         │────► |      │    │
│ │         │      │      │    │
│ └▲──▲───▲─┘      └───▲──┘    |
│  │  |   |            │       |
│  |  |   |          noise     │
│  │  │ state                  │
│  │ language tokens           │
│  image(s)                    │
└──────────────────────────────┘</code></pre>
<ul>
<li><p>对输入进行的处理：</p>
<ul>
<li>Images:[(Batch, C,512,512), (Batch, C,512,512)] (两路相机)</li>
<li>Img_masks:([B],[B])
(image级别的mask，不同机器人配置的相机个数不一样，为了统一对齐格式)</li>
<li>Lang_tokens:(Batch, lang_len=48)</li>
<li>Lang_masks:(Batch, 48)</li>
<li>State:(Batch, state_dim=32)
实验中机器人关节dim=6，其余的是padding</li>
<li>Actions:(Batch,chunk_size=50,action_dim=32)</li>
<li>Noise:(Batch,chunk_size=50,noise_dim=action_dim=32)</li>
</ul></li>
<li><p>对输入进行处理</p>
<ul>
<li><p>第一步：多模态融合(embed_prefix)</p>
<ul>
<li>对每张images(Batch,C=3,H=512,W=512),vit的结构中14x14是一个patch,则图像的sequence长度是L=(512/14)x(512/14)=32x32=1024,提取经过vlm模型的vision_model的last_hidden_state得到(Batch,
Len=1024,D=hidden_dim=768)</li>
<li>经过vlm结构的connector(把图像特征对齐到跟语言空间一致)最终是(Batch,64,960)</li>
<li>对文本进行处理：
<ul>
<li>Lang_tokens:(Batch,48) -&gt; Embedding -&gt;(Batch,48,960)</li>
<li>State;(Batch,32)-&gt;nn.Linear()-&gt;(Batch,960)-&gt;unsqueezed:(Batch,1,960)</li>
</ul></li>
<li>输出拼接：
<ul>
<li>Embs:[image_embed, image_embed, lang_embed, state_embed]</li>
<li>Pad_masks:[1,1,...(B,64), 1,1,1...(B,64), 1/0...(B,48), 1(B,1)
]</li>
<li>Attn_masks:[0,0,...(64), 0,0,...(64), 0,0,0,...(48), 1]</li>
</ul></li>
</ul>
<hr /></li>
<li><p>第二步：融合action和time(embed_suffix)</p>
<ul>
<li><p>Flow Matching原理</p>
<figure>
<img src="/2026/01/20/smolvla/image-20260120165038918.png"
alt="image-20260120165038918" />
<figcaption aria-hidden="true">image-20260120165038918</figcaption>
</figure></li>
<li><p>根据flow matching的原理：</p>
<ul>
<li><p>输入actions:(batch, chunk_size=50, action_dim=32),</p>
<p>输入noise:(batch, 50,32)</p>
<p>Time:(B) -&gt; (B,1,1)</p></li>
<li><p>则x_t = noise + (1-time)*actions, 速度u_t =
noise-actions</p></li>
<li><p>这里注意对noise采样是正态分布随机采样，但是对时间t采取的是beta分布：</p></li>
</ul>
<figure>
<img src="/2026/01/20/smolvla/image-20260120165422935.png"
alt="image-20260120165422935" />
<figcaption aria-hidden="true">image-20260120165422935</figcaption>
</figure>
<blockquote>
<p>[!CAUTION]</p>
<p>那我们如何将time信息和noise_action很好的fuse在一起，这是个problem,
我们来看embed_suffix是如何做的。</p>
</blockquote></li>
<li><p>对noised_action:(Batch,chunk_size=50,action_dim=32)-&gt;nn.Linear()-&gt;(batch,
chunk_size=50,720)</p></li>
<li><p>对time:(B)-&gt;sin_cos的方法对(0,1)区间的时间数值进行编码-&gt;(Batch,720)-&gt;repeat-&gt;(Batch,50,720)</p></li>
<li><p>用mlp(Linear-&gt;Silu-&gt;Linear-&gt;Silu)
将action和time结合起来, 最后得到noised_actions_with_time:(batch,
50,720), pad_masks:[1,1,1...] (B,50), attn_masks:[1,1,1,...] (B,50)</p>
<hr /></li>
</ul></li>
<li><p>最后把所有的输入信息concate在一起</p>
<ul>
<li><p>[img, img, lang, state, noised_action_with_time]</p></li>
<li><p>Pad_masks: (B, 64x2+48+1+50). -&gt; (B,227)</p></li>
<li><p>Attn_masks: (B, 64x2+48+1+50). -&gt; (B,227)</p>
<figure>
<img src="/2026/01/20/smolvla/image-20260120170605891.png"
alt="image-20260120170605891" />
<figcaption aria-hidden="true">image-20260120170605891</figcaption>
</figure>
<hr /></li>
</ul></li>
<li><p>第三步： VLM_with_expert的推理</p>
<ul>
<li>首先，总体来看通过vlm_with_expert的推理会得到一个(batch,50,720)的tensor-&gt;nn.Linear(720,32=padded
action dim)-&gt;得到估计的v_t</li>
<li>和上面的u_t真值做mse_loss</li>
<li>所以这一步的关键其实是看VLM_with_expert是怎么把这些信息做整合的</li>
</ul>
<figure>
<img src="/2026/01/20/smolvla/image-20260120212329700.png"
alt="image-20260120212329700" />
<figcaption aria-hidden="true">image-20260120212329700</figcaption>
</figure></li>
</ul></li>
</ul>
<hr />
<ul>
<li><p>输入:</p>
<ul>
<li>Inputs_embeds:[prefix_embes, suffix_embs]</li>
<li>Prefix_embs:[img, img, lang, state] (Batch,64+64+48+1,960)</li>
<li>Suffix_embeds: [noised_actions]. (B,50,720)</li>
</ul></li>
<li><p>VLM_with_Expert的网络结构</p>
<ul>
<li><p>[vlm.text_model, lm_expert(transformer
decoder结构)]。分别有16层；是一个典型的“大模型带小模型”的结构。并且这两个网络结构的层数是一样的，在每一层都进行特征融合。</p></li>
<li><p>16层计算</p>
<ul>
<li>对于奇数层，采取的是joint
attention,代码里的方法是forward_attn_layer(把vlm每次的hidden
state和expert每次的hidden
state拼接在一起，作为一个统一的长序列，在同一个注意力机制下相互观察)</li>
<li>对于偶数层采取的是cross_attention,代码里的方法是forward_cross_attn_layer(互相查询方式的观察)</li>
<li>每一层的观察得到attn_output:(batch, 177+50, 960)</li>
<li>然后分开使用各自网络这一层的linear, norm, mlp层回到各自hidden
state的维度，vlm是（Batch,177,960), expert是（Batch,50,720)
(虽然attn_out_dim=960,但是expert可以通过自己层的proj回到720的维度)</li>
</ul></li>
<li><p>接下来可以关注一下奇数层的joint_attention(self_attention)和偶数层的cross_attention具体是如何实现的。</p>
<ul>
<li><p>Self_attention: 即将两个hidden state
tensor堆叠起来，然后运行multihead attntion计算，互相观察。</p>
<figure>
<img src="/2026/01/20/smolvla/image-20260121113442948.png"
alt="image-20260121113442948" />
<figcaption aria-hidden="true">image-20260121113442948</figcaption>
</figure></li>
<li><p>Cross_attention</p>
<figure>
<img src="/2026/01/20/smolvla/image-20260121113625317.png"
alt="image-20260121113625317" />
<figcaption aria-hidden="true">image-20260121113625317</figcaption>
</figure></li>
</ul></li>
</ul></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/20/vae/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/20/vae/" class="post-title-link" itemprop="url">vae</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-20 16:24:26 / Modified: 16:32:07" itemprop="dateCreated datePublished" datetime="2026-01-20T16:24:26+08:00">2026-01-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="vae是什么">vae是什么</h3>
<p>VAE由三部分组成，VAE encoder, latent space, vae decoder vae
encoder将原始的数据信息X进行编码映射到latent space空间中，latent
space是代表训练数据的一个隐式空间，从中进行采样得到采样数据Z'，最终通过VAE
DECODER的解码过程可以把数据恢复X'</p>
<h3 id="贝叶斯框架下的因果推断">贝叶斯框架下的因果推断</h3>
<figure>
<img src="/2026/01/20/vae/image-20260120163052283.png"
alt="image-20260120163052283" />
<figcaption aria-hidden="true">image-20260120163052283</figcaption>
</figure>
<h3 id="重参数化技巧">重参数化技巧</h3>
<figure>
<img src="/2026/01/20/vae/image-20260120163204904.png"
alt="image-20260120163204904" />
<figcaption aria-hidden="true">image-20260120163204904</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/19/camera-read/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/19/camera-read/" class="post-title-link" itemprop="url">camera的同步和异步读取（opencv和realsense）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-19 14:38:36 / Modified: 14:53:43" itemprop="dateCreated datePublished" datetime="2026-01-19T14:38:36+08:00">2026-01-19</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://huggingface.co/docs/lerobot/main/en/cameras?shell_restart=Intel+Realsense+Camera</p>
<h3 id="代码"><strong>代码：</strong></h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig</span><br><span class="line">from lerobot.cameras.opencv.camera_opencv import OpenCVCamera</span><br><span class="line">from lerobot.cameras.configs import ColorMode, Cv2Rotation</span><br><span class="line"></span><br><span class="line"># Construct an `OpenCVCameraConfig` with your desired FPS, resolution, color mode, and rotation.</span><br><span class="line">config = OpenCVCameraConfig(</span><br><span class="line">    index_or_path=2,</span><br><span class="line">    fps=30,</span><br><span class="line">    width=640,</span><br><span class="line">    height=480,</span><br><span class="line">    color_mode=ColorMode.RGB,</span><br><span class="line">    rotation=Cv2Rotation.NO_ROTATION</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Instantiate and connect an `OpenCVCamera`, performing a warm-up read (default).</span><br><span class="line">camera = OpenCVCamera(config)</span><br><span class="line">camera.connect()</span><br><span class="line"></span><br><span class="line"># Read frames asynchronously in a loop via `async_read(timeout_ms)`</span><br><span class="line">try:</span><br><span class="line">    for i in range(10):</span><br><span class="line">        # frame = camera.async_read(timeout_ms=200)</span><br><span class="line">        frame = camera.read()</span><br><span class="line">        print(f&quot;Sync frame &#123;i&#125; shape:&quot;, frame.shape)</span><br><span class="line">finally:</span><br><span class="line">    camera.disconnect()</span><br></pre></td></tr></table></figure>
<h3 id="两者区别"><strong>两者区别：</strong></h3>
<p>同步读取会阻塞，每次取出一帧；</p>
<p>异步每次直接取最新的帧。</p>
<ul>
<li><p><strong>同步模式 (Sync)：</strong> <code>读取帧1</code> -&gt;
<code>处理(200ms)</code> -&gt; <code>读取帧2</code>。 在你处理帧 1
的期间，硬件产生的帧 2, 3, 4, 5 都在系统缓冲区排队。当你下次调用
<code>read()</code> 时，系统往往会给你排在最前面的旧帧（帧
2），而不是最新的帧 6。</p></li>
<li><p><strong>异步模式 (Async)：</strong>
后台有一个独立的线程在疯狂抓图，永远只保留<strong>最新</strong>的一帧。当你处理完（200ms）去拿图时，它直接把帧
6 给你，把过时的帧 2-5 全部丢弃。</p></li>
</ul>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 26%" />
<col style="width: 57%" />
</colgroup>
<thead>
<tr>
<th><strong>模式</strong></th>
<th><strong>核心逻辑</strong></th>
<th><strong>表现结果</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>同步 (Sync)</strong></td>
<td><code>t1 = read() + sleep(200ms)</code></td>
<td>每秒只能处理约 5 帧，且画面滞后感随时间累积。</td>
</tr>
<tr>
<td><strong>异步 (Async)</strong></td>
<td><code>t1 = async_read()</code></td>
<td>即使你有 <code>sleep(200ms)</code>，<code>async_read</code>
拿到的那一瞬间依然是硬件当前最新的画面，没有滞后感（只有跳帧感）。</td>
</tr>
</tbody>
</table>
<p>在机器人控制（如 LeRobot
训练）中，<strong>同步读取是致命的</strong>。如果你用同步模式训练，机器人看到的永远是
200ms 之前的“幻影”，这会导致机械臂动作永远慢半拍，无法精准抓取物体。</p>
<h3
id="realsense为什么不用考虑同步异步"><strong>realsense为什么不用考虑同步异步</strong></h3>
<p><strong>RealSense
相机依然需要考虑同步和异步，但它在“底层架构”上已经帮你解决掉了一大部分
OpenCV 存在的痛点。</strong></p>
<p>RealSense
不仅仅是一个摄像头，它是一个<strong>小型计算单元</strong>。它通过
<code>Intel RealSense SDK (librealsense)</code> 提供了比原生 OpenCV
更高级的数据流管理机制。</p>
<p>以下是 RealSense 在处理读取模式时的三个核心差异：</p>
<hr />
<h3 id="硬件级的时间戳同步-hardware-sync">1. 硬件级的时间戳同步
(Hardware Sync)</h3>
<p>普通的 USB 摄像头（OpenCV 调用）只是简单地把帧扔给系统。而 RealSense
的红外、深度、彩色模组在硬件层面是<strong>共用时钟</strong>的。</p>
<ul>
<li><strong>OpenCV
的尴尬</strong>：如果你想同时读彩色图和深度图，同步读取时可能彩色图是
10ms 前的，深度图是 20ms 前的，两者在物理空间上对不齐。</li>
<li><strong>RealSense 的优势</strong>：它提供了一个
<code>align</code>（对齐）对象和 <code>wait_for_frames()</code>
机制。当你调用它时，SDK
会确保你拿到的那一个“组合包”里，彩色和深度信息在时间轴上是完全重合的。</li>
</ul>
<hr />
<h3 id="wait_for_frames-vs-poll_for_frames">2.
<code>wait_for_frames</code> vs <code>poll_for_frames</code></h3>
<p>RealSense SDK
原生就设计了两种读取模式，完美对应了你关心的“同步”与“异步”：</p>
<ul>
<li><p>wait_for_frames() (同步阻塞)：</p>
<p>它会死等下一组对齐的帧。虽然它是阻塞的，但它有一个关键特性：它会自动清空陈旧的缓冲区。它保证一旦解除阻塞，你拿到的就是传感器上最新的那一帧，而不是堆积的旧帧。</p></li>
<li><p>poll_for_frames() (非阻塞/异步)：</p></li>
</ul>
<p>它类似于异步读取。它会立刻返回，如果有新帧就给你，没有就返回空。这允许你的
AI 模型在等待图像的同时去执行其他逻辑。</p>
<hr />
<h3 id="后台回调机制-callback-api-真正的异步">3. 后台回调机制 (Callback
API) —— 真正的异步</h3>
<p>RealSense 最强大的地方在于它支持
<strong>Callback（回调）模式</strong>，这是比 LeRobot
异步读取更彻底的方案：</p>
<p>Python</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># RealSense 伪代码：回调模式</span><br><span class="line">def my_callback(frame):</span><br><span class="line">    # 每当硬件产生一个新帧，这个函数就会被自动触发</span><br><span class="line">    process_frame(frame)</span><br><span class="line"></span><br><span class="line">pipeline.start(config, my_callback) </span><br></pre></td></tr></table></figure>
<p>在这种模式下，不是你“去取”数据，而是数据“主动推”给你。这种方式绕过了操作系统复杂的缓冲区排队，延迟是最低的。</p>
<hr />
<h3 id="为什么大家觉得-realsense-不用考虑延迟">4. 为什么大家觉得
RealSense “不用考虑”延迟？</h3>
<p>主要是因为 RealSense
的驱动（SDK）做得太好了，它在后台帮你干了以下重活：</p>
<ol type="1">
<li><strong>多传感器对齐</strong>：自动处理深度、彩色、惯导（IMU）的数据同步。</li>
<li><strong>自动丢弃旧帧</strong>：它的缓冲区管理非常智能，默认配置下极少出现像
OpenCV 那种“画面堆积”的情况。</li>
<li><strong>专用协议</strong>：RealSense 使用自定义的 UVC
扩展协议，数据传输比标准 USB 摄像头更高效。</li>
</ol>
<hr />
<h3 id="总结">总结</h3>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 41%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>普通相机 + OpenCV</strong></th>
<th><strong>RealSense + SDK</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>缓冲区管理</strong></td>
<td>简单 FIFO（导致旧帧堆积延迟）</td>
<td>智能管理（自动对齐与舍弃）</td>
</tr>
<tr>
<td><strong>多模态同步</strong></td>
<td>几乎没有（彩色/深度往往错位）</td>
<td>硬件级纳秒同步</td>
</tr>
<tr>
<td><strong>异步实现</strong></td>
<td>需要像 LeRobot 这样额外写线程包装</td>
<td>SDK 原生支持异步回调和非阻塞轮询</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：用 RealSense
时，你不是“不用考虑”同步异步，而是 <strong>SDK 默认的</strong>
<code>wait_for_frames()</code>
<strong>已经通过内部的异步机制帮你把最难受的“旧帧堆积”问题解决了。</strong></p>
<p>如果你是从普通 USB 摄像头转到
RealSense，你会发现机器人动作变得精准了，这正是因为传感器端到端的
Latency 被 SDK 压到了最低。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/13/linux-clash/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/13/linux-clash/" class="post-title-link" itemprop="url">linux-clash科学上网</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-13 15:57:15 / Modified: 16:00:04" itemprop="dateCreated datePublished" datetime="2026-01-13T15:57:15+08:00">2026-01-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>参考教程：</p>
<p>https://docs.lefly.co/contents/linux/cfw.html#%E5%BC%80%E5%90%AFcfw%E4%BB%A3%E7%90%86</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2026/01/04/act-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/04/act-network/" class="post-title-link" itemprop="url">act-network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2026-01-04 09:37:27" itemprop="dateCreated datePublished" datetime="2026-01-04T09:37:27+08:00">2026-01-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2026-01-20 15:23:34" itemprop="dateModified" datetime="2026-01-20T15:23:34+08:00">2026-01-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="act-network的整体结构">act network的整体结构</h3>
<blockquote>
<p>[!NOTE]</p>
<p>lerobot/src/lerobot/policies/act/modeling_act.py</p>
</blockquote>
<h3 id="act的整体结构">act的整体结构</h3>
<figure>
<img src="/2026/01/04/act-network/Pasted%20image%2020260104104655.png"
alt="s" />
<figcaption aria-hidden="true">s</figcaption>
</figure>
<ol type="1">
<li>vae encoder inputs:
<ul>
<li>action: (Batch, chunk_size=100, action_dim)</li>
<li>Robot_state(Batch, state_dim)</li>
</ul></li>
<li>Transformer encoder:
<ul>
<li>接收来自vae encoder的输入</li>
<li>Robot state:</li>
<li>Env_state: (Batch, env_dim)</li>
<li>image:(Batch, n_cameras, C, H, W)</li>
</ul></li>
<li>Vae encoder只在train阶段参与。</li>
</ol>
<h3 id="vae-encoder">vae encoder</h3>
<ul>
<li><p>Dim_model = 512: 编码空间的维度大小</p></li>
<li><p>对输入进行处理</p>
<ul>
<li>动作：(Batch, chunk_size=100, action_dim=6)
-&gt;nn.Linear()-&gt;(B,100, dim_model=512)</li>
<li>Robot_state:（Batch,
robot_state=6）-&gt;nn.Linear()-&gt;(Batch,512)-&gt;unsqueezed-&gt;(B,1,512)</li>
<li>Cls_embed: (Batch,1,512)</li>
</ul></li>
<li><p>Inputs_concate: [cls_embed, robot_state, action_embed]
-&gt;(B,1+1+100,512)</p></li>
<li><p>准备Inputs_concate_feature对应的位置编码pos_embed=(102,512)
采用固定的sin编码</p>
<hr /></li>
<li><p>最终输入表示为X=layer(x, pos_embed, key_padding_mask)</p>
<ul>
<li>注意这里为什么有key_padding_mask, 因为chunk_size=100,
100个动作为一组，但是不一定都是全的，比如末尾动作不足100个有0
padding.</li>
</ul></li>
<li><p>看一下网络结构：</p>
<ul>
<li>Vae
encoder由多个ACTEncoderLayer组成，每个ACTEncoderLayer是由nn.MultiHeadAttention+dropout+Linear层组成</li>
<li><figure>
<img src="/2026/01/04/act-network/image-20260120152238278.png"
alt="image-20260120152238278" />
<figcaption aria-hidden="true">image-20260120152238278</figcaption>
</figure></li>
</ul></li>
</ul>
<hr />
<ul>
<li>最终输出：是经过注意力加权之后的tensor。（由对应的clas_token的位置取出）
<ul>
<li>Att_output_tensor:(Batch, dim_model=512)</li>
</ul></li>
</ul>
<h3 id="latent-apace处理from-vae-encoder">latent apace处理（from vae
encoder）</h3>
<ul>
<li><p>Latent_dim=32 定义隐空间的维度</p></li>
<li><p>(Batch, dim_model=512) -&gt; nn.Linear() -&gt; (Batch, 32x2),
第一个32的维度对应高斯分布的均值u,第二个32的维度对应方差 <span
class="math display">\[
2log(\sigma)
\]</span></p></li>
<li><p>根据重参数化的采样技巧：</p>
<ul>
<li><figure>
<img src="/2026/01/04/act-network/image-20260120152310177.png"
alt="image-20260120152310177" />
<figcaption aria-hidden="true">image-20260120152310177</figcaption>
</figure></li>
</ul></li>
</ul>
<p>​ 可以得到采样后的latent sample: (Batch, 32)</p>
<hr />
<blockquote>
<p>[!NOTE]</p>
<p>从上面可以知道，vae encoder只作用于训练的时候。vae
encoder输出包含有机器人观测和动作信息的tensor:(batch, 32)会用在act
decoder里面。那么在推理的时候不用vae encoder,采用的做法是zero
latent,为什么？（比如为什么不用正态分布）</p>
</blockquote>
<blockquote>
<p>[!IMPORTANT]</p>
<p>vae中，隐空间被约束为标准正态分布，均值0表示最普遍最平均的状态。这里跟SD中不一样，注意为什么？
LEAVE A QUESTION</p>
</blockquote>
<h2 id="act-decoder">ACT Decoder</h2>
<h3 id="act-decoder---transformer-encoder">ACT Decoder - transformer
encoder</h3>
<blockquote>
<p>[!IMPORTANT]</p>
<p>Transformer encoder和 vae
encoder的网络结构几乎是一样的。（transformer结构就是vae的一种实现）</p>
</blockquote>
<p>所以这里act decoder中transformer
encoder的网络结构和上面一样，不赘述。</p>
<p>（即由多个ActEncoderLayer组成，每个ACTEncoderLayer由一个nn.MultiheadAttention
+ skip connection + dropout +
Linear层组成，将输入的数据互相attention之后输出一个包含加权了之后的tensor）</p>
<ul>
<li>输入：
<ul>
<li>Encoder_in_tokens(即vae_encoder_output/transformer encoder input/
隐空间采样的样本latent sample):(Batch,lantent_dim=32) -&gt; nn.Linear()
-&gt; (Batch, model_dim=512)</li>
<li>Robot_state: (B, robot_state_dim) -&gt;nn.Linear()
-&gt;(Batch,512)</li>
<li>Cls:</li>
<li>Encoder_pos_emb</li>
<li>image_features: (Batch, n_cameras, C, H,W)-&gt;
<ul>
<li>对于每张图片，经过image backbone, nn.Conv2d(fc.in_channels-&gt;512)
-&gt; (Batch,512, h,w)</li>
<li>对应的pos_embed就是(1,512,h,w)</li>
<li>-&gt;transpose-&gt;((h,w), batch, dim=512),
pos_embed:((h,w)=300,batch,512)</li>
<li>-&gt;list(cam_features) 看成300个(batch, 512)</li>
</ul></li>
</ul></li>
<li>输入特征inputs:
<ul>
<li>[encoder_in_tokens(batch,512),
robot_state(batch,512),image_feature:(batch,512)...(两个相机一共300x2个cameras特征点)]</li>
<li>stack一下是:(sequence_length, batch, dim) s=1+1+300x2=1+1+600</li>
<li>输入的pos_embed:(1+1+600，batch, dim_model=512)
<ul>
<li>1表示encoder_in_tokens,即从vae encoder拿到的先验tensor,在vae
eoncder结构里对应cls;</li>
<li>第二个1表示robot_state</li>
<li>600表示camera的特征长度。</li>
</ul></li>
<li>没有key_padding_mask</li>
</ul></li>
<li>经过transformer
encoder的attention一通计算，输出att_out(sequence_len, batch, dim_model)
<ul>
<li>附：注意经过attention输出的维度和输入都是一样的，在vae_encoder里面输出也是这个维度，只是我们取出了第0位的cls向量作为最终tensor结果。</li>
<li>所以就transformer encoder的输入输出来说，输入是(S=sequence_len,
B=batch, D=dim_model),输出是经过注意力加权之后的tensor(S,B,D)</li>
</ul></li>
</ul>
<h3 id="vae-decoder--transformer-decoder">vae decoder- transformer
decoder</h3>
<ul>
<li><p>输入</p>
<ul>
<li>Decoder_in:(chunk_size=100, batch_size, dim_model=512)</li>
<li>Decoder_pos_embed:(chunk_size=10-0,512)-&gt;unsqueezed(1)-&gt;(100,1,512)</li>
<li>Encoder_out:(S=602,Batch,D=512)</li>
<li>Encoder_embed: 就是前面transfromer encoder的pos_embed,
因为经过attention输入输出都是(S,B,D),维度位置没有变</li>
</ul></li>
<li><p>对于transfromer的decoder的网络结构：</p>
<ul>
<li><p>transformerDecoder由一个actDecoderLayer组成，其中一个ACTDecoderLayer由2个MultiA</p></li>
<li><p>对于第一个multiattention块的q, k,
v是相对于decoder_in进行self_attention的。</p></li>
<li><p>第二个q是decoder_in+decoder_in_pos,k来自于encoder_output+encoder_output_pos,v=encoder_output</p></li>
<li><p>最终经过attention后输出的特征tensor是(chunk_size=100,batch,512)</p></li>
<li><p>-&gt;transpose
-&gt;(Batch,chunk_len=100,512)-&gt;nn.Linear(512,action_dim=6)-&gt;(batch,100,6)</p></li>
</ul></li>
</ul>
<h3 id="损失函数">损失函数</h3>
<p><span class="math display">\[
reconstruction loss + kl_weight*kld_loss
\]</span></p>
<ul>
<li><p>这里的reconstruction_loss是L1_loss(predict_action_chunk=b,s,d,
ground_truth_action=b,s,d)</p></li>
<li><p>Kld_loss是vae的dkl(latent_pdf || stand_normal)
均值和方差的维度是(batch, latent_dim=32)</p></li>
<li><p>上面的损失函数就是vae的损失，其来源来自于：</p>
<figure>
<img src="/2026/01/04/act-network/image-20260120152331170.png"
alt="image-20260120152331170" />
<figcaption aria-hidden="true">image-20260120152331170</figcaption>
</figure></li>
</ul>
<h3 id="act推理">act推理</h3>
<ul>
<li>用queue_size=100存储动作快，如果不采用时间集成的方法，参考原来的论文（https://www.alphaxiv.org/zh/overview/2304.13705），则直接pop_left即可。</li>
<li>如果用时间集成，权重wᵢ = exp(-temporal_ensemble_coeff *
i)，归一化之后乘以各自的值。
<ul>
<li>coefficient works:
<ul>
<li>Setting it to 0 uniformly weighs all actions.</li>
<li>Setting it positive gives more weight to older actions.</li>
<li>Setting it negative gives more weight to newer actions.</li>
<li>论文设置0.01，更关注old action</li>
</ul></li>
</ul></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://77philosophia.github.io/2025/12/24/git-clone-too-slow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="philosophia">
      <meta itemprop="description" content="我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Garfield's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/24/git-clone-too-slow/" class="post-title-link" itemprop="url">git-clone-too-slow</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-24 15:28:58 / Modified: 15:30:26" itemprop="dateCreated datePublished" datetime="2025-12-24T15:28:58+08:00">2025-12-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>git clone lerobo项目比较大，git clone一直中断。</p>
<p>解决方法：</p>
<p>先在gitee上创建仓库，从github上面同步代码库。然后从gitee上git clone
.(很快)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">philosophia</p>
  <div class="site-description" itemprop="description">我们孜孜以求的，不过是以人类有限的生命与身躯，触及一点点宇宙的真理</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">philosophia</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
